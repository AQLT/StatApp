{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk, re, pprint\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "#os.chdir('C:/Users/torna/Documents/StatApp/StatApp')\n",
    "#os.chdir('/Users/alainquartierlatente/Desktop/Ensae/StatApp')\n",
    "#os.chdir('/home/aqlt/Documents/Ensae/StatApp')\n",
    "os.chdir('C:/Users/Kim Antunez/Documents/Projets_autres/StatApp')\n",
    "\n",
    "#nom_dossier = \"ens\"\n",
    "nom_dossier = \"100k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99718\n"
     ]
    }
   ],
   "source": [
    "# Penser à changer selon taille\n",
    "with open(\"data/corpus_trie%s.file\" %nom_dossier, \"rb\") as f:\n",
    "    corpus = pickle.load(f) \n",
    "ens_tweets = [phrase.split() for phrase in corpus]\n",
    "phrases = ens_tweets\n",
    "print(len(phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lancement du modèle\n",
    "\n",
    "### Nouvelle simulation \n",
    "\n",
    "Paramètres fixés : le nombre de mots tirés dans le *negative sampling* et la proba utilisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramètres à modifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 5, 10, 15, 20, 25]\n",
    "\n",
    "# renseigner dans l'ordre : dim, epoch, window, learning_rate\n",
    "#simuls = [(100,100,4,0.02),(50,10,3,0.01)]\n",
    "simuls = [(100,80,4,0.02)]\n",
    "#dim = 100 / ep = 100 / w = 4 / lr = 0.02\n",
    "\n",
    "# Rq : numero_simulation n'est plus utilisé désormais ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1 à 1:36 : 1.15\n",
      "seed 5 à 1:38 : 1.26\n",
      "seed 10 à 1:39 : 1.29\n",
      "seed 15 à 1:40 : 1.27\n",
      "seed 20 à 1:41 : 1.04\n",
      "seed 25 à 1:42 : 1.08\n"
     ]
    }
   ],
   "source": [
    "for dim, epoch, window, learning_rate in simuls: #suppr\n",
    "\n",
    "    for seed in seeds:\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        model = word2vec.Word2Vec(phrases, size=dim, window=window,negative=5,\n",
    "                                  alpha=learning_rate,min_alpha=learning_rate, seed=seed,\n",
    "                          min_count=0, workers=4, iter=epoch)\n",
    "        # Kim : j'ai mis min_count = 0, devons-nous mettre 4 ?\n",
    "        \n",
    "        end = time.time()\n",
    "        # Affichage temps de tournage du modèle\n",
    "        currentDT = datetime.datetime.now()\n",
    "        print(\"seed {} à {}:{} : {}\".format(seed, currentDT.hour, currentDT.minute, round((end - start)/60, 2)))\n",
    "\n",
    "        # Sauvegarde \n",
    "        chemin = \"data/{}/gensim/dim{}_ep{}_w{}_lr{}_seed{}\".format(nom_dossier, dim, epoch, window, str(learning_rate)[2:], seed)\n",
    "        if not os.path.isdir(chemin):\n",
    "            os.mkdir(chemin)\n",
    "\n",
    "        model.save(chemin + \"/word2vec.model\")\n",
    "\n",
    "\n",
    "# phrases = les phrases du corpus\n",
    "# size = dimension du vecteur\n",
    "# window = fenetre\n",
    "# negative = nb de neg samples\n",
    "# alpha = learning rate\n",
    "# seed \n",
    "# min_count = fréquence min des mots qui nous intéresse\n",
    "# workers = nb de cores sur l'ordi\n",
    "# iter = nb epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation déjà effectuée (faire de nouvelles epoch)\n",
    "\n",
    "CETTE PARTIE BUG ET EST A RECODER !\n",
    "\n",
    "Paramètres à modifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1 #Nouveau nombre d'epoch à effectuer\n",
    "learning_rate = 0.01\n",
    "numero_simulation = 100\n",
    "seed = 1\n",
    "version = 1 #version du fichier à modifier : pex version = \"1\" pour word2vec1.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 622, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 489, in gensim.models.word2vec_inner.init_w2v_config\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 622, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 489, in gensim.models.word2vec_inner.init_w2v_config\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 622, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 489, in gensim.models.word2vec_inner.init_w2v_config\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 622, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 489, in gensim.models.word2vec_inner.init_w2v_config\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chemin = \"data/\" + nom_dossier + \"/gensim/Simulation_\" + str(numero_simulation) + \"_seed\" + str(seed)       \n",
    "model = gensim.models.keyedvectors.KeyedVectors.load(chemin + \"/word2vec\" + str(version) +\".model\")  \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#### NE MARCHE PAS ! \n",
    "#model.train(phrases, epochs=epoch, total_examples=model.corpus_count, \n",
    "#            start_alpha=learning_rate, end_alpha=learning_rate) #, compute_loss=True\n",
    "   \n",
    "end = time.time()\n",
    "\n",
    "# Sauvegarde \n",
    "#model.save(chemin + \"/word2vec\" + str(version+1) +\".model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Quelques fonctions intégrées dans gensim (non utilisées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99718"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['il', 'mérite', 'd', 'être', 'bloquer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = model.wv.vocab\n",
    "list(vocab)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8029589 ,  0.5978353 , -0.49241248, -0.5030643 , -0.03345106,\n",
       "       -0.0076034 ,  0.26484555,  0.8762143 , -0.4996161 ,  0.08204883,\n",
       "       -0.08663379,  0.75073415,  0.6436939 , -0.14495811,  0.647974  ,\n",
       "        0.6855672 , -0.269488  ,  0.2696291 ,  0.49807718, -0.59831595],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['wesh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kim Antunez\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8841527"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"homme\",\"femme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblio : \n",
    "    \n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "http://kavita-ganesan.com/how-to-incorporate-phrases-into-word2vec-a-text-mining-approach/#.XmvT7XIiE2x\n",
    "https://rare-technologies.com/word2vec-tutorial/\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
