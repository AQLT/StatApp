{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'‚Äô\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk, re, pprint\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "os.chdir('/Users/alainquartierlatente/Desktop/Ensae/StatApp')\n",
    "df = pd.read_csv(\"data/sample_3.txt\",sep='\\n',header=None)\n",
    "print(string.punctuation + \"'‚Äô\")\n",
    "df = df.values\n",
    "def mise_en_forme_phrase (phrase):\n",
    "    phrase = phrase.lower()\n",
    "    # On enl√®ve la ponctuation mais √ßa peut se discuter (garder les @ et #?)\n",
    "    phrase = re.sub('( @[^ ]*)|(^@[^ ]*)',\"nickname\", phrase) #Remplace @... par nickname\n",
    "    # On enl√®ve la ponctuation + certaines apostrophes\n",
    "    phrase = phrase.translate(str.maketrans('', '', string.punctuation + \"'‚Äô\"))\n",
    "    # On enl√®ve les passages √† la ligne\n",
    "    phrase = re.sub('\\\\n', ' ', phrase)\n",
    "    # On enl√®ve les espaces multiples et les espaces √† la fin des phrases\n",
    "    phrase = re.sub(' +', ' ', phrase)\n",
    "    phrase = re.sub(' +$', '', phrase)\n",
    "    phrase.isalpha()\n",
    "    return(phrase)\n",
    "mise_en_forme_phrase_v = np.vectorize(mise_en_forme_phrase) # Pour vectoriser la fonction\n",
    "df = mise_en_forme_phrase_v(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df).to_csv(\"data/sample_3.txt\",sep='\\n',header=None, index=False)\n",
    "os.chdir('/Users/alainquartierlatente/Desktop/Ensae/StatApp')\n",
    "df = pd.read_csv(\"data/sample_3.txt\",sep='\\n',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['il m√©rite d √™tre bloquer la lettre de l alphabet'], ['nickname et fi√®re je t en voi att j avais oubli√©'], ['il est 1 heure'], ['eeeeh jfais la go qui a de les programmes mais j ai m√™me pas de navigo ptdddddr üò≠'], ['en tout cas la demoiselle a bien raison']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['il',\n",
       "  'm√©rite',\n",
       "  'd',\n",
       "  '√™tre',\n",
       "  'bloquer',\n",
       "  'la',\n",
       "  'lettre',\n",
       "  'de',\n",
       "  'l',\n",
       "  'alphabet'],\n",
       " ['nickname',\n",
       "  'et',\n",
       "  'fi√®re',\n",
       "  'je',\n",
       "  't',\n",
       "  'en',\n",
       "  'voi',\n",
       "  'att',\n",
       "  'j',\n",
       "  'avais',\n",
       "  'oubli√©'],\n",
       " ['il', 'est', '1', 'heure'],\n",
       " ['eeeeh',\n",
       "  'jfais',\n",
       "  'la',\n",
       "  'go',\n",
       "  'qui',\n",
       "  'a',\n",
       "  'de',\n",
       "  'les',\n",
       "  'programmes',\n",
       "  'mais',\n",
       "  'j',\n",
       "  'ai',\n",
       "  'm√™me',\n",
       "  'pas',\n",
       "  'de',\n",
       "  'navigo',\n",
       "  'ptdddddr',\n",
       "  'üò≠'],\n",
       " ['en', 'tout', 'cas', 'la', 'demoiselle', 'a', 'bien', 'raison']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(data)\n",
    "[phrase[0].split() for phrase in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))\n",
    "print(\"mise en form ok\")\n",
    "phrases = []\n",
    "for index, row in df.iterrows():\n",
    "    for j, column in row.iteritems():\n",
    "        phrases.append(column.split())\n",
    "        \n",
    "#phrases = phrases[0:10000]\n",
    "#phrases = df2.apply(mise_en_forme_phrase)\n",
    "\n",
    "print(phrases[0:10])\n",
    "#raw = ''.join([''.join(phrase) for phrase in phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'il', 'm√©rite', 'd', '√™tre', 'bloquer', 'la', 'lettre', 'de', 'l', 'alph', 'name', '0', 'dtype', 'object']\n",
      "[['il m√©rite d‚Äô √™tre bloquer la lettre de l‚Äô alphabet ']]\n"
     ]
    }
   ],
   "source": [
    "df2 = df.astype(str)\n",
    "for j, column in df2[0:1].iteritems():\n",
    "    print(mise_en_forme_phrase(column))\n",
    "print((df2.values[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [item for sublist in phrases for item in sublist]\n",
    "print(type(words))\n",
    "vocabulary = set(words)\n",
    "print(\"Nombre de mots :\", len(words))\n",
    "print(\"Taille du vocabulaire :\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pour changer la taille des graphiques :\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "fdist = nltk.FreqDist(words)\n",
    "print(\"Les 10 mots les plus communs sont :\")\n",
    "print(fdist.most_common(10))\n",
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour mettre √† jour le graphique en direct\n",
    "def live_plot(data, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling rate et negative sampling\n",
    "On va simplifier un peu le corpus en enlevant certains mots. Pour cela on va faire un sous-√©chantillonnage du corpus pour supprimer certains mots. \n",
    "\n",
    "Pour chaque mot $w_i$ on note $z(w_i)$ la proportion d'apparition de ce mot, c'est-√†-dire le rapport entre le nombre de fois que ce mot apparait et le nombre total de mots. La probabilit√© de garder un mot le mot $w_i$ est :\n",
    "$$\n",
    "\\mathbb P(w_i) = \\left(\\sqrt{\\frac{z(w_i)}{q}} + 1 \\right)\n",
    "\\times\n",
    "\\frac{q}{z(w_i)}\n",
    "$$\n",
    "Le param√®tre $q$ est appel√© \"sample\" ‚Äì √©chantillonnage ‚Äì contr√¥le le nombre de sous-√©chantillonnages. La valeur par d√©faut est 0,001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_proba(x):\n",
    "    result = (sqrt(x)+1)*(1/x)\n",
    "    return(result)\n",
    "calcul_proba_v = np.vectorize(calcul_proba) # Pour vectoriser la fonction\n",
    "# Fonction pour cr√©er l'√©chantillon\n",
    "def creer_echantillon(phrases, vocabulary , probabilities_subsampling,  window = 2):\n",
    "    #Sub-sampling\n",
    "    nouveau_corpus = [] \n",
    "    for phrase in phrases: #on parcourt tous les articles du corpus\n",
    "        nouveau_corpus.append([]) #on cr√©e une sous liste √† chaque nouvel article\n",
    "        for word in phrase: #et pour tous les mots de l'article\n",
    "        # Les mots √† supprimer sont les mots tels que la loi g√©n√©r√©e U([0,1]) soit > proba\n",
    "        # On garde donc les mots si U([0,1]) <= proba\n",
    "            proba_w = probabilities_subsampling[vocabulary.index(word)]\n",
    "            if np.random.uniform(low=0.0, high=1.0) <= proba_w: # Je garde le mot\n",
    "                nouveau_corpus[-1].append(word) \n",
    "    phrases = [phrase for phrase in nouveau_corpus if len(phrase)>1] # On enl√®ve les phrases avec 1 seul mot\n",
    "    test_sample = []\n",
    "    for phrase in phrases:\n",
    "        # Pour chaque phrase on prend au hasard un mot focus et un mot contexte\n",
    "        focus = list(range(0, len(phrase)))\n",
    "        focus = random.choice(focus)\n",
    "        i = focus\n",
    "        index_i = vocabulary.index(phrase[i])\n",
    "        i_contexte = list(range(max(i-window,0), min(i+window+1, len(phrase))))\n",
    "        i_contexte.remove(i)\n",
    "        i_contexte = random.choice(i_contexte)\n",
    "        j = i_contexte\n",
    "        index_j = vocabulary.index(phrase[j])\n",
    "        test_sample.append([index_i, index_j])\n",
    "    return(test_sample)\n",
    "\n",
    "sample = 0.001\n",
    "fdist = nltk.FreqDist(words)\n",
    "vocabulary = list(set(words))\n",
    "proportion = np.array([(fdist[w]/ (len(words) * sample)) for w in vocabulary])\n",
    "p_subsampling = calcul_proba_v(proportion) # C'est le vecteur contenant les proba de sub-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme avec softmax\n",
    "Si on note $\\theta$ le param√®tre √† estimer, $L(\\theta)$ la fonction de perte et $\\eta$ le taux d'apprentissage (*learning rate*) alors :\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta^{(t)})\n",
    "$$\n",
    "C'est ce que l'on appelle la descente de gradient. On va appliquer cette m√©thode √† chaque √©tape pour la matrice d'input et la matrice d'output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ad4ebbd62bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtest_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreer_echantillon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_subsampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfocus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcompteur\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-bd0122979a81>\u001b[0m in \u001b[0;36mcreer_echantillon\u001b[0;34m(phrases, vocabulary, probabilities_subsampling, window)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Les mots √† supprimer sont les mots tels que la loi g√©n√©r√©e U([0,1]) soit > proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# On garde donc les mots si U([0,1]) <= proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mproba_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities_subsampling\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mproba_w\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mnouveau_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dim = 10\n",
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Attention: torch.rand g√©n√®re une loi uniforme et torch.randn une loi normale\n",
    "input = torch.randn(len(vocabulary), dim)\n",
    "output = torch.randn(len(vocabulary), dim)\n",
    "input = autograd.Variable(input, requires_grad=True)\n",
    "output = autograd.variable(output, requires_grad=True)\n",
    "\n",
    "loss_tot = []\n",
    "temps_par_epoch = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(epoch):\n",
    "    #print(i)\n",
    "    loss_val = 0\n",
    "    start_epoch = time.time()\n",
    "    test_sample = creer_echantillon(phrases, vocabulary, p_subsampling)\n",
    "    for focus, context in test_sample:\n",
    "        # Multiplication matricielle: \n",
    "        data = torch.matmul(input[focus,], torch.t(output))\n",
    "        #log_probs = F.log_softmax(data, dim=0)\n",
    "        #loss = F.nll_loss(log_probs.view(1,-1), torch.tensor([context]))\n",
    "        # Il semble que cela combine les deux pr√©c√©dentes fonctions : \n",
    "        # https://pytorch.org/docs/stable/nn.functional.html#cross-entropy\n",
    "        loss = F.cross_entropy(data.view(1,-1), torch.tensor([context]))\n",
    "        #print(loss)\n",
    "        loss_val += loss.data\n",
    "        # Pour ensuite d√©river les matrices par rapport √† la loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Il faut modifier juste le .data pour ne pas perdre la structure\n",
    "        input.data = input.data - learning_rate * input.grad.data\n",
    "        output.data = output.data - learning_rate * output.grad.data\n",
    "        \n",
    "        input.grad.data.zero_()\n",
    "        output.grad.data.zero_()\n",
    "    \n",
    "    end_epoch = time.time()\n",
    "    temps_par_epoch.append(end_epoch - start_epoch)\n",
    "    loss_val = loss_val / len(vocabulary)\n",
    "    loss_tot.append(loss_val)\n",
    "    live_plot(loss_tot)\n",
    "end = time.time()\n",
    "print(round((end - start)/60, 2))\n",
    "#print(input)        \n",
    "#plt.plot(loss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme avec negsampling\n",
    "\n",
    "Pour le n√©gative sampling, la probabilit√© de garder le mot $w_i$ est √©gale √† :\n",
    "$$\n",
    "\\mathbb P(w_i) = \\frac{f(w_i)^{3/4}}{\n",
    "\\sum_{j=1}^n f(w_j)^{3/4}\n",
    "}\n",
    "$$\n",
    "Avec $f(w_j)$ la fr√©quence d'apparition du mot $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tirage_neg_sampling(vocabulary, proba_negativesampling, focus, context, K = 5):\n",
    "    #proba_negativesampling[focus] = 0\n",
    "    #proba_negativesampling[context] = 0\n",
    "    liste_vocab = list(range(len(vocabulary)))\n",
    "    neg_sampling = np.random.choice(liste_vocab, size=K, p=proba_negativesampling)\n",
    "   # while( (focus in neg_sampling) | (context in neg_sampling)):\n",
    "   #     neg_sampling = np.random.choice(liste_vocab, size=K, p=proba_negativesampling)\n",
    "    return(neg_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2\n",
    "dim = 10\n",
    "epoch = 50\n",
    "learning_rate = 0.01\n",
    "K = 5\n",
    "\n",
    "def word2vec_1(phrases, dim=10, epoch = 10, learning_rate = 0.01, K = 5, sample = 0.001, window = 2, plot = True):\n",
    "    words = [item for sublist in phrases for item in sublist]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    vocabulary = list(set(words))\n",
    "    proportion = np.array([(fdist[w]/ (len(words) * sample)) for w in vocabulary])\n",
    "    p_subsampling = calcul_proba_v(proportion)\n",
    "    p_negativesampling = np.array([(fdist[w]**(3/4)) for w in vocabulary])\n",
    "    p_negativesampling /= p_negativesampling.sum()\n",
    "    input = torch.randn(len(vocabulary), dim)\n",
    "    output = torch.randn(len(vocabulary), dim)\n",
    "    input = autograd.Variable(input, requires_grad=True)\n",
    "    output = autograd.variable(output, requires_grad=True)\n",
    "\n",
    "    loss_tot = []\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(epoch):\n",
    "        #print(i)\n",
    "        loss_val = 0\n",
    "        test_sample = creer_echantillon(phrases, vocabulary, p_subsampling, window)\n",
    "        for focus, context in test_sample:\n",
    "            neg_sample = tirage_neg_sampling(vocabulary, p_negativesampling,\n",
    "                                             focus, context,\n",
    "                                             K = K)\n",
    "            #vect_sample = np.append(context, neg_sample)\n",
    "            data = torch.matmul(input[focus,], torch.t(output[context,]))\n",
    "            loss1 = - F.logsigmoid(data)\n",
    "\n",
    "            data = torch.matmul(input[focus,], torch.t(output[neg_sample,]))\n",
    "            loss2 = - F.logsigmoid(-data).sum()\n",
    "            #print(loss)\n",
    "            loss_val += loss1 + loss2\n",
    "            # Pour ensuite d√©river les matrices par rapport √† la loss\n",
    "            (loss1+loss2).backward()\n",
    "\n",
    "            # Il faut modifier juste le .data pour ne pas perdre la structure\n",
    "            input.data = input.data - learning_rate * input.grad.data\n",
    "            output.data = output.data - learning_rate * output.grad.data\n",
    "\n",
    "            input.grad.data.zero_()\n",
    "            output.grad.data.zero_()\n",
    "        loss_val = loss_val / len(vocabulary)\n",
    "        loss_tot.append(loss_val)\n",
    "        if plot:\n",
    "            live_plot(loss_tot)\n",
    "    end = time.time()\n",
    "    print(round((end - start)/60, 2))\n",
    "    return(((input + output)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35\n"
     ]
    }
   ],
   "source": [
    "W3 = word_2_vec(phrases, epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
