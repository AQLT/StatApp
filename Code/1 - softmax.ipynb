{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk, re, pprint\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import pickle\n",
    "\n",
    "os.chdir('/Users/alainquartierlatente/Desktop/Ensae/StatApp')\n",
    "\n",
    "with open(\"data/corpus_trie.file\", \"rb\") as f:\n",
    "    corpus = pickle.load(f) \n",
    "phrases = [phrase.split() for phrase in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Nombre de mots : 31394494\n",
      "Taille du vocabulaire : 634691\n"
     ]
    }
   ],
   "source": [
    "words = [item for sublist in phrases for item in sublist]\n",
    "print(type(words))\n",
    "vocabulary = set(words)\n",
    "print(\"Nombre de mots :\", len(words))\n",
    "print(\"Taille du vocabulaire :\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pour changer la taille des graphiques :\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "fdist = nltk.FreqDist(words)\n",
    "print(\"Les 10 mots les plus communs sont :\")\n",
    "print(fdist.most_common(10))\n",
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour mettre à jour le graphique en direct\n",
    "def live_plot(data, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling rate et negative sampling\n",
    "On va simplifier un peu le corpus en enlevant certains mots. Pour cela on va faire un sous-échantillonnage du corpus pour supprimer certains mots. \n",
    "\n",
    "Pour chaque mot $w_i$ on note $z(w_i)$ la proportion d'apparition de ce mot, c'est-à-dire le rapport entre le nombre de fois que ce mot apparait et le nombre total de mots. La probabilité de garder un mot le mot $w_i$ est :\n",
    "$$\n",
    "\\mathbb P(w_i) = \\left(\\sqrt{\\frac{z(w_i)}{q}} + 1 \\right)\n",
    "\\times\n",
    "\\frac{q}{z(w_i)}\n",
    "$$\n",
    "Le paramètre $q$ est appelé \"sample\" – échantillonnage – contrôle le nombre de sous-échantillonnages. La valeur par défaut est 0,001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_proba(x):\n",
    "    result = (sqrt(x)+1)*(1/x)\n",
    "    return(result)\n",
    "calcul_proba_v = np.vectorize(calcul_proba) # Pour vectoriser la fonction\n",
    "# Fonction pour créer l'échantillon\n",
    "def creer_echantillon(phrases, vocabulary , probabilities_subsampling,  window = 2):\n",
    "    #Sub-sampling\n",
    "    nouveau_corpus = [] \n",
    "    for phrase in phrases: #on parcourt tous les articles du corpus\n",
    "        nouveau_corpus.append([]) #on crée une sous liste à chaque nouvel article\n",
    "        for word in phrase: #et pour tous les mots de l'article\n",
    "        # Les mots à supprimer sont les mots tels que la loi générée U([0,1]) soit > proba\n",
    "        # On garde donc les mots si U([0,1]) <= proba\n",
    "            proba_w = probabilities_subsampling[vocabulary.index(word)]\n",
    "            if np.random.uniform(low=0.0, high=1.0) <= proba_w: # Je garde le mot\n",
    "                nouveau_corpus[-1].append(word) \n",
    "    phrases = [phrase for phrase in nouveau_corpus if len(phrase)>1] # On enlève les phrases avec 1 seul mot\n",
    "    test_sample = []\n",
    "    for phrase in phrases:\n",
    "        # Pour chaque phrase on prend au hasard un mot focus et un mot contexte\n",
    "        focus = list(range(0, len(phrase)))\n",
    "        focus = random.choice(focus)\n",
    "        i = focus\n",
    "        index_i = vocabulary.index(phrase[i])\n",
    "        i_contexte = list(range(max(i-window,0), min(i+window+1, len(phrase))))\n",
    "        i_contexte.remove(i)\n",
    "        i_contexte = random.choice(i_contexte)\n",
    "        j = i_contexte\n",
    "        index_j = vocabulary.index(phrase[j])\n",
    "        test_sample.append([index_i, index_j])\n",
    "    return(test_sample)\n",
    "\n",
    "sample = 0.001\n",
    "fdist = nltk.FreqDist(words)\n",
    "vocabulary = list(set(words))\n",
    "proportion = np.array([(fdist[w]/ (len(words) * sample)) for w in vocabulary])\n",
    "p_subsampling = calcul_proba_v(proportion) # C'est le vecteur contenant les proba de sub-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme avec softmax\n",
    "Si on note $\\theta$ le paramètre à estimer, $L(\\theta)$ la fonction de perte et $\\eta$ le taux d'apprentissage (*learning rate*) alors :\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta^{(t)})\n",
    "$$\n",
    "C'est ce que l'on appelle la descente de gradient. On va appliquer cette méthode à chaque étape pour la matrice d'input et la matrice d'output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ad4ebbd62bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtest_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreer_echantillon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_subsampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfocus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcompteur\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-bd0122979a81>\u001b[0m in \u001b[0;36mcreer_echantillon\u001b[0;34m(phrases, vocabulary, probabilities_subsampling, window)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Les mots à supprimer sont les mots tels que la loi générée U([0,1]) soit > proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# On garde donc les mots si U([0,1]) <= proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mproba_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities_subsampling\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mproba_w\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mnouveau_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dim = 10\n",
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Attention: torch.rand génère une loi uniforme et torch.randn une loi normale\n",
    "input = torch.randn(len(vocabulary), dim)\n",
    "output = torch.randn(len(vocabulary), dim)\n",
    "input = autograd.Variable(input, requires_grad=True)\n",
    "output = autograd.variable(output, requires_grad=True)\n",
    "\n",
    "loss_tot = []\n",
    "temps_par_epoch = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(epoch):\n",
    "    #print(i)\n",
    "    loss_val = 0\n",
    "    start_epoch = time.time()\n",
    "    test_sample = creer_echantillon(phrases, vocabulary, p_subsampling)\n",
    "    for focus, context in test_sample:\n",
    "        # Multiplication matricielle: \n",
    "        data = torch.matmul(input[focus,], torch.t(output))\n",
    "        #log_probs = F.log_softmax(data, dim=0)\n",
    "        #loss = F.nll_loss(log_probs.view(1,-1), torch.tensor([context]))\n",
    "        # Il semble que cela combine les deux précédentes fonctions : \n",
    "        # https://pytorch.org/docs/stable/nn.functional.html#cross-entropy\n",
    "        loss = F.cross_entropy(data.view(1,-1), torch.tensor([context]))\n",
    "        #print(loss)\n",
    "        loss_val += loss.data\n",
    "        # Pour ensuite dériver les matrices par rapport à la loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Il faut modifier juste le .data pour ne pas perdre la structure\n",
    "        input.data = input.data - learning_rate * input.grad.data\n",
    "        output.data = output.data - learning_rate * output.grad.data\n",
    "        \n",
    "        input.grad.data.zero_()\n",
    "        output.grad.data.zero_()\n",
    "    \n",
    "    end_epoch = time.time()\n",
    "    temps_par_epoch.append(end_epoch - start_epoch)\n",
    "    loss_val = loss_val / len(vocabulary)\n",
    "    loss_tot.append(loss_val)\n",
    "    live_plot(loss_tot)\n",
    "end = time.time()\n",
    "print(round((end - start)/60, 2))\n",
    "#print(input)        \n",
    "#plt.plot(loss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_finale = (input + output)/2\n",
    "with open(\"data/matrice_finale_softmax.file\", \"wb\") as f:\n",
    "    pickle.dump(matrice_finale, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
