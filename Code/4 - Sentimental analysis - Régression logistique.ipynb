{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer le modèle Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/torna/Documents/StatApp/StatApp')\n",
    "#os.chdir('C:/Users/Kim Antunez/Documents/Projets_autres')\n",
    "#os.chdir('/Users/alainquartierlatente/Desktop/Ensae/StatApp')\n",
    "#os.chdir('/home/aqlt/Documents/Ensae/StatApp')\n",
    "\n",
    "nom_dossier = 'ens'\n",
    "# dim = 50\n",
    "dim = 300\n",
    "epoch = 100\n",
    "window = 4\n",
    "learning_rate = 0.02\n",
    "    \n",
    "chemin1 = \"data/{}/gensim/dim{}_ep{}_w{}_lr{}_seed1\".format(nom_dossier, dim, epoch, window, str(learning_rate)[2:]) \n",
    "model1 = gensim.models.keyedvectors.KeyedVectors.load(chemin1 + \"/word2vec.model\")\n",
    "chemin2 = \"data/{}/gensim/dim{}_ep{}_w{}_lr{}_seed5\".format(nom_dossier, dim, epoch, window, str(learning_rate)[2:])  \n",
    "model2 = gensim.models.keyedvectors.KeyedVectors.load(chemin2 + \"/word2vec.model\")\n",
    "chemin3 = \"data/{}/gensim/dim{}_ep{}_w{}_lr{}_seed10\".format(nom_dossier, dim, epoch, window, str(learning_rate)[2:])  \n",
    "model3 = gensim.models.keyedvectors.KeyedVectors.load(chemin3 + \"/word2vec.model\")\n",
    "chemin4 = \"data/{}/gensim/dim{}_ep{}_w{}_lr{}_seed15\".format(nom_dossier, dim, epoch, window, str(learning_rate)[2:])  \n",
    "model4 = gensim.models.keyedvectors.KeyedVectors.load(chemin4 + \"/word2vec.model\")\n",
    "chemin5 = \"data/{}/gensim/dim{}_ep{}_w{}_lr{}_seed20\".format(nom_dossier, dim, epoch, window, str(learning_rate)[2:])  \n",
    "model5 = gensim.models.keyedvectors.KeyedVectors.load(chemin5 + \"/word2vec.model\")\n",
    "chemin6 = \"data/{}/gensim/dim{}_ep{}_w{}_lr{}_seed25\".format(nom_dossier, dim, epoch, window, str(learning_rate)[2:]) \n",
    "model6 = gensim.models.keyedvectors.KeyedVectors.load(chemin6 + \"/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice = {word : (model1.wv[word]+model2.wv[word]+model3.wv[word]+model4.wv[word]+model5.wv[word]+model6.wv[word])/6\n",
    "           for word in model1.wv.index2word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On importe les données de train et on calcule le \"sentence-embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on veut récup les fichiers\n",
    "with open(\"data/sentimental_analysis/corpus_test.file\", \"rb\") as f:\n",
    "    corpus_test = pickle.load(f)\n",
    "with open(\"data/sentimental_analysis/corpus_train.file\", \"rb\") as f:\n",
    "    corpus_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_sentence_embedding(phrase):\n",
    "    if len(phrase)>0:\n",
    "        sum1 = sum(matrice[mot] for mot in phrase if mot in model1.wv.index2word)\n",
    "        sum2 = sum(matrice[\"lowfrequencyword\"] for mot in phrase if mot not in model1.wv.index2word)\n",
    "        return (sum1+sum2)/len(phrase)\n",
    "    else:\n",
    "        return matrice[\"lowfrequencyword\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [[tweet.split(),score] for (score, tweet) in corpus_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_emb = [[score,calcul_sentence_embedding(phrase)] for (phrase,score) in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [score for (score, embedding) in phrases_emb]\n",
    "X_train = [embedding for (score, embedding) in phrases_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "reg_log = lr.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18048  6952]\n",
      " [ 7229 17771]]\n",
      "0.71638\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred = reg_log.predict(X_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "# Matrice confusion\n",
    "cm = metrics.confusion_matrix(Y_train,Y_train_pred)\n",
    "print(cm)\n",
    "# Accuracy\n",
    "acc = metrics.accuracy_score(Y_train,Y_train_pred)\n",
    "print(acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_test = [[tweet.split(),score] for (score, tweet) in corpus_test]\n",
    "phrases_test_emb = [[score,calcul_sentence_embedding(phrase)] for (phrase,score) in phrases_test]\n",
    "Y_test = [score for (score, embedding) in phrases_test_emb]\n",
    "X_test = [embedding for (score, embedding) in phrases_test_emb]\n",
    "Y_test_pred = reg_log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1438  546]\n",
      " [ 633 1365]]\n",
      "0.703917629331994\n"
     ]
    }
   ],
   "source": [
    "# Matrice confusion\n",
    "cm_test = metrics.confusion_matrix(Y_test,Y_test_pred)\n",
    "print(cm_test)\n",
    "# Accuracy\n",
    "acc_test = metrics.accuracy_score(Y_test,Y_test_pred)\n",
    "print(acc_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi ce score ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21204\n",
      "39650\n"
     ]
    }
   ],
   "source": [
    "phrases = [tweet.split() for (score, tweet) in corpus_train]\n",
    "mots = [item for sublist in phrases for item in sublist]\n",
    "vocabulaire = list(dict.fromkeys(mots))\n",
    "n = 0\n",
    "absents = []\n",
    "for word in vocabulaire:\n",
    "    if word not in model1.wv.index2word:\n",
    "        n+=1\n",
    "        absents.append(word)\n",
    "print(n)\n",
    "print(len(vocabulaire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347793190416141"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n/len(vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words =  set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppr_stop(phrase):\n",
    "    return [mot for mot in phrase if mot not in stop_words]\n",
    "    \n",
    "phrases_stop = [[suppr_stop(tweet.split()),score] for (score, tweet) in corpus_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['est', 'très', 'heureux', 'que', 'aujourdhui', 'marque', '6', 'merveilleux', 'mois', 'avec', 'stephanie'], 1]\n",
      "[['très', 'heureux', 'aujourdhui', 'marque', '6', 'merveilleux', 'mois', 'stephanie'], 1]\n"
     ]
    }
   ],
   "source": [
    "print(phrases[0])\n",
    "print(phrases_stop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_emb_stop = [[score,calcul_sentence_embedding(phrase)] for (phrase,score) in phrases_stop]\n",
    "Y_train_stop = [score for (score, embedding) in phrases_emb_stop]\n",
    "X_train_stop = [embedding for (score, embedding) in phrases_emb_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_stop = LogisticRegression()\n",
    "reg_log_st = lr_stop.fit(X_train_stop,Y_train_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17455  7545]\n",
      " [ 7415 17585]]\n",
      "0.7008\n",
      "[[1388  596]\n",
      " [ 617 1381]]\n",
      "0.6953792064289301\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred_stop = reg_log_st.predict(X_train_stop)\n",
    "\n",
    "# Matrice confusion\n",
    "cm_stop = metrics.confusion_matrix(Y_train_stop,Y_train_pred_stop)\n",
    "print(cm_stop)\n",
    "# Accuracy\n",
    "acc_stop = metrics.accuracy_score(Y_train_stop,Y_train_pred_stop)\n",
    "print(acc_stop) \n",
    "\n",
    "\n",
    "phrases_test_stop = [[suppr_stop(tweet.split()),score] for (score, tweet) in corpus_test]\n",
    "phrases_test_emb_stop = [[score,calcul_sentence_embedding_stop(phrase)] for (phrase,score) in phrases_test_stop]\n",
    "Y_test_stop = [score for (score, embedding) in phrases_test_emb_stop]\n",
    "X_test_stop = [embedding for (score, embedding) in phrases_test_emb_stop]\n",
    "Y_test_pred_stop = reg_log_st.predict(X_test_stop)\n",
    "\n",
    "# Matrice confusion\n",
    "cm_test_stop = metrics.confusion_matrix(Y_test_stop,Y_test_pred_stop)\n",
    "print(cm_test_stop)\n",
    "# Accuracy\n",
    "acc_test_stop = metrics.accuracy_score(Y_test_stop,Y_test_pred_stop)\n",
    "print(acc_test_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauve les modèles\n",
    "with open(\"data/sentimental_analysis/reg_log_300.file\", \"wb\") as f:\n",
    "    pickle.dump(reg_log, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"data/sentimental_analysis/reg_log_st_300.file\", \"wb\") as f:\n",
    "    pickle.dump(reg_log_st, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
