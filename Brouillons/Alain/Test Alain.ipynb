{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'‚Äô\n",
      "[['il', 'm√©rite', 'd', '√™tre', 'bloquer', 'la', 'lettre', 'de', 'l', 'alphabet'], ['reinedonna', 'et', 'fi√®re', 'je', 't', 'en', 'voi', 'att', 'j', 'avais', 'oubli√©'], ['il', 'est', '1', 'heure'], ['eeeeh', 'jfais', 'la', 'go', 'qui', 'a', 'de', 'les', 'programmes', 'mais', 'j', 'ai', 'm√™me', 'pas', 'de', 'navigo', 'ptdddddr', 'üò≠'], ['en', 'tout', 'cas', 'la', 'demoiselle', 'a', 'bien', 'raison'], ['le', 'rathalos', 'est', 'un', 'gros', 'fils', 'de', 'pute', 'bonne', 'nuit'], ['stephanielevy75', 'et', 'puis', 'un', 'jour', 'pfffffffffus', 'rien', 'ne', 'fonctionne', 'l', 'humain', 'se', 'meurt', '√†', 'petit', 'feu'], ['rebeudeter', 'je', 'veux', 'pas', 'en', 'savoir', 'sasuke', 'j', 'sais', 'pas', 'qui', 'c', 'est', 'mais', 'j', 'crois', 'il', 'est', 'sombre'], ['√†', 'partir', 'de', 'quand', 'peut', 'on', 'se', 'qualifier', 'de', 'fan'], ['go', 'profit√©', 'de', 'les', '6h20', 'de', 'sommeil', 'qu', 'il', 'me', 'reste']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import math \n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "os.chdir('/Users/alainquartierlatente/Desktop/Ensae/StatApp')\n",
    "print(string.punctuation + \"'‚Äô\")\n",
    "def mise_en_forme_phrase (phrase):\n",
    "    phrase = phrase.lower()\n",
    "    # On el√®ve la ponctuation mais √ßa peut se discuter (garder les @ et #?)\n",
    "    re.sub('( @[^ ]*)|(^@[^ ]*)',\"nickname\", phrase) #Remplace @... par nickname\n",
    "    phrase = re.sub('@', '', phrase)\n",
    "    phrase = phrase.translate(str.maketrans('', '', string.punctuation + \"'‚Äô\"))\n",
    "    # On enl√®ve les passages √† la ligne\n",
    "    phrase = re.sub('\\\\n', ' ', phrase)\n",
    "    # On enl√®ve les espaces multiples et les espaces √† la fin des phrases\n",
    "    phrase = re.sub(' +', ' ', phrase)\n",
    "    phrase = re.sub(' +$', '', phrase)\n",
    "    return(phrase.split())\n",
    "#f = open('data/sample_3.txt')\n",
    "#raw = f.read()\n",
    "#print(type(raw))\n",
    "with open('data/sample_3.txt') as myfile:\n",
    "    phrases = [mise_en_forme_phrase(next(myfile)) for x in range(10000)]\n",
    "print(phrases[0:10])\n",
    "#raw = ''.join([''.join(phrase) for phrase in phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, pprint\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "# words = word_tokenize(raw) # Plus utile maintenant\n",
    "words = [item for sublist in phrases for item in sublist]\n",
    "print(type(words))\n",
    "## On enl√®ve la ponctuation et on met en minuscule :\n",
    "#words = [word.lower() for word in words if word.isalpha()] # plus utile maintenant\n",
    "vocabulary = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots : 111627\n",
      "Taille du vocabulaire : 19307\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de mots :\", len(words))\n",
    "print(\"Taille du vocabulaire :\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 10 mots les plus communs sont :\n",
      "[('de', 3954), ('le', 2558), ('est', 2147), ('je', 2037), ('les', 1813), ('j', 1785), ('√†', 1725), ('c', 1693), ('la', 1622), ('pas', 1582)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a216a68d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour changer la taille des graphiques :\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "fdist = nltk.FreqDist(words)\n",
    "print(\"Les 10 mots les plus communs sont :\")\n",
    "print(fdist.most_common(10))\n",
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling rate\n",
    "On va simplifier un peu le corpus en enlevant certains mots. Pour cela on va faire un sous-√©chantillonnage du corpus pour supprimer certains mots. \n",
    "\n",
    "Pour chaque mot $w_i$ on note $z(w_i)$ la proportion d'apparition de ce mot, c'est-√†-dire le rapport entre le nombre de fois que ce mot apparait et le nombre total de mots. La probabilit√© de garder un mot le mot $w_i$ est :\n",
    "$$\n",
    "\\mathbb P(w_i) = \\left(\\sqrt{\\frac{z(w_i)}{q}} + 1 \\right)\n",
    "\\times\n",
    "\\frac{q}{z(w_i)}\n",
    "$$\n",
    "Le param√®tre $q$ est appel√© \"sample\" ‚Äì √©chantillonnage ‚Äì contr√¥le le nombre de sous-√©chantillonnages. La valeur par d√©faut est 0,001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['il', 'm√©rite', 'd', '√™tre', 'bloquer', 'la', 'lettre', 'de', 'l', 'alphabet'], ['reinedonna', 'et', 'fi√®re', 'je', 't', 'en', 'voi', 'att', 'j', 'avais', 'oubli√©'], ['il', 'est', '1', 'heure'], ['eeeeh', 'jfais', 'la', 'go', 'qui', 'a', 'de', 'les', 'programmes', 'mais', 'j', 'ai', 'm√™me', 'pas', 'de', 'navigo', 'ptdddddr', 'üò≠'], ['en', 'tout', 'cas', 'la', 'demoiselle', 'a', 'bien', 'raison'], ['le', 'rathalos', 'est', 'un', 'gros', 'fils', 'de', 'pute', 'bonne', 'nuit'], ['stephanielevy75', 'et', 'puis', 'un', 'jour', 'pfffffffffus', 'rien', 'ne', 'fonctionne', 'l', 'humain', 'se', 'meurt', '√†', 'petit', 'feu'], ['rebeudeter', 'je', 'veux', 'pas', 'en', 'savoir', 'sasuke', 'j', 'sais', 'pas', 'qui', 'c', 'est', 'mais', 'j', 'crois', 'il', 'est', 'sombre'], ['√†', 'partir', 'de', 'quand', 'peut', 'on', 'se', 'qualifier', 'de', 'fan'], ['go', 'profit√©', 'de', 'les', '6h20', 'de', 'sommeil', 'qu', 'il', 'me', 'reste']]\n",
      "[['il', 'd', 'la', 'de', 'l'], ['et', 'je', 't', 'en', 'j'], ['il', 'est'], ['la', 'qui', 'a', 'de', 'les', 'mais', 'j', 'ai', 'm√™me', 'pas', 'de', 'üò≠'], ['en', 'tout', 'la', 'a', 'bien'], ['le', 'est', 'un', 'de'], ['et', 'un', 'l', '√†'], ['je', 'pas', 'en', 'j', 'pas', 'qui', 'c', 'est', 'mais', 'j', 'il', 'est'], ['√†', 'de', 'on', 'de'], ['de', 'les', 'de', 'qu', 'il', 'me']]\n"
     ]
    }
   ],
   "source": [
    "def calcul_proba(x):\n",
    "    result = (sqrt(x)+1)*(1/x)\n",
    "    if(result > 1):\n",
    "        result = 1\n",
    "    return(result)\n",
    "\n",
    "def sub_sampling(phrases, probabilities):\n",
    "    #On g√©n√®re len(probabilities) loi uniforme sur [0,1]\n",
    "    runif = np.random.uniform(low=0.0, high=1.0, size=len(probabilities))\n",
    "    # Les mots √† supprimer sont les mots tels que la loi g√©n√©r√©e soit > proba\n",
    "    mots_a_supprimer = {w for (i, w) in enumerate(vocabulary) if probabilities[i] > runif[i]}\n",
    "    nouveau_corpus = [] \n",
    "\n",
    "    for phrase in phrases: #on parcourt tous les articles du corpus\n",
    "        nouveau_corpus.append([]) #on cr√©e une sous liste √† chaque nouvel article\n",
    "        for word in phrase: #et pour tous les mots de l'article\n",
    "            if word not in mots_a_supprimer:\n",
    "                nouveau_corpus[-1].append(word)\n",
    "    return(nouveau_corpus)\n",
    "calcul_proba_v = np.vectorize(calcul_proba)\n",
    "\n",
    "sample = 0.001\n",
    "words = [item for sublist in phrases for item in sublist]\n",
    "fdist = nltk.FreqDist(words)\n",
    "vocabulary = set(words)\n",
    "proportion = np.array([(fdist[w]/ (len(words) * sample)) for w in vocabulary])\n",
    "    \n",
    "probabilities_subsampling = calcul_proba_v(proportion)\n",
    "probabilities_subsampling\n",
    "\n",
    "\n",
    "phrases2 = sub_sampling(phrases, probabilities_subsampling)\n",
    "print(phrases[0:10])\n",
    "print(phrases2[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G√©n√©ration de l'√©chantillon de test\n",
    "Comment on g√®re les doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques tests:\n",
    "window = 2\n",
    "i = 1\n",
    "i_contexte = list(range(max(i-window,0), min(i+window+1, len(phrases[1]))))\n",
    "i_contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a8218df4c511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtest_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#test_sample_w = create_sample_word(phrases, vocabulary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#test_sample_w[:10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-a8218df4c511>\u001b[0m in \u001b[0;36mcreate_sample\u001b[0;34m(phrases, vocabulary, window)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mi_contexte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi_contexte\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mindex_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mtest_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#words = [item for sublist in phrases2 for item in sublist]\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "def create_sample(phrases, vocabulary, window = 2):\n",
    "    test_sample = []\n",
    "    for phrase in phrases:\n",
    "        for i in list(range(0, len(phrase))):\n",
    "            index_i = vocabulary.index(phrase[i])\n",
    "            i_contexte = list(range(max(i-window,0), min(i+window+1, len(phrase))))\n",
    "            i_contexte.remove(i)\n",
    "            for j in i_contexte:\n",
    "                index_j = vocabulary.index(phrase[j])\n",
    "                test_sample.append([index_i, index_j])\n",
    "    return(test_sample)\n",
    "def create_sample_word(phrases, vocabulary, window = 2):\n",
    "    test_sample = []\n",
    "    for phrase in phrases:\n",
    "        for i in list(range(0, len(phrase))):\n",
    "            i_contexte = list(range(max(i-window,0), min(i+window+1, len(phrase))))\n",
    "            i_contexte.remove(i) # faire ici un tirage\n",
    "            for j in i_contexte:\n",
    "                test_sample.append([phrase[i], phrase[j]])\n",
    "    return(test_sample)\n",
    "\n",
    "test_sample = create_sample(phrases, vocabulary)\n",
    "test_sample_w = create_sample_word(phrases, vocabulary)\n",
    "test_sample_w[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "M1 = torch.rand(len(vocabulary), len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
