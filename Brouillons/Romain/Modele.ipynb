{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début d'implémentation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des données\n",
    "A réfléchir par la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 0 : on importe ce qui est nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Créer le vocabulaire à partir du corpus de phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'il est un roi',\n",
    "    'elle est une reine',\n",
    "    'il est un homme',\n",
    "    'elle est une femme',\n",
    "    'londres est la capitale du royaume uni',\n",
    "    \"berlin est la capitale de l allemagne\",\n",
    "    'paris est la capitale de la france',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['il', 'est', 'un', 'roi'],\n",
       " ['elle', 'est', 'une', 'reine'],\n",
       " ['il', 'est', 'un', 'homme'],\n",
       " ['elle', 'est', 'une', 'femme'],\n",
       " ['londres', 'est', 'la', 'capitale', 'du', 'royaume', 'uni'],\n",
       " ['berlin', 'est', 'la', 'capitale', 'de', 'l', 'allemagne'],\n",
       " ['paris', 'est', 'la', 'capitale', 'de', 'la', 'france']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokens = [phrase.split() for phrase in corpus]\n",
    "    return tokens\n",
    "\n",
    "t_corpus = tokenize(corpus)\n",
    "t_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['il',\n",
       " 'est',\n",
       " 'un',\n",
       " 'roi',\n",
       " 'elle',\n",
       " 'une',\n",
       " 'reine',\n",
       " 'homme',\n",
       " 'femme',\n",
       " 'londres',\n",
       " 'la',\n",
       " 'capitale',\n",
       " 'du',\n",
       " 'royaume',\n",
       " 'uni',\n",
       " 'berlin',\n",
       " 'de',\n",
       " 'l',\n",
       " 'allemagne',\n",
       " 'paris',\n",
       " 'france']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = []\n",
    "for phrase in t_corpus:\n",
    "    for mot in phrase:\n",
    "        if mot not in voc:\n",
    "            voc.append(mot)\n",
    "voc_size = len(voc)\n",
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : on s'intéresse aux mots centraux et aux contextes suivant taille de fenêtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_index = {w: index for (index, w) in enumerate(voc)}\n",
    "index_mot = {index: w for (index, w) in enumerate(voc)}\n",
    "\n",
    "taille_fenetre = 3\n",
    "index_pairs = []\n",
    "# On traite chaque phrase.\n",
    "for phrase in t_corpus:\n",
    "    indices = [mot_index[mot] for mot in phrase]\n",
    "    # On traite chaque mot comme un mot central\n",
    "    for center_word in range(len(indices)):\n",
    "        # Pour chaque fenetre possible\n",
    "        for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "            context_word = center_word + w\n",
    "            # On fait attention à ne pas sauter de phrases\n",
    "            if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "                continue\n",
    "            context_word_ind = indices[context_word]\n",
    "            index_pairs.append((indices[center_word], context_word_ind))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 0,  3],\n",
       "       [ 1,  0],\n",
       "       [ 1,  2],\n",
       "       [ 1,  3],\n",
       "       [ 2,  0],\n",
       "       [ 2,  1],\n",
       "       [ 2,  3],\n",
       "       [ 3,  0],\n",
       "       [ 3,  1],\n",
       "       [ 3,  2],\n",
       "       [ 4,  1],\n",
       "       [ 4,  5],\n",
       "       [ 4,  6],\n",
       "       [ 1,  4],\n",
       "       [ 1,  5],\n",
       "       [ 1,  6],\n",
       "       [ 5,  4],\n",
       "       [ 5,  1],\n",
       "       [ 5,  6],\n",
       "       [ 6,  4],\n",
       "       [ 6,  1],\n",
       "       [ 6,  5],\n",
       "       [ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 0,  7],\n",
       "       [ 1,  0],\n",
       "       [ 1,  2],\n",
       "       [ 1,  7],\n",
       "       [ 2,  0],\n",
       "       [ 2,  1],\n",
       "       [ 2,  7],\n",
       "       [ 7,  0],\n",
       "       [ 7,  1],\n",
       "       [ 7,  2],\n",
       "       [ 4,  1],\n",
       "       [ 4,  5],\n",
       "       [ 4,  8],\n",
       "       [ 1,  4],\n",
       "       [ 1,  5],\n",
       "       [ 1,  8],\n",
       "       [ 5,  4],\n",
       "       [ 5,  1],\n",
       "       [ 5,  8],\n",
       "       [ 8,  4],\n",
       "       [ 8,  1],\n",
       "       [ 8,  5],\n",
       "       [ 9,  1],\n",
       "       [ 9, 10],\n",
       "       [ 9, 11],\n",
       "       [ 1,  9],\n",
       "       [ 1, 10],\n",
       "       [ 1, 11],\n",
       "       [ 1, 12],\n",
       "       [10,  9],\n",
       "       [10,  1],\n",
       "       [10, 11],\n",
       "       [10, 12],\n",
       "       [10, 13],\n",
       "       [11,  9],\n",
       "       [11,  1],\n",
       "       [11, 10],\n",
       "       [11, 12],\n",
       "       [11, 13],\n",
       "       [11, 14],\n",
       "       [12,  1],\n",
       "       [12, 10],\n",
       "       [12, 11],\n",
       "       [12, 13],\n",
       "       [12, 14],\n",
       "       [13, 10],\n",
       "       [13, 11],\n",
       "       [13, 12],\n",
       "       [13, 14],\n",
       "       [14, 11],\n",
       "       [14, 12],\n",
       "       [14, 13],\n",
       "       [15,  1],\n",
       "       [15, 10],\n",
       "       [15, 11],\n",
       "       [ 1, 15],\n",
       "       [ 1, 10],\n",
       "       [ 1, 11],\n",
       "       [ 1, 16],\n",
       "       [10, 15],\n",
       "       [10,  1],\n",
       "       [10, 11],\n",
       "       [10, 16],\n",
       "       [10, 17],\n",
       "       [11, 15],\n",
       "       [11,  1],\n",
       "       [11, 10],\n",
       "       [11, 16],\n",
       "       [11, 17],\n",
       "       [11, 18],\n",
       "       [16,  1],\n",
       "       [16, 10],\n",
       "       [16, 11],\n",
       "       [16, 17],\n",
       "       [16, 18],\n",
       "       [17, 10],\n",
       "       [17, 11],\n",
       "       [17, 16],\n",
       "       [17, 18],\n",
       "       [18, 11],\n",
       "       [18, 16],\n",
       "       [18, 17],\n",
       "       [19,  1],\n",
       "       [19, 10],\n",
       "       [19, 11],\n",
       "       [ 1, 19],\n",
       "       [ 1, 10],\n",
       "       [ 1, 11],\n",
       "       [ 1, 16],\n",
       "       [10, 19],\n",
       "       [10,  1],\n",
       "       [10, 11],\n",
       "       [10, 16],\n",
       "       [10, 10],\n",
       "       [11, 19],\n",
       "       [11,  1],\n",
       "       [11, 10],\n",
       "       [11, 16],\n",
       "       [11, 10],\n",
       "       [11, 20],\n",
       "       [16,  1],\n",
       "       [16, 10],\n",
       "       [16, 11],\n",
       "       [16, 10],\n",
       "       [16, 20],\n",
       "       [10, 10],\n",
       "       [10, 11],\n",
       "       [10, 16],\n",
       "       [10, 20],\n",
       "       [20, 11],\n",
       "       [20, 16],\n",
       "       [20, 10]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_pairs_np = np.array(index_pairs)\n",
    "index_pairs_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(voc_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "embedding_dims = 10\n",
    "W1 = Variable(torch.randn(embedding_dims, voc_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(voc_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in index_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data\n",
    "        \n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4947,  0.3315, -1.2877, -1.5805,  0.9756, -1.2312,  0.3168, -0.7427,\n",
       "          0.3270,  0.4944, -0.0674,  0.2432,  0.5014,  0.4188,  0.2039, -1.8357,\n",
       "         -0.9471,  0.1982,  0.9237, -1.1309,  1.3950],\n",
       "        [-0.3704,  0.5868,  1.1734,  0.9662,  0.2670,  1.5990,  0.7265,  0.6291,\n",
       "         -0.5157,  0.1331,  1.1935, -0.2927, -0.6029,  0.5952,  1.1235,  0.5484,\n",
       "         -1.7887,  1.0046,  1.1069, -0.7145,  0.3478],\n",
       "        [ 0.5176, -0.2069,  1.7235, -0.2625,  1.0030,  0.8355,  0.3853, -0.6875,\n",
       "         -0.1186,  0.5999,  0.5016,  0.6227, -0.1554,  0.1733,  0.8219, -0.2846,\n",
       "         -0.5204, -0.1643, -0.1752,  0.0122,  0.0879],\n",
       "        [ 0.2715, -0.9451,  0.0382,  0.8749, -0.7978, -0.0213, -0.3202, -1.4086,\n",
       "         -0.5324, -1.3127, -0.0517, -0.2834,  0.4801, -1.6827, -0.8192, -0.2363,\n",
       "         -0.2977, -0.7848, -1.2861, -0.1196, -1.0171],\n",
       "        [-0.6902, -1.3185,  2.3124,  0.1529,  0.3183, -0.4200, -0.4375,  0.0459,\n",
       "          0.6620, -1.1570, -1.2753, -0.0809, -0.0140, -1.5285, -0.2183, -1.2372,\n",
       "         -0.3015, -0.7190, -0.1785, -2.2582, -0.5817],\n",
       "        [-0.2512,  0.4628,  0.4565, -0.1837,  0.7967,  0.4890, -1.1316,  0.9833,\n",
       "         -0.3598,  0.9778, -0.5789, -0.1018, -1.2056,  1.3962, -0.1650, -1.3438,\n",
       "         -0.1439,  0.3033,  0.7959, -0.4613,  0.2376],\n",
       "        [-0.8638,  1.4106, -0.7106,  0.9906,  1.1300, -0.0693, -0.6287, -0.5336,\n",
       "         -0.8820, -1.0350, -0.5838,  1.0769,  0.1161, -1.2621, -0.4200,  0.0291,\n",
       "          1.1705,  0.0080,  0.1254, -0.7805, -0.4893],\n",
       "        [ 1.6570, -0.5480, -1.3056, -0.7439, -0.3754, -0.2273, -0.8590, -0.6811,\n",
       "         -1.1542, -0.7118, -0.8033, -0.1592,  1.7346, -0.3913,  0.6674,  0.1071,\n",
       "          0.8926,  1.3726, -0.5385, -0.3475, -1.7580],\n",
       "        [ 0.6991, -1.2441,  2.1011,  0.4044,  1.1432, -0.0030, -0.6425,  0.4063,\n",
       "          0.4419, -0.2549,  0.1074,  0.3296,  0.6773, -1.1724, -1.1405,  0.3128,\n",
       "          0.6980, -1.5692,  0.1747, -0.9318,  0.0490],\n",
       "        [-0.4191,  0.0736,  1.0151,  0.6290, -0.4457, -1.2149,  0.2558,  0.6855,\n",
       "          0.2653,  0.4077, -0.6408, -0.0833,  0.6567,  0.3814, -0.4808,  0.6638,\n",
       "         -1.4324,  0.0675, -0.0432,  0.0253, -0.7722]], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0563, -0.6103, -0.3536,  0.7024, -0.6107,  0.9776,  0.1286, -2.1149,\n",
       "          1.1871,  1.6894],\n",
       "        [-1.5432, -0.0383,  0.5483, -0.4739, -1.2686,  0.5057, -0.2272,  0.1025,\n",
       "          1.8997,  0.6612],\n",
       "        [ 0.4759, -0.6195, -0.3552, -0.2747, -1.7599,  1.4184, -0.4935,  0.6567,\n",
       "          1.8008,  1.1913],\n",
       "        [-1.3825, -0.5304,  1.8013,  0.4008,  1.4296, -1.0825, -0.2343,  0.8421,\n",
       "         -1.0614,  0.0057],\n",
       "        [-0.1359,  0.0525,  0.0738, -0.3734,  0.3258, -0.4658, -0.4858, -1.1292,\n",
       "         -1.1494,  0.1789],\n",
       "        [ 0.5469, -0.3780, -0.8927,  0.0305,  0.3106,  0.2116, -0.3787, -0.6493,\n",
       "         -0.4919, -0.3775],\n",
       "        [ 1.6229, -0.0247, -0.0207,  1.6323, -0.2777,  1.3059, -0.6560,  0.6080,\n",
       "         -0.8605, -2.1835],\n",
       "        [ 0.0979, -1.8978,  1.1006, -2.3466,  1.8410,  0.7941, -0.6344,  0.2943,\n",
       "         -1.0345,  0.4165],\n",
       "        [ 0.9177, -0.2124,  0.7497,  0.9389, -0.7027,  1.1617,  0.1121, -0.3598,\n",
       "          0.5035, -0.1728],\n",
       "        [ 0.4171,  0.8769, -0.3796,  0.0638, -0.1480, -0.6409,  0.8469,  0.1205,\n",
       "          1.2016, -0.2074],\n",
       "        [ 0.8802, -1.4868, -0.5330, -1.3423, -0.2686,  0.5951, -0.1618, -0.3892,\n",
       "         -0.3273, -0.2623],\n",
       "        [-0.4410, -0.2412, -0.6006, -2.4527, -0.7985, -1.1924, -1.1512,  0.1321,\n",
       "          0.2816, -0.6692],\n",
       "        [-0.3418, -0.3480, -1.0902, -0.0544,  0.3938, -0.7022, -1.2450, -0.5949,\n",
       "         -1.0433,  0.9260],\n",
       "        [-0.6005, -0.1279,  1.2765, -0.6397, -0.8605,  0.8670, -0.2171,  0.8889,\n",
       "          0.5223,  0.6536],\n",
       "        [ 0.5159, -0.0523, -1.2365, -1.5510, -0.0107, -1.1840,  0.1981, -0.4415,\n",
       "          1.9112,  1.3705],\n",
       "        [ 1.0832,  0.2902, -0.6411,  0.8291, -0.1627,  1.4943,  0.1539,  0.9495,\n",
       "          0.1468, -1.0862],\n",
       "        [ 0.8546,  0.2992,  0.2327,  0.3392,  0.3550, -0.2792, -0.4614, -1.0350,\n",
       "         -1.5396, -0.8782],\n",
       "        [-0.3868,  0.7989, -0.5073,  0.1382,  0.7064,  0.0264, -0.6432,  0.0911,\n",
       "          0.2614, -0.6311],\n",
       "        [-0.6364,  0.2890, -0.1509, -0.3058,  0.2615,  1.2186,  0.0856,  0.5737,\n",
       "          0.4909,  0.2406],\n",
       "        [ 1.8297, -0.2448, -1.4866,  0.2672,  0.0674,  1.2611, -0.6392, -0.2331,\n",
       "         -0.1736,  1.2262],\n",
       "        [-0.7037, -0.1510,  1.1047, -0.7069,  0.5989, -0.5131,  0.2165,  0.4729,\n",
       "         -0.7555, -0.3867]], requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(u, v):\n",
    "    return (np.dot(u, v)  / (math.sqrt(np.dot(u, u))) *  (math.sqrt(np.dot(v, v))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
