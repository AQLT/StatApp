{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début d'implémentation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 0 : Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import *\n",
    "from numpy.random import multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/torna/Documents/StatApp/StatApp/data/sample1.txt\",sep='\\n',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Créer le vocabulaire à partir du corpus de phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[0:100]\n",
    "\n",
    "corpus = []\n",
    "for index, row in df2.iterrows():\n",
    "    for j, column in row.iteritems():\n",
    "        corpus.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_corr = []\n",
    "\n",
    "for phrase in corpus:\n",
    "    # Suppression de la ponctuation\n",
    "    phrase = phrase.replace(\"?\",\"\")\n",
    "    phrase = phrase.replace(\".\",\"\")\n",
    "    phrase = phrase.replace(\"!\",\"\")\n",
    "    phrase = phrase.replace(\";\",\"\")\n",
    "    phrase = phrase.replace(\",\",\"\")\n",
    "    phrase = phrase.replace(\":\",\"\")\n",
    "    phrase = phrase.replace(\"#\",\"\")\n",
    "    # On met tout en minuscule\n",
    "    phrase = phrase.lower()\n",
    "    # On ajoute la phrase\n",
    "    corpus_corr.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokens = [phrase.split() for phrase in corpus]\n",
    "    return tokens\n",
    "\n",
    "t_corpus = tokenize(corpus_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les mentions @nicknames\n",
    "corpus_ok = []\n",
    "for phrase in t_corpus:\n",
    "    phrase_bis = []\n",
    "    for mot in phrase:\n",
    "        if mot[0] == '@':\n",
    "            mot = \"nickname\"\n",
    "        phrase_bis.append(mot)\n",
    "    if len(phrase) >= 5:\n",
    "        corpus_ok.append(phrase_bis)\n",
    "t_corpus = corpus_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505\n"
     ]
    }
   ],
   "source": [
    "voc = []\n",
    "freqs = {}\n",
    "for phrase in t_corpus:\n",
    "    for mot in phrase:\n",
    "        if mot not in voc:\n",
    "            voc.append(mot)\n",
    "            freqs[mot] = 1\n",
    "        else:\n",
    "            freqs[mot] +=1\n",
    "voc_size = len(voc)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calcul des probas pour le subsampling et le negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mots = 0\n",
    "for phrase in t_corpus:\n",
    "    total_mots += len(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in freqs.items():\n",
    "    freqs[key] = value / total_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilité d'être gardé dans le subsampling\n",
    "p_sub = {word: min((math.sqrt(freqs[word]/0.001)+1)*(0.001/freqs[word]),1) for word in freqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_neg_1 = {word: freqs[word]**(3/4) for word in freqs}\n",
    "total_neg = 0\n",
    "for word in p_neg_1:\n",
    "    total_neg+=p_neg_1[word]\n",
    "p_neg = {word: p_neg_1[word]/total_neg for word in p_neg_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subSampling(phrase):\n",
    "    phrase_samp = []\n",
    "    for mot in phrase:\n",
    "        if np.random.random() < (p_sub[mot]):\n",
    "                phrase_samp.append(mot)\n",
    "    return phrase_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_index = {w: index for (index, w) in enumerate(voc)}\n",
    "index_mot = {index: w for (index, w) in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Créations pairs mots centraux / contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#taille_fenetre = 4\n",
    "#index_pairs = []\n",
    "# On traite chaque phrase.\n",
    "#for phrase in t_corpus:\n",
    " #   indices = [mot_index[mot] for mot in phrase]\n",
    "    # On traite chaque mot comme un mot central\n",
    "   # for center_word in range(len(indices)):\n",
    "       # Pour chaque fenetre possible\n",
    "       # for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "      #      context_word = center_word + w\n",
    "            # On fait attention à ne pas sauter de phrases\n",
    "     #       if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "    #            continue\n",
    "   #         context_word_ind = indices[context_word]\n",
    "  #          index_pairs.append((indices[center_word], context_word_ind))\n",
    "            \n",
    "#index_pairs_np = np.array(index_pairs)\n",
    "#index_pairs_np[0:150]            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss à l'époque 0: 924.6293334960938\n",
      "Loss à l'époque 10: 965.7247924804688\n",
      "Loss à l'époque 20: 838.9000854492188\n",
      "Loss à l'époque 30: 815.5311279296875\n",
      "Loss à l'époque 40: 772.3552856445312\n",
      "Loss à l'époque 50: 801.9276733398438\n",
      "Loss à l'époque 60: 785.9378051757812\n",
      "Loss à l'époque 70: 780.1234130859375\n",
      "Loss à l'époque 80: 721.62939453125\n",
      "Loss à l'époque 90: 742.2842407226562\n"
     ]
    }
   ],
   "source": [
    "#Couche d'entrée\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(voc_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# Choix de dimension\n",
    "embedding_dims = 10\n",
    "# Initialisation\n",
    "# Variable : comme Tensor mais avec les valeurs qui changent pendant le traitement\n",
    "W1 = Variable(torch.randn(embedding_dims, voc_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(voc_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 100 # \"époques\"\n",
    "learning_rate = 0.01\n",
    "taille_fenetre = 5\n",
    "\n",
    "\n",
    "# Différentes étapes\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for phrase in t_corpus:\n",
    "        # Sub-sampling : pour chaque phrase, on réalise le subsampling éventuel.\n",
    "        phrase_samp = subSampling(phrase)\n",
    "        # Ensuite, on choisit un mot focus/contexte au hasard\n",
    "        \n",
    "        # On crée tous les couples par phrase\n",
    "        index_pairs = []\n",
    "        indices = [mot_index[mot] for mot in phrase_samp]\n",
    "        # On traite chaque mot comme un mot central\n",
    "        for center_word in range(len(indices)):\n",
    "        # Pour chaque fenetre possible\n",
    "            for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "                context_word = center_word + w\n",
    "                # On fait attention à ne pas sauter de phrases\n",
    "                if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "                    continue\n",
    "                context_word_ind = indices[context_word]\n",
    "                index_pairs.append((indices[center_word], context_word_ind))\n",
    "                \n",
    "        # On en choisit une\n",
    "        focus, context = choice(index_pairs)\n",
    "        \n",
    "        # Negative samples\n",
    "        sampled_index = np.array(multinomial(4, list(p_neg.values())))\n",
    "        word_list = []\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                 word_list.append(index)\n",
    "\n",
    "        \n",
    "        x = Variable(get_input_layer(focus)).float()\n",
    "        y = Variable(torch.from_numpy(np.array([context])).long())\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "               \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        # nll_loss(pred/target) - negative log likehood\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y)\n",
    "        loss_val += loss.data\n",
    "\n",
    "        # Propagation - revoir Pytorch.optimization\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "    if epo%10==0:\n",
    "        print(f\"Loss à l'époque {epo}: {loss_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Différents problèmes :\n",
    "# - la loss ne diminue pas toujours (mais la tendance est OK)\n",
    "# - revoir la partie sélection du couple, parfois bug ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5412,  0.0038,  0.0387, -1.5949,  1.5272,  0.8823, -0.6790, -0.7588,\n",
       "         -0.0372, -0.8157],\n",
       "        [-0.5245,  0.3148, -0.4191, -1.4192, -0.3776,  0.5609,  0.1891, -0.2417,\n",
       "          1.6929,  0.3959],\n",
       "        [-0.1487, -1.3347,  0.3655, -0.3361, -1.6402,  0.5023, -1.7574,  0.4513,\n",
       "          0.3055, -0.7681],\n",
       "        [ 1.2889,  1.3248, -0.2384, -1.1272, -0.1775, -0.7825,  0.1188,  0.1040,\n",
       "          0.0136, -1.5974],\n",
       "        [-0.5304,  0.8723, -0.0518,  1.1733, -0.7015,  0.6484, -1.2254, -0.1206,\n",
       "          0.1947, -0.1241],\n",
       "        [-0.4542,  0.3151,  0.2092,  0.1911, -0.1548, -1.5418,  0.7176, -0.2667,\n",
       "          0.5463, -1.1887],\n",
       "        [-0.6697, -0.0636,  0.9601,  0.3905, -0.6237, -0.7213, -0.0601, -0.5279,\n",
       "          0.8414, -0.1267],\n",
       "        [ 0.5183,  0.6076,  0.9916, -0.4725, -0.5053,  1.1060, -0.4358,  1.3260,\n",
       "         -1.2716,  0.5337],\n",
       "        [ 1.0686, -1.2510,  0.1488, -0.5754,  0.9399, -1.9468,  0.9206,  1.1858,\n",
       "          1.0060, -1.0008],\n",
       "        [-1.1071,  0.1962, -0.3748,  0.3867, -0.1931,  0.8531,  0.6101, -0.8743,\n",
       "         -0.6412, -1.4110]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance/similarité cosinus\n",
    "def cos_distance(u, v):\n",
    "    return (np.dot(u, v)  / (math.sqrt(np.dot(u, u)) *  (math.sqrt(np.dot(v, v)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des poids\n",
    "mot_poids = {index_mot[index]: poids.detach().numpy() for (index, poids) in enumerate(W2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 5 : Résultats du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mot_plus_proche(word, n=5):\n",
    "    word_distance = {}\n",
    "    for mot in mot_poids:\n",
    "        if mot != word:\n",
    "            word_distance[mot] = (cos_distance(mot_poids[mot],(mot_poids[word])))\n",
    "    word_distance = sorted(word_distance.items(), key=lambda t: t[1],reverse=True)\n",
    "    return word_distance[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pioche', 0.7683040038002439),\n",
       " ('seconde', 0.7341993959590819),\n",
       " ('jsuis', 0.676605263948548),\n",
       " ('»', 0.6594659518233653),\n",
       " ('que', 0.654054055958198),\n",
       " ('best', 0.6465407881263392),\n",
       " ('quand', 0.6366425112567645),\n",
       " ('applis', 0.6221293993377353),\n",
       " ('bien', 0.6157058808791527),\n",
       " ('pu', 0.6060293637744669)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot_poids\n",
    "mot_plus_proche(\"mort\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
