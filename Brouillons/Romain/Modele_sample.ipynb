{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début d'implémentation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 0 : Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/torna/Documents/StatApp/StatApp/data/sample1.txt\",sep='\\n',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Créer le vocabulaire à partir du corpus de phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[0:10]\n",
    "\n",
    "corpus = []\n",
    "for index, row in df2.iterrows():\n",
    "    for j, column in row.iteritems():\n",
    "        corpus.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_corr = []\n",
    "\n",
    "for phrase in corpus:\n",
    "    # Suppression de la ponctuation\n",
    "    phrase = phrase.replace(\"?\",\"\")\n",
    "    phrase = phrase.replace(\".\",\"\")\n",
    "    phrase = phrase.replace(\"!\",\"\")\n",
    "    phrase = phrase.replace(\";\",\"\")\n",
    "    phrase = phrase.replace(\",\",\"\")\n",
    "    phrase = phrase.replace(\":\",\"\")\n",
    "    # On met tout en minuscule\n",
    "    phrase = phrase.lower()\n",
    "    # On ajoute la phrase\n",
    "    corpus_corr.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokens = [phrase.split() for phrase in corpus]\n",
    "    return tokens\n",
    "\n",
    "t_corpus = tokenize(corpus_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les mentions @nicknames\n",
    "for phrase in t_corpus:\n",
    "    for mot in phrase:\n",
    "        if mot[0] == '@':\n",
    "            phrase.remove(mot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "voc = []\n",
    "freqs = {}\n",
    "for phrase in t_corpus:\n",
    "    for mot in phrase:\n",
    "        if mot not in voc:\n",
    "            voc.append(mot)\n",
    "            freqs[mot] = 1\n",
    "        else:\n",
    "            freqs[mot] +=1\n",
    "voc_size = len(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sub-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mots = 0\n",
    "for phrase in t_corpus:\n",
    "    total_mots += len(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in freqs.items():\n",
    "    freqs[key] = value / total_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop = {word: min((math.sqrt(freqs[word]/0.001)+1)*(0.001/freqs[word]),1) for word in freqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [word for word in voc if np.random.random() < (p_drop[word])]\n",
    "voc_size = len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_samp = []\n",
    "for phrase in t_corpus:\n",
    "    phrase_samp = []\n",
    "    for mot in phrase:\n",
    "        if mot in train_words:\n",
    "            phrase_samp.append(mot)\n",
    "    corpus_samp.append(phrase_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Créations pairs mots centraux / contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_index = {w: index for (index, w) in enumerate(train_words)}\n",
    "index_mot = {index: w for (index, w) in enumerate(train_words)}\n",
    "\n",
    "taille_fenetre = 4\n",
    "index_pairs = []\n",
    "# On traite chaque phrase.\n",
    "for phrase in corpus_samp:\n",
    "    indices = [mot_index[mot] for mot in phrase]\n",
    "    # On traite chaque mot comme un mot central\n",
    "    for center_word in range(len(indices)):\n",
    "        # Pour chaque fenetre possible\n",
    "        for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "            context_word = center_word + w\n",
    "            # On fait attention à ne pas sauter de phrases\n",
    "            if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "                continue\n",
    "            context_word_ind = indices[context_word]\n",
    "            index_pairs.append((indices[center_word], context_word_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pairs_np = np.array(index_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Couche d'entrée\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(voc_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# Choix de dimension\n",
    "embedding_dims = 10\n",
    "# Initialisation\n",
    "# Variable : comme Tensor mais avec les valeurs qui changent pendant le traitement\n",
    "W1 = Variable(torch.randn(embedding_dims, voc_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(voc_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 5 # \"époques\"\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Différentes étapes\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for word, context in index_pairs:\n",
    "        x = Variable(get_input_layer(word)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([context])).long())\n",
    "\n",
    "        # Matmul = produits matriciels de deux tensors\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        # Calcul softmax\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "        \n",
    "        # nll_loss(pred/target) - negative log likehood\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data\n",
    "        \n",
    "        # Propagation\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0504,  0.6502,  0.4829, -0.3945,  0.0564,  0.4515,  2.2502,  0.5078,\n",
       "         -0.0333,  0.4068],\n",
       "        [-0.4991, -0.0359, -0.0813,  0.6033, -0.2522,  1.1190,  0.6002,  0.7018,\n",
       "          0.1371, -0.9672],\n",
       "        [-0.2307, -1.7345, -0.4785, -0.5673,  1.2067,  0.5514,  1.3583, -0.5170,\n",
       "          0.4886, -0.0573],\n",
       "        [ 0.8444, -0.8946, -1.5188, -0.1301,  1.5705,  0.2099,  0.0330, -1.9745,\n",
       "         -0.4497, -1.2978],\n",
       "        [ 1.6875,  1.2736, -0.5157,  1.1318,  0.8934,  0.4470,  0.5132,  0.4536,\n",
       "          0.3562,  0.1779],\n",
       "        [ 0.4951, -1.1750, -0.8675, -0.0324, -0.8883, -1.5926, -1.4009, -1.2481,\n",
       "         -1.3336,  0.2735],\n",
       "        [-1.0025, -1.3701,  1.0445, -0.0246,  0.7833, -0.9320, -0.6022, -0.1857,\n",
       "          1.7320,  0.6987],\n",
       "        [ 1.6818, -1.3460, -1.1980, -0.2698, -0.8087,  1.0260, -0.0188,  0.3060,\n",
       "          0.7526,  0.5768],\n",
       "        [ 0.6953,  0.0922,  0.3825,  0.9420,  0.3644,  0.6338,  0.2019, -0.7270,\n",
       "          1.6056, -0.9292],\n",
       "        [ 0.2215,  1.2444, -0.8942,  1.4688,  1.4808, -0.5880, -0.6112, -1.3414,\n",
       "         -0.7020,  0.7139]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance/similarité cosinus\n",
    "def cos_distance(u, v):\n",
    "    return (np.dot(u, v)  / (math.sqrt(np.dot(u, u)) *  (math.sqrt(np.dot(v, v)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des poids\n",
    "mot_poids = {index_mot[index]: poids.detach().numpy() for (index, poids) in enumerate(W2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 5 : Résultats du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mot_plus_proche(word, n=5):\n",
    "    word_distance = {}\n",
    "    for mot in mot_poids:\n",
    "        if mot != word:\n",
    "            word_distance[mot] = (cos_distance(mot_poids[mot],(mot_poids[word])))\n",
    "    word_distance = sorted(word_distance.items(), key=lambda t: t[1],reverse=True)\n",
    "    return word_distance[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
