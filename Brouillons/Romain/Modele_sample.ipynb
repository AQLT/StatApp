{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début d'implémentation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 0 : Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import *\n",
    "from numpy.random import multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/torna/Documents/StatApp/StatApp/data/sample1.txt\",sep='\\n',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Créer le vocabulaire à partir du corpus de phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[0:100]\n",
    "\n",
    "corpus = []\n",
    "for index, row in df2.iterrows():\n",
    "    for j, column in row.iteritems():\n",
    "        corpus.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_corr = []\n",
    "\n",
    "for phrase in corpus:\n",
    "    # Suppression de la ponctuation\n",
    "    phrase = phrase.replace(\"?\",\"\")\n",
    "    phrase = phrase.replace(\".\",\"\")\n",
    "    phrase = phrase.replace(\"!\",\"\")\n",
    "    phrase = phrase.replace(\";\",\"\")\n",
    "    phrase = phrase.replace(\",\",\"\")\n",
    "    phrase = phrase.replace(\":\",\"\")\n",
    "    phrase = phrase.replace(\"#\",\"\")\n",
    "    # On met tout en minuscule\n",
    "    phrase = phrase.lower()\n",
    "    # On ajoute la phrase\n",
    "    corpus_corr.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokens = [phrase.split() for phrase in corpus]\n",
    "    return tokens\n",
    "\n",
    "t_corpus = tokenize(corpus_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les mentions @nicknames\n",
    "corpus_ok = []\n",
    "for phrase in t_corpus:\n",
    "    phrase_bis = []\n",
    "    for mot in phrase:\n",
    "        if mot[0] == '@':\n",
    "            mot = \"nickname\"\n",
    "        phrase_bis.append(mot)\n",
    "    if len(phrase) >= 5:\n",
    "        corpus_ok.append(phrase_bis)\n",
    "t_corpus = corpus_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505\n"
     ]
    }
   ],
   "source": [
    "voc = []\n",
    "freqs = {}\n",
    "for phrase in t_corpus:\n",
    "    for mot in phrase:\n",
    "        if mot not in voc:\n",
    "            voc.append(mot)\n",
    "            freqs[mot] = 1\n",
    "        else:\n",
    "            freqs[mot] +=1\n",
    "voc_size = len(voc)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calcul des probas pour le subsampling et le negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mots = 0\n",
    "for phrase in t_corpus:\n",
    "    total_mots += len(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in freqs.items():\n",
    "    freqs[key] = value / total_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilité d'être gardé dans le subsampling\n",
    "p_sub = {word: min((math.sqrt(freqs[word]/0.001)+1)*(0.001/freqs[word]),1) for word in freqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_neg_1 = {word: freqs[word]**(3/4) for word in freqs}\n",
    "total_neg = 0\n",
    "for word in p_neg_1:\n",
    "    total_neg+=p_neg_1[word]\n",
    "p_neg = {word: p_neg_1[word]/total_neg for word in p_neg_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subSampling(phrase):\n",
    "    phrase_samp = []\n",
    "    for mot in phrase:\n",
    "        if np.random.random() < (p_sub[mot]):\n",
    "                phrase_samp.append(mot)\n",
    "    return phrase_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_index = {w: index for (index, w) in enumerate(voc)}\n",
    "index_mot = {index: w for (index, w) in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Créations pairs mots centraux / contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#taille_fenetre = 4\n",
    "#index_pairs = []\n",
    "# On traite chaque phrase.\n",
    "#for phrase in t_corpus:\n",
    " #   indices = [mot_index[mot] for mot in phrase]\n",
    "    # On traite chaque mot comme un mot central\n",
    "   # for center_word in range(len(indices)):\n",
    "       # Pour chaque fenetre possible\n",
    "       # for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "      #      context_word = center_word + w\n",
    "            # On fait attention à ne pas sauter de phrases\n",
    "     #       if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "    #            continue\n",
    "   #         context_word_ind = indices[context_word]\n",
    "  #          index_pairs.append((indices[center_word], context_word_ind))\n",
    "            \n",
    "#index_pairs_np = np.array(index_pairs)\n",
    "#index_pairs_np[0:150]            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss à l'époque 0: 924.6293334960938\n",
      "Loss à l'époque 10: 965.7247924804688\n",
      "Loss à l'époque 20: 838.9000854492188\n",
      "Loss à l'époque 30: 815.5311279296875\n",
      "Loss à l'époque 40: 772.3552856445312\n",
      "Loss à l'époque 50: 801.9276733398438\n",
      "Loss à l'époque 60: 785.9378051757812\n",
      "Loss à l'époque 70: 780.1234130859375\n",
      "Loss à l'époque 80: 721.62939453125\n",
      "Loss à l'époque 90: 742.2842407226562\n"
     ]
    }
   ],
   "source": [
    "#Couche d'entrée\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(voc_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# Choix de dimension\n",
    "embedding_dims = 10\n",
    "# Initialisation\n",
    "# Variable : comme Tensor mais avec les valeurs qui changent pendant le traitement\n",
    "W1 = Variable(torch.randn(embedding_dims, voc_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(voc_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 100 # \"époques\"\n",
    "learning_rate = 0.01\n",
    "taille_fenetre = 5\n",
    "\n",
    "\n",
    "# Différentes étapes\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for phrase in t_corpus:\n",
    "        # Sub-sampling : pour chaque phrase, on réalise le subsampling éventuel.\n",
    "        phrase_samp = subSampling(phrase)\n",
    "        # Ensuite, on choisit un mot focus/contexte au hasard\n",
    "        \n",
    "        # On crée tous les couples par phrase\n",
    "        index_pairs = []\n",
    "        indices = [mot_index[mot] for mot in phrase_samp]\n",
    "        # On traite chaque mot comme un mot central\n",
    "        for center_word in range(len(indices)):\n",
    "        # Pour chaque fenetre possible\n",
    "            for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "                context_word = center_word + w\n",
    "                # On fait attention à ne pas sauter de phrases\n",
    "                if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "                    continue\n",
    "                context_word_ind = indices[context_word]\n",
    "                index_pairs.append((indices[center_word], context_word_ind))\n",
    "                \n",
    "        # On en choisit une\n",
    "        focus, context = choice(index_pairs)\n",
    "        \n",
    "        # Negative samples\n",
    "        sampled_index = np.array(multinomial(4, list(p_neg.values())))\n",
    "        word_list = []\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                 word_list.append(index)\n",
    "\n",
    "        \n",
    "        x = Variable(get_input_layer(focus)).float()\n",
    "        y = Variable(torch.from_numpy(np.array([context])).long())\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "               \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        # nll_loss(pred/target) - negative log likehood\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y)\n",
    "        loss_val += loss.data\n",
    "\n",
    "        # Propagation - revoir Pytorch.optimization\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "    if epo%10==0:\n",
    "        print(f\"Loss à l'époque {epo}: {loss_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Différents problèmes :\n",
    "# - la loss ne diminue pas toujours (mais la tendance est OK)\n",
    "# - revoir la partie sélection du couple, parfois bug ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3431,  1.1478, -0.3646,  0.9585,  1.2924,  1.7197,  1.7613, -0.4461,\n",
       "          0.1445, -1.4674],\n",
       "        [ 0.1458,  0.0233,  0.5180, -1.9255,  0.5275,  0.6784, -1.3668,  1.1555,\n",
       "          1.4334, -1.1032],\n",
       "        [-1.2393,  1.1643,  1.1756,  0.1937, -0.8903, -0.4984, -0.0095, -1.3632,\n",
       "          1.6272,  0.1972],\n",
       "        [ 1.3565,  0.7157,  0.9915, -0.1102, -0.4371, -0.7880,  0.3525, -0.4517,\n",
       "          0.5484, -0.7496],\n",
       "        [ 1.1653, -0.2500,  1.4164,  1.5448,  1.7698, -1.3863, -0.8301, -0.3855,\n",
       "          0.3140, -1.2120],\n",
       "        [-0.7621,  0.5651,  1.0514, -0.4918,  1.3155, -1.9175, -0.3335, -1.6094,\n",
       "          0.3318, -1.0806],\n",
       "        [ 0.6262,  0.4406,  1.9083,  0.1963,  0.8050,  0.2576, -1.3520,  0.6384,\n",
       "         -0.7337,  1.0205],\n",
       "        [-0.1517, -0.1985,  0.1403, -1.0160, -0.2920, -1.9189, -0.7851, -2.0339,\n",
       "          2.3004, -0.4903],\n",
       "        [-0.5881, -0.7291,  0.9962,  0.6894, -0.6274,  0.4354,  0.0166, -1.6822,\n",
       "          0.3377,  0.2643],\n",
       "        [ 0.2961, -0.5734, -0.2786, -0.6650, -2.0198,  1.5315,  0.4407,  0.0998,\n",
       "          0.6324, -0.1451]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance/similarité cosinus\n",
    "def cos_distance(u, v):\n",
    "    return (np.dot(u, v)  / (math.sqrt(np.dot(u, u)) *  (math.sqrt(np.dot(v, v)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des poids\n",
    "mot_poids = {index_mot[index]: poids.detach().numpy() for (index, poids) in enumerate(W2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 5 : Résultats du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mot_plus_proche(word, n=5):\n",
    "    word_distance = {}\n",
    "    for mot in mot_poids:\n",
    "        if mot != word:\n",
    "            word_distance[mot] = (cos_distance(mot_poids[mot],(mot_poids[word])))\n",
    "    word_distance = sorted(word_distance.items(), key=lambda t: t[1],reverse=True)\n",
    "    return word_distance[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rathalos', 0.7310312707276541),\n",
       " ('y', 0.7117773667606464),\n",
       " ('mtue', 0.6972553839008429),\n",
       " ('réveille', 0.6645014134124577),\n",
       " ('madre', 0.6638478961311802),\n",
       " ('😭', 0.6551260058122947),\n",
       " ('dodo', 0.6544916175622695),\n",
       " ('pense', 0.6393783723895966),\n",
       " ('glisse', 0.6249456152564727),\n",
       " ('✅', 0.6135785278345168)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot_poids\n",
    "mot_plus_proche(\"mort\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
