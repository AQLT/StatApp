{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début d'implémentation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 0 : Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "from random import *\n",
    "import random\n",
    "from random import randrange\n",
    "from random import sample \n",
    "\n",
    "from numpy.random import multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Créations du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_couples = {\n",
    "    0:(\"homme\",\"femme\"),\n",
    "    1:(\"chien\",\"chat\"),\n",
    "    2:(\"vanille\",\"chocolat\"),\n",
    "    3:(\"poli\",\"courtois\"),\n",
    "    4:(\"choqué\",\"horrifié\"),\n",
    "    5:(\"propre\",\"sale\"),\n",
    "    6:(\"canapé\",\"fauteuil\"),\n",
    "    7:(\"téléphone\",\"portable\"),\n",
    "    8:(\"voiture\",\"camion\"),\n",
    "    9:(\"grand\",\"petit\")\n",
    "}\n",
    "\n",
    "couples_contexte = {\n",
    "(\"homme\",\"femme\") : [\"sexe\",\"genre\",\"mari\",\"épouse\",\"famille\",\"inégalités\",\"enfant\",\"amour\",\"couple\",\"différence\"],                  \n",
    "(\"chien\",\"chat\") : [\"animal\",\"domestique\",\"maison\",\"aboyer\",\"miauler\",\"labrador\",\"poil\",\"pattes\",\"spa\",\"ami\"],\n",
    "(\"vanille\",\"chocolat\") : [\"parfum\",\"goût\",\"fraise\",\"glace\",\"boule\",\"gâteau\",\"bonbon\",\"blanc\",\"noir\",\"préférence\"],\n",
    "(\"poli\",\"courtois\") : [\"bonjour\",\"merci\",\"pardon\",\"stp\",\"règles\",\"apprendre\",\"savoir-vivre\",\"savoir-être\",\"formule\",\"demander\"],\n",
    "(\"choqué\",\"horrifié\") : [\"peur\",\"terreur\",\"phobie\",\"clown\",\"noir\",\"araignée\",\"rat\",\"cri\",\"oh\",\"ah\"],\n",
    "(\"propre\",\"sale\") : [\"nettoyer\",\"vaisselle\",\"laver\",\"savon\",\"produit\",\"salissures\",\"traces\",\"net\",\"éclatant\",\"brillant\"],\n",
    "(\"canapé\",\"fauteuil\") : [\"maison\",\"salon\",\"assis\",\"confortable\",\"sofa\",\"convertible\",\"cuir\",\"télé\",\"accoudoir\",\"plaid\"],\n",
    "(\"téléphone\",\"portable\") : [\"appel\",\"allo\",\"décrocher\",\"sms\",\"numéro\",\"mobile\",\"orange\",\"sfr\",\"free\",\"bouygues\"],\n",
    "(\"voiture\",\"camion\") : [\"moto\",\"véhicule\",\"conduire\",\"rouler\",\"conducteur\",\"volant\",\"chauffeur\",\"permis\",\"vitesse\",\"passager\"],\n",
    "(\"grand\",\"petit\") : [\"taille\",\"géant\",\"nain\",\"s\",\"m\",\"l\",\"xl\",\"mesure\",\"énorme\",\"longueur\"]\n",
    "}\n",
    "\n",
    "bruits = ['ALLATES','NEBULES','LONGTON','PAWESIN','SORENGO','SEXTEES','TAXIONS','ELEWIJT','EPEISME','APOTOME',\n",
    "'REMORVA','HOBOKEN','MALICES','SOMZEEN','NEDDOIS','RECLIVA','GILEREZ','GAULDOS','GOINFRA','RAYERES','BOTTEES','VAGITES','PRELEES','GARDERA','ANATASE','DATASSE',\n",
    "'BILLAGE','POUCAVE','REFUGES','REDIMAI','SOLANGE','EMBOISE','BACHAGA','FAMINER','ECUMONS','HARWELL','VEURDES','AZURERA','ENUQUAI','MAULAIS','MEVENIR','GAUDENT',\n",
    "'MENTANA','REHERSE','ARBECEY','FAUXAMI','BALADOU','REVERDI','BEDAVES','BORAMES','PONTIFE','ALESIEZ','REFIXER','IRRUIEZ','DRESSEZ','PYROIDE','GNAQUAI','LAVETON',\n",
    "'RERESTA','EPRISSE','FROLERA','RAVEURS','CITRINE','NAUNHOF','FEUTRIE','RETASEE','TIMBREE','GANTERA','HALENES','SUSIENS','DEGOUTE','BINAGES','VEAUTES','LAISSAI',\n",
    "'STERANE','RECERNE','RELIQUE','FLAGGER','DELAVAS','SUADERA','PINIERS','DOMPTER','CARRARE','ROSAZIA','LIMITAS','EUCLIDE','FARCANT','BLATERE','STRICTS','REFLOTS',\n",
    "'PAGELLE','BLAUZAC','BROCHER','TOPDOWN','UNIFERE','EHOUPER','ASININS','ASSAKIS','HOTELES','DENIOTE']\n",
    "\n",
    "i = 0\n",
    "for mot in bruits:\n",
    "    mot_min = mot.lower()\n",
    "    bruits[i] = mot_min\n",
    "    i += 1\n",
    "    \n",
    "t_corpus = []\n",
    "for i in range(10000):\n",
    "    indiceCouple = randrange(0,10)\n",
    "    membreCouple = randrange(0,2)\n",
    "    # On a notre mot clé\n",
    "    mot_cle = liste_couples[indiceCouple][membreCouple]\n",
    "    # On récupère les contextes, on va en tirer 5\n",
    "    liste_mot = couples_contexte[liste_couples[indiceCouple]]\n",
    "    ech_contexte = sample(liste_mot,5)\n",
    "    # On génère le bruit \n",
    "    ech_bruit = sample(bruits,3)\n",
    "    # On crée la phrase\n",
    "    phrase = []\n",
    "    phrase.append(mot_cle)\n",
    "    for mot in ech_contexte:\n",
    "        phrase.append(mot)\n",
    "    for bruit in ech_bruit:\n",
    "        phrase.append(bruit)\n",
    "    random.shuffle(phrase)\n",
    "    t_corpus.append(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Créer le vocabulaire à partir du corpus de phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "voc = []\n",
    "freqs = {}\n",
    "for phrase in t_corpus:\n",
    "    for mot in phrase:\n",
    "        if mot not in voc:\n",
    "            voc.append(mot)\n",
    "            freqs[mot] = 1\n",
    "        else:\n",
    "            freqs[mot] +=1\n",
    "voc_size = len(voc)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Calcul des probas pour le subsampling et le negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mots = 0\n",
    "for phrase in t_corpus:\n",
    "    total_mots += len(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in freqs.items():\n",
    "    freqs[key] = value / total_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilité d'être gardé dans le subsampling\n",
    "p_sub = {word: min((math.sqrt(freqs[word]/0.001)+1)*(0.001/freqs[word]),1) for word in freqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_neg_1 = {word: freqs[word]**(3/4) for word in freqs}\n",
    "total_neg = 0\n",
    "for word in p_neg_1:\n",
    "    total_neg+=p_neg_1[word]\n",
    "p_neg = {word: p_neg_1[word]/total_neg for word in p_neg_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subSampling(phrase):\n",
    "    phrase_samp = []\n",
    "    for mot in phrase:\n",
    "        if np.random.random() < (p_sub[mot]):\n",
    "                phrase_samp.append(mot)\n",
    "    return phrase_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_corpus_samp = []\n",
    "for phrase in t_corpus:\n",
    "        # Sub-sampling : pour chaque phrase, on réalise le subsampling éventuel.\n",
    "        phrase_samp = subSampling(phrase)\n",
    "        if len(phrase_samp)>1:\n",
    "            t_corpus_samp.append(phrase_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_index = {w: index for (index, w) in enumerate(voc)}\n",
    "index_mot = {index: w for (index, w) in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "def live_plot(data, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Couche d'entrée\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(voc_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# Choix de dimension\n",
    "embedding_dims = 10\n",
    "# Initialisation\n",
    "# Variable : comme Tensor mais avec les valeurs qui changent pendant le traitement\n",
    "W1 = Variable(torch.randn(embedding_dims, voc_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(voc_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 50 # \"époques\"\n",
    "learning_rate = 0.01\n",
    "taille_fenetre = 5\n",
    "loss_tot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focus : 5\n",
      "contexte :  6\n",
      "x : tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "[218]\n",
      "z1 :  tensor([ 0.6433,  0.1203,  0.3058,  0.2454,  0.0305, -0.1281,  0.7255,  0.2352,\n",
      "         0.1547, -0.2560], grad_fn=<MvBackward>)\n",
      "[10]\n",
      "vec_context : tensor([-0.5344,  0.5194,  0.1026, -0.0805,  0.8045, -0.5671,  0.6041, -1.3938,\n",
      "        -0.1720, -0.0047], grad_fn=<SelectBackward>)\n",
      "[10]\n",
      "z2 : tensor(-0.0874, grad_fn=<DotBackward>)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "epo = 0\n",
    "loss_val = 0\n",
    "for phrase in t_corpus_samp[0:1]: #ajout [0,1]\n",
    "        # On crée tous les couples par phrase\n",
    "        index_pairs = []\n",
    "        indices = [mot_index[mot] for mot in phrase]\n",
    "        # On traite chaque mot comme un mot central\n",
    "        for center_word in range(len(indices)):\n",
    "        # Pour chaque fenetre possible\n",
    "            for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "                context_word = center_word + w\n",
    "                # On fait attention à ne pas sauter de phrases\n",
    "                if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "                    continue\n",
    "                context_word_ind = indices[context_word]\n",
    "                index_pairs.append((indices[center_word], context_word_ind))\n",
    "        if len(index_pairs) > 0:\n",
    "            # On en choisit une\n",
    "            focus, context = choice(index_pairs)\n",
    "            print(\"focus :\", focus)\n",
    "            print(\"contexte : \", context)\n",
    "\n",
    "            # Calcul loss #DIFF\n",
    "            x = Variable(get_input_layer(focus)).float() # [218, 1] one-hot vector avec un 1 à la place du focus\n",
    "            print(\"x :\",x) #{218, 1] one-hot vector avec un 1 à la place du focus\n",
    "            print(list(x.size()))\n",
    "            #y = Variable(torch.from_numpy(np.array([context])).long())\n",
    "            #print(\"y :\",y) #[1, 1] vaut la place du contexte\n",
    "            z1 = torch.matmul(W1, x) # [10, 218] * [218, 1] = [10, 1]\n",
    "            print(\"z1 : \",z1)\n",
    "            vec_context = W2[context] # [1,10]\n",
    "            print(\"vec_context :\",vec_context)\n",
    "            z2 = torch.matmul(vec_context, z1) ) #[1, 10] * [10, 1] = [1, 1]\n",
    "            #z2 = torch.matmul(W2, z1) #[218, 10] * [10, 1] = [218, 1]\n",
    "            print(\"z2 :\", z2)\n",
    "\n",
    "            # soft max #DIFF\n",
    "            #log_softmax = F.log_softmax(z2, dim=0)\n",
    "            #loss = F.nll_loss(log_softmax.view(1,-1), y)\n",
    "            #loss_val += loss.data\n",
    "\n",
    "            # non soft max #DIFF\n",
    "            sigmoid = F.logsigmoid(z2)\n",
    "            loss1 =  -sigmoid\n",
    "            loss_val += loss1\n",
    "        \n",
    "            # Negative samples #DIFF\n",
    "            sampled_index = np.array(multinomial(4, list(p_neg.values())))\n",
    "            word_list = []\n",
    "            for index, count in enumerate(sampled_index):\n",
    "                for _ in range(count):\n",
    "                     word_list.append(index)\n",
    "\n",
    "            # Loss des negative sample\n",
    "            \n",
    "            vec_neg = W2[word_list]\n",
    "            z2 = torch.matmul(vec_neg, z1)\n",
    "            sigmoid = F.logsigmoid(-z2)\n",
    "            loss2 = -sigmoid.sum()\n",
    "            loss_val += loss2\n",
    "            \n",
    "            # Propagation\n",
    "            (loss1+loss2).backward() #DIFF\n",
    "            #loss.backward() #DIFF\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            W2.data -= learning_rate * W2.grad.data\n",
    "            W1.grad.data.zero_()\n",
    "            W2.grad.data.zero_()\n",
    "loss_tot.append(loss_val / voc_size)\n",
    "#live_plot(loss_tot)\n",
    "\n",
    "end = time.time()\n",
    "#print(round((end - start)/60, 2))\n",
    "\n",
    "#    if epo%10==0:\n",
    "#        print(f\"Loss à l'époque {epo}: {loss_val/voc_size}\")\n",
    "#print(f\"Loss à l'époque {num_epochs}: {loss_val/voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Différentes étapes\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for phrase in t_corpus_samp:\n",
    "        # On crée tous les couples par phrase\n",
    "        index_pairs = []\n",
    "        indices = [mot_index[mot] for mot in phrase]\n",
    "        # On traite chaque mot comme un mot central\n",
    "        for center_word in range(len(indices)):\n",
    "        # Pour chaque fenetre possible\n",
    "            for w in range(-taille_fenetre, taille_fenetre + 1):\n",
    "                context_word = center_word + w\n",
    "                # On fait attention à ne pas sauter de phrases\n",
    "                if context_word < 0 or context_word >= len(indices) or center_word == context_word:\n",
    "                    continue\n",
    "                context_word_ind = indices[context_word]\n",
    "                index_pairs.append((indices[center_word], context_word_ind))\n",
    "        if len(index_pairs) > 0:\n",
    "            # On en choisit une\n",
    "            focus, context = choice(index_pairs)\n",
    "\n",
    "            # Calcul loss \n",
    "            \n",
    "            # Loss du couple OK\n",
    "            x = Variable(get_input_layer(focus)).float()\n",
    "            z1 = torch.matmul(W1, x)\n",
    "            vec_context = W2[context]\n",
    "            z2 = torch.matmul(vec_context, z1)\n",
    "\n",
    "            sigmoid = F.logsigmoid(z2)\n",
    "            loss1 =  -sigmoid\n",
    "            loss_val += loss1\n",
    "        \n",
    "            # Negative samples\n",
    "            sampled_index = np.array(multinomial(4, list(p_neg.values())))\n",
    "            word_list = []\n",
    "            for index, count in enumerate(sampled_index):\n",
    "                for _ in range(count):\n",
    "                     word_list.append(index)\n",
    "\n",
    "            # Loss des negative sample\n",
    "            \n",
    "            vec_neg = W2[word_list]\n",
    "            z2 = torch.matmul(vec_neg, z1)\n",
    "            sigmoid = F.logsigmoid(-z2)\n",
    "            loss2 = -sigmoid.sum()\n",
    "            loss_val += loss2\n",
    "            \n",
    "            # Propagation\n",
    "            (loss1+loss2).backward()\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "            W1.grad.data.zero_()\n",
    "            W2.grad.data.zero_()\n",
    "    loss_tot.append(loss_val / voc_size)\n",
    "    live_plot(loss_tot)\n",
    "end = time.time()\n",
    "print(round((end - start)/60, 2))\n",
    "\n",
    "#    if epo%10==0:\n",
    "#        print(f\"Loss à l'époque {epo}: {loss_val/voc_size}\")\n",
    "#print(f\"Loss à l'époque {num_epochs}: {loss_val/voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = (W1.t() + W2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les poids - utile si traitement trop long\n",
    "with open (\"mat.txt\", \"w\") as f:\n",
    "    for i in range (0,len (W3)) :\n",
    "        for j in range (0, len (W3[i])) :\n",
    "            f.write ( str (W3[i][j]) + \"\\t\")\n",
    "        f.write (\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance/similarité cosinus\n",
    "def cos_distance(u, v):\n",
    "    return (np.dot(u, v)  / (math.sqrt(np.dot(u, u)) *  (math.sqrt(np.dot(v, v)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des poids\n",
    "mot_poids = {index_mot[index]: poids.detach().numpy() for (index, poids) in enumerate(W3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 5 : Résultats du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mot_plus_proche(word, n=10):\n",
    "    word_distance = {}\n",
    "    for mot in mot_poids:\n",
    "        if mot != word:\n",
    "            word_distance[mot] = (cos_distance(mot_poids[mot],(mot_poids[word])))\n",
    "    word_distance = sorted(word_distance.items(), key=lambda t: t[1],reverse=True)\n",
    "    return word_distance[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_plus_proche(\"grand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Données fictives - Construction fichier pour ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = [\"homme\",\"femme\",\"chien\",\"chat\",\"vanille\",\"chocolat\",\n",
    "    \"poli\",\"courtois\",\"choqué\",\"horrifié\",\"propre\",\"sale\",\"canapé\",\"fauteuil\",\n",
    "    \"téléphone\",\"portable\",\"voiture\",\"camion\",\"grand\",\"petit\"]\n",
    "\n",
    "df_res = pd.DataFrame.from_dict(mot_poids).transpose()\n",
    "df_res = df_res.loc[liste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['label'] = 'Autre' \n",
    "\n",
    "df_res.loc[\"homme\",'label'] = 'Couple 1'\n",
    "df_res.loc[\"femme\",'label'] = 'Couple 1'\n",
    "\n",
    "df_res.loc[\"chien\",'label'] = 'Couple 2'\n",
    "df_res.loc[\"chat\",'label'] = 'Couple 2'\n",
    "\n",
    "df_res.loc[\"vanille\",'label'] = 'Couple 3'\n",
    "df_res.loc[\"chocolat\",'label'] = 'Couple 3'\n",
    "\n",
    "df_res.loc[\"poli\",'label'] = 'Couple 4'\n",
    "df_res.loc[\"courtois\",'label'] = 'Couple 4'\n",
    "\n",
    "df_res.loc[\"choqué\",'label'] = 'Couple 5'\n",
    "df_res.loc[\"horrifié\",'label'] = 'Couple 5'\n",
    "\n",
    "df_res.loc[\"propre\",'label'] = 'Couple 6'\n",
    "df_res.loc[\"sale\",'label'] = 'Couple 6'\n",
    "\n",
    "df_res.loc[\"canapé\",'label'] = 'Couple 7'\n",
    "df_res.loc[\"fauteuil\",'label'] = 'Couple 7'\n",
    "\n",
    "df_res.loc[\"téléphone\",'label'] = 'Couple 8'\n",
    "df_res.loc[\"portable\",'label'] = 'Couple 8'\n",
    "\n",
    "df_res.loc[\"voiture\",'label'] = 'Couple 9'\n",
    "df_res.loc[\"camion\",'label'] = 'Couple 9'\n",
    "\n",
    "df_res.loc[\"grand\",'label'] = 'Couple 10'\n",
    "df_res.loc[\"petit\",'label'] = 'Couple 10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "features = [0, 1, 2, 3, 4, 5, 6, 7,8,9]\n",
    "# Separating out the features\n",
    "x = df_res.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = df_res.loc[:,['label']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['Axe 1', 'Axe 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['mot'] = df_res.index.values\n",
    "df_res = df_res.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, df_res[['label','mot']]],  axis = 1)\n",
    "#pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation simple statique \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Axe 1', fontsize = 15)\n",
    "ax.set_ylabel('Axe 2', fontsize = 15)\n",
    "ax.set_title('ACP en 2 composantes', fontsize = 20)\n",
    "targets = list(set(df_res['label']))\n",
    "colors = ['blue', 'red', 'green', 'purple', 'pink', 'orange', 'black', 'brown', 'grey', 'magenta']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['label'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'Axe 1']\n",
    "               , finalDf.loc[indicesToKeep, 'Axe 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation interactive \n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "#Travailler avec sous-table sinon trop lourd\n",
    "subFinalDf = pd.concat([finalDf[(finalDf['label'] != \"Autre\")],finalDf.iloc[1:100:]],  axis = 0)\n",
    "\n",
    "\n",
    "subFinalDf.iplot(kind='scatter',\n",
    "              mode='markers',\n",
    "              x='Axe 1',\n",
    "              y='Axe 2',\n",
    "              categories='label',\n",
    "              text='mot',\n",
    "              xTitle='Axe 1',\n",
    "              yTitle='Axe 2',\n",
    "              title='ACP en 2 composantes',\n",
    "              filename='cufflinks/simple-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "liste2 = [\"homme\",\"femme\",\"sexe\",\"genre\",\"mari\",\"épouse\",\"famille\",\"inégalités\",\"enfant\",\"amour\",\"couple\",\"différence\",                  \n",
    "\"chien\",\"chat\",\"animal\",\"domestique\",\"maison\",\"aboyer\",\"miauler\",\"labrador\",\"poil\",\"pattes\",\"spa\",\"ami\",\n",
    "\"vanille\",\"chocolat\",\"parfum\",\"goût\",\"fraise\",\"glace\",\"boule\",\"gâteau\",\"bonbon\",\"blanc\",\"noir\",\"préférence\",\n",
    "\"poli\",\"courtois\",\"bonjour\",\"merci\",\"pardon\",\"stp\",\"règles\",\"apprendre\",\"savoir-vivre\",\"savoir-être\",\"formule\",\"demander\",\n",
    "\"choqué\",\"horrifié\",\"peur\",\"terreur\",\"phobie\",\"clown\",\"noir\",\"araignée\",\"rat\",\"cri\",\"oh\",\"ah\",\n",
    "\"propre\",\"sale\",\"nettoyer\",\"vaisselle\",\"laver\",\"savon\",\"produit\",\"salissures\",\"traces\",\"net\",\"éclatant\",\"brillant\",\n",
    "\"canapé\",\"fauteuil\",\"maison\",\"salon\",\"assis\",\"confortable\",\"sofa\",\"convertible\",\"cuir\",\"télé\",\"accoudoir\",\"plaid\",\n",
    "\"téléphone\",\"portable\",\"appel\",\"allo\",\"décrocher\",\"sms\",\"numéro\",\"mobile\",\"orange\",\"sfr\",\"free\",\"bouygues\",\n",
    "\"voiture\",\"camion\",\"moto\",\"véhicule\",\"conduire\",\"rouler\",\"conducteur\",\"volant\",\"chauffeur\",\"permis\",\"vitesse\",\"passager\",\n",
    "\"grand\",\"petit\",\"taille\",\"géant\",\"nain\",\"s\",\"m\",\"l\",\"xl\",\"mesure\",\"énorme\",\"longueur\"\n",
    "]\n",
    "\n",
    "df_res2 = pd.DataFrame.from_dict(mot_poids).transpose()\n",
    "df_res2 = df_res2.loc[liste2]\n",
    "\n",
    "df_res2['label'] = 'Autre' \n",
    "\n",
    "df_res2.loc[\n",
    "    [\"homme\",\"femme\",\"sexe\",\"genre\",\"mari\",\"épouse\",\"famille\",\"inégalités\",\"enfant\",\"amour\",\"couple\",\"différence\"],\n",
    "    'label'] = \"Couple 1\"\n",
    "df_res2.loc[\n",
    "    [\"chien\",\"chat\",\"animal\",\"domestique\",\"maison\",\"aboyer\",\"miauler\",\"labrador\",\"poil\",\"pattes\",\"spa\",\"ami\"],\n",
    "    'label'] = \"Couple 2\"\n",
    "df_res2.loc[\n",
    "    [\"vanille\",\"chocolat\",\"parfum\",\"goût\",\"fraise\",\"glace\",\"boule\",\"gâteau\",\"bonbon\",\"blanc\",\"noir\",\"préférence\"],\n",
    "    'label'] = \"Couple 3\"\n",
    "df_res2.loc[\n",
    "    [\"poli\",\"courtois\",\"bonjour\",\"merci\",\"pardon\",\"stp\",\"règles\",\"apprendre\",\"savoir-vivre\",\"savoir-être\",\"formule\",\"demander\"],\n",
    "    'label'] = \"Couple 4\"\n",
    "df_res2.loc[\n",
    "    [\"choqué\",\"horrifié\",\"peur\",\"terreur\",\"phobie\",\"clown\",\"noir\",\"araignée\",\"rat\",\"cri\",\"oh\",\"ah\"],\n",
    "    'label'] = \"Couple 5\"\n",
    "df_res2.loc[\n",
    "    [\"propre\",\"sale\",\"nettoyer\",\"vaisselle\",\"laver\",\"savon\",\"produit\",\"salissures\",\"traces\",\"net\",\"éclatant\",\"brillant\"],\n",
    "    'label'] = \"Couple 6\"\n",
    "df_res2.loc[\n",
    "    [\"canapé\",\"fauteuil\",\"maison\",\"salon\",\"assis\",\"confortable\",\"sofa\",\"convertible\",\"cuir\",\"télé\",\"accoudoir\",\"plaid\"],\n",
    "    'label'] = \"Couple 7\"\n",
    "df_res2.loc[\n",
    "    [\"téléphone\",\"portable\",\"appel\",\"allo\",\"décrocher\",\"sms\",\"numéro\",\"mobile\",\"orange\",\"sfr\",\"free\",\"bouygues\"],\n",
    "    'label'] = \"Couple 8\"\n",
    "df_res2.loc[\n",
    "    [\"voiture\",\"camion\",\"moto\",\"véhicule\",\"conduire\",\"rouler\",\"conducteur\",\"volant\",\"chauffeur\",\"permis\",\"vitesse\",\"passager\"],\n",
    "    'label'] = \"Couple 9\"\n",
    "df_res2.loc[\n",
    "    [\"grand\",\"petit\",\"taille\",\"géant\",\"nain\",\"s\",\"m\",\"l\",\"xl\",\"mesure\",\"énorme\",\"longueur\"],\n",
    "    'label'] = \"Couple 10\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = [0, 1, 2, 3, 4, 5, 6, 7,8,9]\n",
    "# Separating out the features\n",
    "data_x = df_res2.loc[:, features].values\n",
    "# Separating out the target\n",
    "data_y = df_res2.loc[:,['label']].values\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_obj= tsne.fit_transform(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = [1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "                 2,2,2,2,2,2,2,2,2,2,2,2,\n",
    "                 3,3,3,3,3,3,3,3,3,3,3,3,\n",
    "                 4,4,4,4,4,4,4,4,4,4,4,4,\n",
    "                 5,5,5,5,5,5,5,5,5,5,5,5,\n",
    "                 6,6,6,6,6,6,6,6,6,6,6,6,\n",
    "                 7,7,7,7,7,7,7,7,7,7,7,7,\n",
    "                 8,8,8,8,8,8,8,8,8,8,8,8,\n",
    "                 9,9,9,9,9,9,9,9,9,9,9,9,\n",
    "                 10,10,10,10,10,10,10,10,10,10,10,10          \n",
    "                ]\n",
    "tsne_df = pd.DataFrame({'X':tsne_obj[:,0],\n",
    "                        'Y':tsne_obj[:,1],\n",
    "                       'digit':test_features})\n",
    "sns.scatterplot(x=\"X\", y=\"Y\",\n",
    "              data=tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"X\", y=\"Y\",\n",
    "              hue=\"digit\",\n",
    "              palette=['purple','red',\"blue\",\"yellow\",\"black\",\"gray\",\"pink\",\"orange\",\"green\",\"darkgreen\"],\n",
    "              legend='full',\n",
    "              data=tsne_df);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
