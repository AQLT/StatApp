{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel - Word2vec en utilisant Pytorch\n",
    "\n",
    "Ce notebook explique comment implémenter la technique de NLP, appelée word2vec, à l’aide de Pytorch. Word2vec a pour objectif principal de construire un \"word embedding\", c’est-à-dire une représentation de mots (\"latent and semantic free\") dans un espace continu. Pour ce faire, cette approche exploite un réseau de neurones peu profond, avec seulement 2 couches. Ce tutoriel explique : \n",
    "\n",
    "* comment générer l'ensemble de données adapté à word2vec\n",
    "* comment construire le réseau de neurones\n",
    "* comment accélérer l'approche\n",
    "\n",
    "\n",
    "## Les données\n",
    "\n",
    "Présentons les concepts de base du NLP : \n",
    "\n",
    "* Corpus : le corpus est la collection de textes définissant le jeu de données\n",
    "* vocabulaire: l'ensemble des mots contenu dans les données\n",
    "\n",
    "En guise d'exemple, nous utilisons le nouveau corpus contenu dans la base de données \"Brown\", disponible dans la librairie nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement des différentes bibliothèques... A faire une fois\n",
    "#nltk.download('universal_tagset') #Pour traduire les types de tags\n",
    "#nltk.download('brown')\n",
    "#nltk.download('words')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('names')\n",
    "#nltk.download('cmudict')\n",
    "\n",
    "# Install a pip package in the current Jupyter kernel PENSER A UTILISER le --user pour l'installer en local !\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install --user torchvision\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexiques et listes de mots dans nltk\n",
    "\n",
    "Le package NLTK comprend également un certain nombre de lexiques et de listes de mots. Celles-ci sont accessibles comme des corpus de texte. Les exemples suivants illustrent l’utilisation des corpus de la liste de mots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'en-basic']\n",
      "['a', 'aa', 'aal', 'aalii']\n",
      "235886\n",
      "['azerbaijani', 'danish', 'dutch', 'english']\n",
      "['aux', 'avec', 'ce', 'ces']\n",
      "['female.txt', 'male.txt']\n",
      "['Abagail', 'Abbe', 'Abbey', 'Abbi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names, stopwords, words\n",
    "\n",
    "print(words.fileids())\n",
    "print(words.words('en')[1:5]) \n",
    "print(len(words.words('en')))\n",
    "print(stopwords.fileids()[1:5])\n",
    "print(stopwords.words('french')[1:5])\n",
    "print(names.fileids())\n",
    "print(names.words('female.txt')[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le corpus du dictionnaire de prononciation CMU contient des transcriptions de plus de 100 000 mots. Vous pouvez y accéder sous forme de liste d'entrées (chaque entrée étant composée d'un mot, d'un identifiant et d'une transcription) ou sous forme de dictionnaire de mots en listes de transcriptions. Les transcriptions sont codées sous forme de n-uplets de chaînes de phonèmes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('acetate', ['AE1', 'S', 'AH0', 'T', 'EY2', 'T'])\n",
      "[['K', 'IH1', 'M'], ['AA0', 'N', 'T', 'UW1', 'N', 'EH0', 'Z']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "print(cmudict.entries()[653]) \n",
    "# charger le corpus entier cmudict dans le dictionnaire python:\n",
    "transcr = cmudict.dict()\n",
    "print([transcr[w][0] for w in 'Kim Antunez'.lower().split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appropriation des données de type corpus\n",
    "\n",
    "Commençons par nous approprier le fonctionnement du format \"corpus\" dans Python...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROWN CORPUS\n",
      "\n",
      "A Standard Corpus of Present-Day Edited American\n",
      "English, for use with Digital Computers.\n",
      "\n",
      "by W. N. Francis and H. Kucera (1964)\n",
      "Department of Linguistics, Brown University\n",
      "Providence, Rhode Island, USA\n",
      "\n",
      "Revised 1971, Revised and Amplified 1979\n",
      "\n",
      "http://www.hit.uib.no/icame/brown/bcm.html\n",
      "\n",
      "Distributed with the permission of the copyright holder,\n",
      "redistribution permitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Description du corpus brown\n",
    "print(brown.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemin de la base de données Brown : \n",
      "<CategorizedTaggedCorpusReader in 'W:/AppData/nltk_data/corpora/brown'>\n",
      "Nombre de mots du fichier ca01 la base Brown \n",
      "\n",
      " 10 premiers noms de fichiers de la base Brown : \n",
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10']\n",
      "\n",
      " 100 premiers caractères du fichier ca01 la base Brown : \n",
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn\n",
      "\n",
      " 10 premiers mots du fichier ca01 la base Brown : \n",
      "['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
      "\n",
      " Mots de la base Brown en entier : \n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "print(\"Chemin de la base de données Brown : \")\n",
    "print(str(nltk.corpus.brown).replace('\\\\\\\\','/'))\n",
    "\n",
    "print(\"Nombre de mots du fichier ca01 la base Brown \")\n",
    "len(brown.words('ca01'))\n",
    "\n",
    "print(\"\\n 10 premiers noms de fichiers de la base Brown : \")\n",
    "print(brown.fileids()[:10])\n",
    "\n",
    "print(\"\\n 100 premiers caractères du fichier ca01 la base Brown : \")\n",
    "print(brown.raw('ca01')[:100])\n",
    "\n",
    "print(\"\\n 10 premiers mots du fichier ca01 la base Brown : \")\n",
    "print(brown.words('ca01')[1:10])\n",
    "\n",
    "#print(\"\\n Première phrase du fichier ca01 la base Brown : \")\n",
    "#print(brown.sents('ca01')[1:10])\n",
    "\n",
    "#print(\"\\n Premier paragraphe du fichier ca01 la base Brown : \")\n",
    "#print(brown.paras('ca01'))\n",
    "\n",
    "print(\"\\n Mots de la base Brown en entier : \")\n",
    "print(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base Brown a la particularité d'être annotée (pas seulement en texte plein). Elle est annotée avec des balises de partie de parole et définit des méthodes supplémentaires étiquetées_*(), dans lesquels les mots sont des nuplets (mot, balise), plutôt que de simples chaînes de mots. Ici les tags semblent correspondre au type des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(brown.tagged_words())\n",
    "print(brown.tagged_words(tagset='universal')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs corpus inclus dans NLTK contiennent des documents classés par sujet, genre, polarité, etc. Outre l’interface de corpus standard, ces corpus permettent d’accéder à la liste des catégories et à l'association entre les documents et leurs catégories (dans les deux sens). On peut accéder aux catégories à l'aide de la méthode categories(). Cette méthode a un argument facultatif qui spécifie un document ou une liste de documents, nous permettant d'associer (un ou plusieurs) documents vers (une ou plusieurs) catégories.\n",
    "\n",
    "Outre la mise en correspondance des catégories et des documents, ces corpus permettent un accès direct à leur contenu via les catégories. Au lieu d'accéder à un sous-ensemble d'un corpus en spécifiant un ou plusieurs ID de fichier, nous pouvons identifier une ou plusieurs catégories, ici la catégorie news. Notez que qu'il le faut pas préciser à la fois les documents et catégories, sinon cela renverait une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['belles_lettres', 'editorial', 'fiction', 'government']\n",
      "['editorial', 'news']\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories()[1:5]) \n",
    "print(brown.categories(['ca01','cb01']))\n",
    "print(brown.tagged_words(categories='news'))\n",
    "\n",
    "#Dans le contexte d'un système de catégorisation de texte,\n",
    "#nous pouvons facilement tester si la catégorie attribuée à un document est correcte :\n",
    "def classify(doc): return 'news'   #\n",
    "doc = 'ca01'\n",
    "print(classify(doc) in brown.categories(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparations de nos données dans brown\n",
    "\n",
    "Revenons au traitement de la base brown pour le NLP. \n",
    "\n",
    "Ici, les caractères autres que des lettres sont supprimés de la chaîne. De plus, le texte est mis en minuscule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of']\n"
     ]
    }
   ],
   "source": [
    "#print(brown.fileids('news')[1:5])\n",
    "#print(brown.sents('ca01')[0][1:5]) #5 premiers mots de la 1e phrase\n",
    "#raw_text = list(itertools.chain.from_iterable(brown.sents('ca01')))[1:5] #idem sans préciser de numéro de phrase\n",
    "#print(' '.join(raw_text))\n",
    "\n",
    "import itertools  #NEW KIM\n",
    "\n",
    "for cat in ['news']:  #On se restreint à la catégorie \"news\" des articles contenus dans brown. \n",
    "    for text_id in brown.fileids(cat): #On parcourt tous les fichiers de la catégorie news\n",
    "        raw_text = list(itertools.chain.from_iterable(brown.sents(text_id))) #on récupère les mots\n",
    "        text = ' '.join(raw_text) #justaxposer les mots en mettant des espaces\n",
    "        text = text.lower() #texte mis en minuscule. \n",
    "        text.replace('\\n', ' ') #on supprime les sauts de lignes\n",
    "        text = re.sub('[^a-z ]+', '', text) #on enlève les caractères non alphabétiques\n",
    "        corpus.append([w for w in text.split() if w != '']) #on intègre ces mots sous forme de \n",
    "        # liste de liste dans corpus\n",
    "\n",
    "print(corpus[0][1:10]) #Les 10 premiers mots du premier texte (ca01) de corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sous-échantillonnage des mots fréquents\n",
    "\n",
    "La première étape du prétraitement des données consiste à équilibrer les occurrences de mots dans les données. Pour ce faire, nous effectuons un sous-échantillonnage des mots fréquents. Appelons $p_i$ la proportion du mot i dans le corpus. Alors la probabilité $P(wi)$ de garder le mot dans le corpus est définie comme suit :\n",
    "\n",
    "\n",
    "\n",
    "$$P(w_i) = \\dfrac{10^{-3}}{p_i}\\left(\\sqrt{10^3 p_i} + 1\\right)$$\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">Formule à comprendre</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random, math\n",
    "\n",
    "# On fixe l'aléa pour bien pouvoir comprendre nos résultats\n",
    "random.seed(1)\n",
    "\n",
    "#Retourne un float aléatoire dans l'intervalle [0.0, 1.0).\n",
    "#print(random.random())\n",
    "\n",
    "# Fonction qui en entrée prend un corpus et renvoie un corpus filtré\n",
    "def subsample_frequent_words(corpus): #création d'une fonction qui prend un corpus en entrée\n",
    "    filtered_corpus = [] \n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus)))) #{'the': 159650, 'fulton': 350, ...\n",
    "    sum_word_counts = sum(list(word_counts.values())) #nombre total de mots\n",
    "    word_counts = {word: word_counts[word]/float(sum_word_counts) for word in word_counts} \n",
    "    # calculs de proportion dont la somme fait 100 {'the': 0.07339892418739369, 'fulton': 0.00016091214197048412,\n",
    "    for text in corpus: #on parcourt tous les articles du corpus\n",
    "        filtered_corpus.append([]) #on crée une sous liste à chaque nouvel article\n",
    "        for word in text: #et pour tous les mots de l'article\n",
    "            if random.random() < (1+math.sqrt(word_counts[word] * 1e3)) * 1e-3 / float(word_counts[word]):\n",
    "                #si une proportion tirée au hasard est inférieure à la probabilité de garder ce mot\n",
    "                filtered_corpus[-1].append(word) #on enlève ce mot. \n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\"> La condition random < P(wi) ne me semble pas intuitive</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On applique la fonction de filtrage au corpus\n",
    "corpus = subsample_frequent_words(corpus)\n",
    "\n",
    "vocabulary = set(itertools.chain.from_iterable(corpus)) # transforme le nouveau corpus au format\n",
    "#set() transforme des listes [] ou tuples () en format set {} et les classe.\n",
    "#{'bernadines', 'removed', 'lining',\n",
    "\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)} # on associe un index a chaque mot\n",
    "#{'bernadines': 0, 'removed': 1,\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)} # on associe un mot a chaque index\n",
    "#{0: 'bernadines', 1: 'removed', "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\"> Je ne comprends pas pourquoi vocabulary est réordonné par rapport à corpus (à quoi cela sert-il) et surtout comment est déterminé l'ordre de vocabulary ?  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire des sacs de mots\n",
    "\n",
    "Word2vec est une approche de \"sac\" de mots. Pour chaque mot de l'ensemble de données, nous devons extraire les mots de contexte (*context words*), c'est-à-dire les mots voisins dans une certaine fenêtre (*window*) de longueur fixe. Par exemple, dans la phrase suivante :\n",
    "\n",
    "*My cat is lazy, it sleeps all day long*\n",
    "\n",
    "Si nous considérons le mot cible (*target word*) *lazy* et que nous choisissons une fenêtre de taille 2, les mots du contexte sont *cat*, *is*, *it* et *sleeps*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return type: <class 'enumerate'>\n",
      "[(0, 'eat'), (1, 'sleep'), (2, 'repeat')]\n",
      "[(2, 'g'), (3, 'e'), (4, 'e'), (5, 'k')]\n"
     ]
    }
   ],
   "source": [
    "#Petit aparté pour comprendre la fonction enumerate de Python\n",
    "l1 = [\"eat\",\"sleep\",\"repeat\"] \n",
    "s1 = \"geek\"\n",
    "obj1 = enumerate(l1) \n",
    "obj2 = enumerate(s1)   \n",
    "print(\"Return type:\",type(obj1))\n",
    "print(list(enumerate(l1))) #C'est plutôt cette fonction d'indiciation de mot qu'on utilise ci-dessous\n",
    "print(list(enumerate(s1,2))) #Ici on compte le nombre de lettres dans un mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 473644 paires de mots cibles et de contexte\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#print(list(enumerate(corpus[0]))[:10])\n",
    "#[(0, 'fulton'), (1, 'county'),\n",
    "\n",
    "# On fixe l'aléa pour bien pouvoir comprendre nos résultats\n",
    "random.seed(1)\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 4 #taille de la fenêtre = 4\n",
    "\n",
    "# créer des pairs de mots cibles et contexte\n",
    "for text in corpus: #pour tous les articles du corpus (nettoyé par la méthodes ci-dessus)\n",
    "    for i, word in enumerate(text): #pour chaque association nb de mots (i) + mot (word)\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j]))\n",
    "                \n",
    "#print(context_tuple_list[0:10])    \n",
    "#[('fulton', 'county'), ('fulton', 'grand'), ('fulton', 'jury'), ('county', 'fulton')...\n",
    "print(\"Il y a {} paires de mots cibles et de contexte\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire le réseau (méthodologie)\n",
    "\n",
    "### L’algorithme du gradient\n",
    "\n",
    "#### Le gradient \"from scratch\"\n",
    "\n",
    "L'algorithme du gradient désigne un algorithme d'optimisation différentiable. Il est par conséquent destiné à minimiser (ou maximiser si on multiplie par -1 par exemple) une fonction réelle différentiable définie sur un espace euclidien (par exemple, ${R}^{n}$, l'espace des n-uplets de nombres réels, muni d'un produit scalaire) ou, plus généralement, sur un espace hilbertien. L'algorithme est itératif et procède donc par améliorations successives. Au point courant, un déplacement est effectué dans la direction opposée au gradient, de manière à faire décroître la fonction. Le déplacement le long de cette direction est déterminé par la technique numérique connue sous le nom de recherche linéaire. Cette description montre que l'algorithme fait partie de la famille des algorithmes à directions de descente.\n",
    "\n",
    "On se donne un point/itéré initial $x_{0} \\in \\mathbb {E}$ et un seuil de tolérance $\\varepsilon \\geqslant 0$\n",
    "L'algorithme du gradient définit une suite d'itérés \n",
    "$ x_{1},x_{2},\\ldots \\in \\mathbb {E}$ jusqu'à ce qu'un test d'arrêt soit satisfait. Il passe de $x_{k}$ à $x_{k+1}$ par les étapes suivantes. \n",
    "\n",
    "1. Simulation : calcul de $\\nabla f(x_{k})$\n",
    "2. Test d'arrêt : si $\\|\\nabla f(x_{k})\\|\\leqslant \\varepsilon$, arrêt.\n",
    "3. Calcul du pas $\\alpha _{k}$ par une règle de recherche linéaire sur f en $x_{k}$ le long de la direction $ -\\nabla f(x_{k})$\n",
    "4. Nouvel itéré : $x_{k+1}=x_{k}-\\alpha _{k}\\nabla f(x_{k})$\n",
    "\n",
    "*Exemple* : \n",
    "\n",
    "Fonction de prédiction d'une prédiction : $h(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "Coût de l'erreur : $J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=0}^{m} (h(x_i) - y_i)^2$\n",
    "\n",
    "Objectif : Trouver les meilleurs paramètres $\\theta_0$ et $\\theta_1$ revient à minimiser (trouver le minimum) la fonction du coût.\n",
    "\n",
    "Algorithme : $\\theta_{j+1} = \\theta_{j} - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_{j}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après 1000 itérations theta_0 = 0.30481117467429525, theta_1 = 1.894204663182967 et la loss vaut 7.178563368300242\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "# -*- coding: utf-8 -*-\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# On fixe l'aléa pour bien pouvoir comprendre nos résultats\n",
    "random.seed(1)\n",
    "\n",
    "def calculer_cost_function(theta_0, theta_1):\n",
    "    global_cost  = 0\n",
    "    for i in range(len(X)):\n",
    "        cost_i = ((theta_0 + (theta_1 * X[i])) - Y[i]) * ((theta_0 + (theta_1 * X[i])) - Y[i]) \n",
    "        global_cost+= cost_i\n",
    "    return (1/ (2 * len(X))) * global_cost\n",
    "\n",
    "# données de régressions\n",
    "df = pd.DataFrame([[6.1101, 17.592], [5.5277, 9.1302], [8.5186, 13.662]]) \n",
    "\n",
    "X =  df.iloc[0:len(df),0]#une seule variable prédictive car régression univariée\n",
    "Y =  df.iloc[0:len(df),1]#Valeurs observées (à prédire)\n",
    "\n",
    "#un tableau pour stocker les valeurs d'erreurs global à chaque tour d'itération\n",
    "COST_RECORDER = []\n",
    "\n",
    "# la taille de notre ensemble de données d'apprentissage\n",
    "M = len(X)\n",
    "\n",
    "learning_rate_ALPHA = float(0.0001) #0.0001\n",
    "initial_theta_0 = float(0)\n",
    "initial_theta_1 = float(0)\n",
    "nombre_iterations = 1000\n",
    "\n",
    "\n",
    "def calculer_derivees_partielles(ancien_theta_0, ancien_theta_1):\n",
    "    derivee_theta_0 = float(0)\n",
    "    derivee_theta_1 = float(0)\n",
    "    for i in range(0, len(X)):\n",
    "        derivee_theta_0 += float(((ancien_theta_0 + (ancien_theta_1 * X[i])) - float(Y[i])))\n",
    "        derivee_theta_1 += (((ancien_theta_0 + (ancien_theta_1 * X[i]))) - float(Y[i])) * float(X[i])  \n",
    "    derivee_theta_0 = (1/M) * derivee_theta_0\n",
    "    derivee_theta_1 = (1/M) * derivee_theta_1\n",
    "    return [derivee_theta_0, derivee_theta_1]\n",
    "\n",
    "def calculer_nouvelles_theta(ancien_theta_0, ancien_theta_1):\n",
    "    [derivee_theta_0, derivee_theta_1] = calculer_derivees_partielles(ancien_theta_0,ancien_theta_1)\n",
    "    nouvelle_theta_0 = ancien_theta_0 - (learning_rate_ALPHA * derivee_theta_0)\n",
    "    nouvelle_theta_1 = ancien_theta_1 - (learning_rate_ALPHA * derivee_theta_1)\n",
    "    COST_RECORDER.append(calculer_cost_function(nouvelle_theta_0, nouvelle_theta_1))\n",
    "    return [nouvelle_theta_0,nouvelle_theta_1]\n",
    "\n",
    "\n",
    "def lancer_gradient_descent():\n",
    "    tmp_theta_0 = initial_theta_0\n",
    "    tmp_theta_1 = initial_theta_1   \n",
    "    for i in range(nombre_iterations):\n",
    "        [nouvelle_theta_0, nouvelle_theta_1] = calculer_nouvelles_theta(tmp_theta_0, tmp_theta_1)\n",
    "        tmp_theta_0 = nouvelle_theta_0\n",
    "        tmp_theta_1 = nouvelle_theta_1\n",
    "    return [tmp_theta_0, tmp_theta_1]         \n",
    "\n",
    "[final_theta_0, final_theta_1] = lancer_gradient_descent()\n",
    "loss = calculer_cost_function(final_theta_0, final_theta_1)\n",
    "print(\"Après {0} itérations theta_0 = {1}, theta_1 = {2} et la loss vaut {3}\".format(nombre_iterations,\n",
    "                                                                                     final_theta_0, final_theta_1,\n",
    "                                                                                     COST_RECORDER[nombre_iterations-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print COST_RECORDER\n",
    "xx = []; yy=[]\n",
    "\n",
    "#dessiner l'avancer des differents de J(theta_0, theta_1)\n",
    "for i in range(len(COST_RECORDER)):\n",
    "   xx.append(i)\n",
    "   yy.append(COST_RECORDER[i])\n",
    "\n",
    "axes = plt.axes()\n",
    "axes.grid()\n",
    "plt.xlabel('Nombre d\\'iterations')\n",
    "plt.ylabel('Cout d\\'erreur global')\n",
    "plt.scatter(xx,yy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L'algorithme du gradient simplifié avec Pytorch\n",
    "\n",
    "Remarque : fortement inspiré de : http://vision.gel.ulaval.ca/~cgagne/enseignement/apprentissage/A2018/presentations/iam-sem11-pytorch.pdf\n",
    "\n",
    "Pytorch sert initialement à manipuler des operations de tenseur et les deriver automatiquement ! \n",
    "\n",
    "A force de faire des operations, Pytorch construit un graphe de calcul. Ce graphe permet de suivre toutes les operations necessaires au calcul du resultat. Ainsi, il est ensuite très facile pour Pytorch de calculer automatiquement la derivée à chaque ́etape du graphe. Pour indiquer que l’on veut que pytorch calcule la derivee par rapport à un certain tenseur, on doit utiliser le paramètre `requires_grad`. \n",
    "\n",
    "Voici ci-dessous l'exemple d'une régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_hat :  tensor([1.6918], grad_fn=<ThAddBackward>) \n",
      "\n",
      "err :  tensor([0.2393], grad_fn=<MulBackward>) \n",
      "\n",
      "tensor([0.7610, 1.4528, 2.1445])\n",
      "tensor([0.7610, 1.4528, 2.1445], grad_fn=<ThMulBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#a = torch.FloatTensor([[1,2,3], [2,3,4]],requires_grad=True)\n",
    "a = Variable(torch.FloatTensor([[1,2,3], [2,3,4]]), requires_grad = True)\n",
    "#a.requires_grad = True # ou définition après coup\n",
    "\n",
    "# y est la vraie sortie et x les vrais contrôles\n",
    "x = Variable(torch.FloatTensor([1.1,2.1,3.1]), requires_grad = True) #x1 x2 x3\n",
    "y = Variable(torch.FloatTensor([1]), requires_grad = True) #y1\n",
    "\n",
    "#On déclare un vecteur de poids et un biais aleatoire :\n",
    "W = torch.randn(3, requires_grad=True) # tensor de 2 dimensions tiré au hasard W=w1,w2,w3\n",
    "b = torch.randn(1, requires_grad=True) # 1 dimension b=b1\n",
    "\n",
    "# On execute la chaîne d’operations (proche de numpy). y_hat est la sortie predite, x l’entree\n",
    "y_hat = W.dot(x) + b  # [yh1] = w1x1 + w2x2 + w3x3 + b1\n",
    "#[1,1] = [1,3][3,1] + [1,1]\n",
    "print(\"\\ny_hat : \",y_hat,\"\\n\")\n",
    "\n",
    "#On calcule l’erreur quadratique. Ici y est la vraie sortie\n",
    "err = 0.5 * (y_hat - y) ** 2 #[e1] = 0.5 (w1x1 + w2x2 + w3x3 + b1 -y1)^2\n",
    "print(\"err : \",err,\"\\n\")\n",
    "\n",
    "#Puisque y est un scalaire, on peut deriver l'equation à l’aide d’une seule methode, backward():\n",
    "err.backward() \n",
    "\n",
    "#derr/dy = - (y_hat - y)\n",
    "#print(y.grad.data)\n",
    "#print(y - y_hat)\n",
    "\n",
    "print(W.grad) \n",
    "#derr/dw1 = w1 (w1x1 + w2x2 + w3x3 + b1 -y1) = w1 (yh1 - y1)\n",
    "print(x*(y_hat-y))\n",
    "\n",
    "\n",
    "#On peut alors recuperer les dérivées dans les tenseurs W et b :\n",
    "W_grad = W.grad\n",
    "b_grad = b.grad\n",
    "\n",
    "#Et pourquoi pas, faire un pas dans la bonne direction :\n",
    "alpha = 0.001 #pas\n",
    "W = W - alpha * W.grad\n",
    "b = b - alpha * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4., 10.]])\n"
     ]
    }
   ],
   "source": [
    "# Un exemple autre exemple pour être sûr de bien comprendre la fonction backward...\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "x = Variable(torch.FloatTensor([[2,1]]), requires_grad=True) #[b1,b2]\n",
    "M = Variable(torch.FloatTensor([[1,2],[3,4]])) #[[a1,a2],[a3,a4]]\n",
    "y = torch.mm(x, M) #[5., 8.] [y1, y2] = [a1b1 + a3b2 , a2b1 + a4b2]\n",
    "y.backward(torch.FloatTensor([[2,1]]), retain_graph = True)\n",
    "print(x.grad.data) #([[ 4., 10.]])\n",
    "\n",
    "#car :\n",
    "#dL/db1 = dL/dy1 * dy1/db1 + dL/dy2 * dy2/db1 = 2 * a1 + 1 * a2 = 4\n",
    "#dL/db2 = dL/dy1 * dy1/db2 + dL/dy2 * dy2/db2 = 2 * a3 + 1 * a4 = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L'utilité d'initialiser les gradients avec zero_grad\n",
    "\n",
    "La fonction  `zero_grad` initialise les gradients de tous les paramètres du modèle à zéro. Dans PyTorch, nous devons initialiser les gradients à 0 avant de commencer à effectuer une rétropropagation du gradient (*backpropagation*)  - calculer le gradient de l'erreur pour chaque neurone d'un réseau de neurones, de la dernière couche vers la première -, car PyTorch accumule les gradients à chaque nouveau passage. Ceci est pratique quand on entraîne des réseaux de neurones récurrents (*Recurrent neural network* ou *RNN*). Ainsi, l’action par défaut consiste à accumuler (c’est-à-dire à sommer) les gradients à chaque appel de `loss.backward()`.\n",
    "\n",
    "De ce fait, lorsque vous démarrez votre boucle d’entraînement, vous devez idéalement mettre à zéro les gradients afin de mettre à jour le paramètre correctement. Sinon, le gradient indiquerait une direction autre que la direction souhaitée vers le minimum (ou le maximum, dans le cas d'objectifs de maximisation).\n",
    "\n",
    "Appeler x.grad.data.zero_() avant y.backward() peut s’assurer que x.grad est exactement le même que le y'(x) courant, et non une somme de tous les y'(x) des itérations précédentes.\n",
    "\n",
    "Voici un exemple pour comprendre : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANS ZERO\n",
      "tensor([[-2.0000]])\n",
      "tensor([[-4.0000]])\n",
      "AVEC ZERO\n",
      "tensor([[1.]])\n",
      "tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.Tensor([[3.14]]), requires_grad=True)\n",
    "\n",
    "print(\"SANS ZERO\")\n",
    "\n",
    "for t in range(2):\n",
    "    y = 2 *x.sin() \n",
    "    y.backward()\n",
    "    print(x.grad.data) ##dy/dx\n",
    "\n",
    "x = Variable(torch.Tensor([[0]]), requires_grad=True) \n",
    "\n",
    "print(\"AVEC ZERO\")\n",
    "\n",
    "for t in range(2):\n",
    "    if x.grad is not None:\n",
    "        x.grad.data.zero_()\n",
    "    y = x.sin() \n",
    "    y.backward()\n",
    "    print(x.grad)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les réseaux de neurones\n",
    "\n",
    "#### Définir un réseau \n",
    "\n",
    "Définir un réseau avec des tenseurs directement serait une Tâche ardue. `Pytorch` nous offre une manière de déclarer facilement des réseaux ! Typiquement on utilise le package `torch.nn` et on hérite de `nn.Module`.\n",
    "\n",
    "La classe `torch.nn.Module` est la classe de base pour tous les modules de réseaux neuronaux. C'est pourquoi le modèle word2vec va ici être une sous-classe de celle-ci. Mais les modules peuvent également contenir d'autres modules, ce qui permet de les imbriquer dans une arborescence (*tree structure*). \n",
    "\n",
    "Un module reçoit en entrée des *tensors* et renvoie d'autres *tensors* en sortie, mais il peut également conserver un état interne tel que les tensors contenant des paramètres pouvant être appris (*learnable*). Le package nn définit également un ensemble de fonctions de *loss* utiles couramment utilisées lors de la formation de réseaux de neurones, comme la `CrossEntropyLoss()` que l'on utilise dans word2vec.\n",
    "\n",
    "`Pytorch` offre plusieurs couches toutes descendantes de  `torch.nn.Module`. Le but est de composer des modules pour créer des modules plus complexes. Voici quelques modules de base. Les autres sont consultables ici : https://pytorch.org/docs/stable/nn.html. \n",
    "\n",
    "* Linéaire `torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "* Convolution 2d `torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1,padding=0, dilation=1, groups=1, bias=True)`\n",
    "\n",
    "\n",
    "Remarque : la plupart des couches sont également disponibles sous la forme de fonctions à partir de `torch.nn.functional`. Mais ATTENTION, le module n’est pas au courant que ces couches existent lorsqu’elles sont déclarées en fonctiond directement. Les paramètres de ces couches ne sont pas pris en compte dans la liste des paramètres. Certaines couches comme le dropout ou la batchnorm ont des comportements différents en entraînement et en test. Changer le mode du réseau change le comportement d’une couche classe, mais pas d’une couche fonction. Pour toutes ces raisons, il vaut mieux utiliser les couches fonctions seulement lorsque la couche n’a pas de paramètres à optimiser et/ou lorsque le comportement ne varie pas entre l’entraînement et le test.\n",
    "\n",
    "Exemple : réseau Lenet-5 \n",
    "![](http://cedric.cnam.fr/vertigo/Cours/ml2/_images/LeNet5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.C1 = nn.Conv2d(1, 6, kernel_size=5) #Convolution C1\n",
    "        self.S2 = nn.MaxPool2d(2) #Subsampling S2\n",
    "        self.C3 = nn.Conv2d(6, 16, kernel_size=5) #Convolution C3\n",
    "        self.S4 = nn.MaxPool2d(2) #Subsampling S4\n",
    "        self.C5 = nn.Linear(16*5*5, 120) #Full connexion C5\n",
    "        self.F6 = nn.Linear(120, 64) #Full connexion F6\n",
    "        self.output = nn.Linear(64, 10) # Gaussian output\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.S2(F.relu(self.C1(x))) #C1 -> S2 = fonction relu\n",
    "        y = self.S4(F.relu(self.C3(y))) #C3 -> S4 = fonction relu\n",
    "        y = y.view(-1, 16*5*5) # redimensionne\n",
    "        y = F.relu(self.C5(y)) \n",
    "        y = F.relu(self.F6(y))\n",
    "        return self.output(y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "     super(Model, self).__init__()\n",
    "     self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "     self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "def forward(self, x):\n",
    "       x = F.relu(self.conv1(x))\n",
    "       return F.relu(self.conv2(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraîner un réseau \n",
    "\n",
    "Une fois que l’on a chargé les données, pour entraîner un réseau, il faut :\n",
    "* un optimisateur qui se trouve dans `torch.optim`.\n",
    "* Une fonction d’erreur qui se trouve dans `torch.nn`, comme les couches.\n",
    "Exemple : classification MNIST, optimisateur = `torch.optim.SGD` et erreur = `torch.nn.CrossEntropyLoss`\n",
    "Exemple : web2vec (ici),  optimisateur = X et erreur = `torch.nn.CrossEntropyLoss`\n",
    "\n",
    "Il est aussi possible avec Pytorch d’utiliser un modèle pré-entraîné (voir lien pour plus de détails)\n",
    "\n",
    "<span style=\"background-color: #FFFF00\"> Exemple ci-dessous ne fonctionne pas (installation de torchvision ne semble pas aboutir sur serveur ENSAE. A eclaircir et tester plus tard) </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-175-219607009874>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-175-219607009874>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    optimizer.zero_grad() # important! remet les gradients à 0\u001b[0m\n\u001b[1;37m                                                              ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST ## Ne marche pas\n",
    "from torchvision.transforms import ToTensor ## Ne marche pas\n",
    "\n",
    "nb_epoch = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9 \n",
    "# télécharge dans ’path/to/data’ \n",
    "train_set = MNIST(train=True, transform=ToTensor(),download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
    "model = Lenet5()\n",
    "model.train() # mettre en mode entraînement\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "for i_epoch in range(nb_epoch):\n",
    "\tfor i_batch, batch in enumerate(train_loader):\n",
    "\t\tX, y = batch\n",
    "        optimizer.zero_grad() # important! remet les gradients à 0\n",
    "\t\ty_hat = model(X) # calcule la prediction\n",
    "\t\tloss = criterion(y_hat, y) # calcule l’erreur \n",
    "\t\tloss.backward() # dérive le graphe\n",
    "\t\toptimizer.step() # effectue une ́etape d’optimisation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire le réseau (application) \n",
    "\n",
    "Il existe deux approches pour word2vec : \n",
    "\n",
    "* CBOW (*Continuous Bag Of Words* ou sac continu de mots). Il prédit le mot cible conditionnellement au contexte. En d'autres termes, les mots de contexte sont l'entrée et le mot cible est la sortie.\n",
    "\n",
    "* Skip-gram. Il prédit le contexte conditionnellement au mot cible. En d'autres termes, le mot cible est l'entrée et les mots de contexte sont la sortie.\n",
    "\n",
    "Le code suivant concerne la méthode CBOW.\n",
    "\n",
    "Le vocabulaire est représenté sous la forme d'un codage *one-hot encoding*, ce qui signifie que la variable d'entrée est un vecteur de la taille du vocabulaire (de taille n si n mots). Pour un mot, ce vecteur vaut 0 partout sauf à l'endroit de l'indice du mot dans le vocabulaire (i pour le ième mot et $x_i$ = 1).\n",
    "\n",
    "Le codage *one-hot* est projeté (*mapped*)  sur un vecteur-mot (*embedding*) , c'est-à-dire une représentation latente du mot en tant que vecteur contenant des valeurs continues et dont la taille est plus petit que le vecteur de codage *one-hot*.\n",
    "\n",
    "Pour chaque mot de contexte, une fonction softmax prend l'*embedding du mot*, produisant une distribution de probabilité du mot cible sur le vocabulaire.\n",
    "\n",
    "<img src=\"https://rguigoures.github.io/images/cbow.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le réseau CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nntorch.nn  as  nn\n",
    "import torch.nn  as  nn #KIM\n",
    "#import torch.autogradtorch.aut  as autograd\n",
    "import torch.autograd  as  autograd #KIM\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Définition du réseau de Neurone appelé Word2Vec\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    #Définition des deux couches\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        #Couche Embedding :méthode nn.Embedding\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size) #Couche de sortie : méthode nn.Linear\n",
    "        \n",
    "    def forward(self, context_word):\n",
    "        emb = self.embeddings(context_word) \n",
    "        hidden = self.linear(emb)\n",
    "        #out = F.log_softmax(hidden) #depreciated\n",
    "        out = F.log_softmax(hidden, dim=1) #puis on utilise la fonction softmax\n",
    "        # KIM For matrices, it’s 1. For others, it’s 0.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"background-color: #FFFF00\">A creuser les codes et méthodos de :  </span>\n",
    "\n",
    "* <span style=\"background-color: #FFFF00\">méthode nn.Embedding</span>\n",
    "* <span style=\"background-color: #FFFF00\">méthode nn.Linear</span>\n",
    "* <span style=\"background-color: #FFFF00\">méthode nn.CrossEntropyLoss</span>\n",
    "* <span style=\"background-color: #FFFF00\">méthode optim.Adam</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le réseau Skip-GRAM\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">TODO\n",
    " </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrêter l'algorithme avant la fin\n",
    "\n",
    "Avant de commencer l’apprentissage, introduisons le concept d’arrêt précoce (*early stopping*). Il vise à arrêter l'apprentissage lorsque la perte (*loss*) ne diminue pas de manière significative (paramètre  `min_percent_gain`) après un certain nombre d'itérations (paramètre  `patience`). Un arrêt précoce est généralement utilisé sur la perte de validation (*validation loss*), mais dans le cas de word2vec, il n’y a pas de validation car l’approche n’est pas supervisée. Nous appliquons plutôt l'arrêt précoce sur les données d'entraînement à la place (*training loss*).\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">mieux comprendre le concept de validation et d'apprentissage supervisé</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
    "        self.patience = patience #patience = 5 => Nombre de loss qui peuvent être accumulées dans list-loss\n",
    "        self.loss_list = [] #On initialise la liste des loss\n",
    "        self.min_percent_gain = min_percent_gain / 100. #min_percent_gain = 0.1\n",
    "        \n",
    "    def update_loss(self, loss): #fonction d'actualisation de la loss\n",
    "        self.loss_list.append(loss) #On ajoute la nouvelle loss calculée dans loss-list\n",
    "        if len(self.loss_list) > self.patience: #Si la liste de loss dépasse patience\n",
    "            del self.loss_list[0] #... on supprime le 1er élément de list-loss\n",
    "    \n",
    "    def stop_training(self): #fonction d'arrêt de l'algorithme, vaut true si on doit l'arrêter\n",
    "        if len(self.loss_list) == 1: #si list-loss ne contient qu'un élément\n",
    "            return False #on continue l'algorithme\n",
    "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list) #on calcule le gain \n",
    "        print(\"Loss gain: {}%\".format(round(100*gain,2))) #on imprime le gain\n",
    "        if gain < self.min_percent_gain: #si le gain est faible\n",
    "            return True #on arrête l'algo \n",
    "        else: #sinon\n",
    "            return False #on le continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage\n",
    "\n",
    "Pour l'apprentissage (*learning*), nous utilisons l'entropie croisée (*cross entropy*) comme fonction de loss. Le réseau de neurones est entraîné avec les paramètres suivants:\n",
    "\n",
    "* taille d'intégration : 200 (*embedding size*)\n",
    "* taille du lot : 2000A (*batch size*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Etape 1 : On transforme les couple target context en couple de tensor (autre format)\n",
    "\n",
    "context_tensor_list = []\n",
    "\n",
    "for target, context in context_tuple_list[0:10]: #[('fulton', 'county'), ('fulton', 'grand')\n",
    "#for target, context in context_tuple_list: # MODIFIE CAR SINON TROP LONG A TOURNER\n",
    "    target_tensor = autograd.Variable(torch.LongTensor([word_to_index[target]])) #fulton devient tensor([6582])\n",
    "    context_tensor = autograd.Variable(torch.LongTensor([word_to_index[context]]))  #county devient tensor([8950])\n",
    "    context_tensor_list.append((target_tensor, context_tensor)) \n",
    "    #(tensor([6582]), tensor([8950])) ajouté à context_tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss:  9.617459\n",
      "Mean Loss:  9.564626\n",
      "Loss gain: 0.55%\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(vocabulary) #12132\n",
    "net = Word2Vec(embedding_size=3, vocab_size=vocabulary_size) # On crée un réseau \"net\" de classe word2vec\n",
    "loss_function = nn.CrossEntropyLoss() #fonction de loss : crossEntropy\n",
    "optimizer = optim.Adam(net.parameters()) #optimizer : Adam\n",
    "#early_stopping = EarlyStopping()\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=5)\n",
    "\n",
    "## Etape 2 : \n",
    "while True:\n",
    "    losses = [] #initialisation du vecteur de loss\n",
    "    for target_tensor, context_tensor in context_tensor_list:    \n",
    "        net.zero_grad() #initialisation importante des gradients comme évoqué avant\n",
    "        log_probs = net(context_tensor) # On applique word2vec au premier couple target context ??\n",
    "        #cela donne un vecteur de taille [1, taille_voc=12132] : tensor([ -9.0275,  -8.8258,  -9.0572, \n",
    "        loss = loss_function(log_probs, target_tensor) #on calcul la loss de ce vecteur : tensor(9.6383, grad_fn=<NllLossBackward>)   \n",
    "        loss.backward() #on dérive le graphe\n",
    "        optimizer.step() #on effectue une étape d'optimisation (descente de gradient ?)\n",
    "        losses.append(loss.data) # on rajoute 9,63 dans losses à la fin il y a autant éléments dans losses que de couples context tensor (10)\n",
    "    print(\"Mean Loss: \", np.mean(losses)) # j'indique la loss moyenne\n",
    "    early_stopping.update_loss(np.mean(losses)) #je regarde si je dois stopper l'algorithme\n",
    "    if early_stopping.stop_training(): #si oui je m'arrête là sinon je recommence\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\">quel est le x dans dLoss/dx quand on fait loss.backward ? Relire théorie</span>\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">Je ne comprends pas la syntaxe net(context_tensor) : pourquoi seul le contexte tensor est en entrée de word2vec ?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accélérer l'approche\n",
    "\n",
    "L'implémentation vue jusqu'à présent est assez lente. Mais bonne nouvelle, il existe des solutions pour accélérer le calcul.\n",
    "\n",
    "### Apprentissage par lots (*batch learning*)\n",
    "\n",
    "Afin d'accélérer l'apprentissage, nous proposons d'utiliser des lots (*batches*). Cela implique que de nombreuses observations sont transmises via le réseau avant d'effectuer la rétropropagation. En plus d’être plus rapide, c’est aussi un bon moyen de régulariser les paramètres du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Fonction qui crée des batches, c'est à dire des sous échantillons de context_tuple_list\n",
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list) #réordonner au hasard le couples target, context\n",
    "    batches = [] #créer une liste de batches vide\n",
    "    batch_target, batch_context, batch_negative = [], [], [] #créer ces autres listes vides\n",
    "    for i in range(len(context_tuple_list)): #On parcourt tous les couples target context. \n",
    "        batch_target.append(word_to_index[context_tuple_list[i][0]]) #on récupère l'id du target\n",
    "        batch_context.append(word_to_index[context_tuple_list[i][1]]) #on récupère l'id du contexte\n",
    "        #batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]]) ## ???????\n",
    "        batch_negative.append([word_to_index[w] for w in context_tuple_list[i]]) \n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1: #si i est le dernier élément\n",
    "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
    "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
    "            tensor_negative = autograd.Variable(torch.from_numpy(np.array(batch_negative)).long())\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 7171, 11014,  1207,  6883,  7922]), tensor([10134,  9238,  5061,  5061,  7165]), tensor([[ 7171, 10134],\n",
      "        [11014,  9238],\n",
      "        [ 1207,  5061],\n",
      "        [ 6883,  5061],\n",
      "        [ 7922,  7165]]))\n"
     ]
    }
   ],
   "source": [
    "batches_test = get_batches(context_tuple_list,5) #dimension : 94729 batches de  3 élément\n",
    "#1er : liste de 5 targets, 2e liste de 5 contextes, 3e : 5 couples targets context\n",
    "print(batches_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">Comment context_tuple_list[i][2] (3e élément) peut exister alors que je pensais que context_tuple_list[i] est de taille 2 ? Revoir l'algo une fois avoir compris ça... \n",
    "</span>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Echantillonnage négatif (*negative sampling*)\n",
    "\n",
    "L'algorithme word2vec par défaut n'exploite que les exemples positifs et la fonction de sortie est un softmax. Cependant, l'utilisation de softmax ralentit l'apprentissage: softmax est normalisé pour tout le vocabulaire, puis tous les poids du réseau sont mis à jour à chaque itération. C'est pourquoi nous décidons d'utiliser une fonction sigmoïde en sortie : seuls les poids impliquant le mot cible sont mis à jour. Mais alors, le réseau n’apprend plus d’exemples négatifs. C’est pourquoi nous devons saisir des exemples négatifs générés artificiellement.\n",
    "\n",
    "Une fois que nous avons construit les données pour les exemples positifs, c'est-à-dire les mots situés au voisinage du mot cible, nous devons créer un ensemble de données avec des exemples négatifs. Pour chaque mot du corpus, la probabilité d'échantillonnage d'un mot de contexte négatif est définie comme suit :\n",
    "\n",
    "\n",
    "\n",
    "$$P(w_i) = \\dfrac{\\mid w_i \\mid^{\\frac{3}{4}}}{\\displaystyle\\sum_{j=1}^n\\mid w_j \\mid^{\\frac{3}{4}}}$$\n",
    "\n",
    "Rappel : La **loi binomiale** B(n,p) concerne le nombre de succès dans n épreuves de Bernoulli indépendantes donnant chacune un résultat binaire, comme dans le jeu de pile ou face. La loi multinomiale est une généralisation de celle-ci, applicable par exemple à n jets d'un dé à six faces. La loi multinomiale est la généralisation multidimensionnelle de la loi binomiale.\n",
    "\n",
    "M(8,p=c(p1,p2)) = [nombre de succès dans 8 épreuve de Bernoulli de proba p1, idem avec p2]\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "\n",
    "#Retourne un sous-échantillon du corpus de taille sample_size\n",
    "def sample_negative(sample_size):\n",
    "    \n",
    "    # Création du vecteur des P(wi)\n",
    "    sample_probability = {}\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus)))) #{'fulton': 14, 'county': 61,\n",
    "    # compte de mots\n",
    "    normalizing_factor = sum([v**0.75 for v in word_counts.values()]) # termes du dénominateur : 34487\n",
    "    for word in word_counts:\n",
    "        sample_probability[word] = word_counts[word]**0.75 / normalizing_factor # array des P(wi) pour chaque mot i du corpus\n",
    "        # {'fulton': 0.00020986448277143268, 'county': 0.0006329075767507745,\n",
    "    \n",
    "    #sous-échantillon du corpus\n",
    "    words = np.array(list(word_counts.keys())) #['fulton' 'county' ... 'fuller']\n",
    "    while True:\n",
    "        word_list = [] #création d'une liste de mots\n",
    "        # list(sample_probability.values()) = [0.00020986448277143268, 0.0006329075767507745, \n",
    "        #de taille 12 132 c'est à dire la taille du vocabulaire\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
    "        # vecteur de 12132 variables de Bernoulli appartenant à [0;sample_size=8]\n",
    "        for index, count in enumerate(sampled_index): #on indicie le vecteur sampled_index (1, 0), (2, 0), (3, 0)\n",
    "            for _ in range(count):\n",
    "                 word_list.append(words[index])\n",
    "        yield word_list #comme un return sauf que retourne un générateur à la place d'un vecteur\n",
    "        #return(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10771 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "## modifié car sinon trop long à tourner...\n",
    "\n",
    "import numpy as np\n",
    "context_tuple_list = []\n",
    "w = 4\n",
    "#negative_samples = sample_negative(8)\n",
    "negative_samples = sample_negative(3)\n",
    "\n",
    "for text in corpus[1:2]: #pour tous les articles du corpus [['austin', 'texas', 'committee',\n",
    "#for text in corpus:\n",
    "    for i, word in enumerate(text): #pour chaque indice de mot et mot de l'article\n",
    "        first_context_word_index = max(0,i-w) #premier indice de mot contexte associé au target word\n",
    "        last_context_word_index = min(i+w, len(text)) #dernier indice de mot context\n",
    "        for j in range(first_context_word_index, last_context_word_index): \n",
    "            #pour tous les indices de mots contextes associé (y compris le mot target du coup)\n",
    "            if i!=j: #si on exclut le mot target de ces mots contextes\n",
    "                context_tuple_list.append((word, text[j], next(negative_samples))) \n",
    "                #j'ajoute à context_tuple_list, le mot de l'article, son contexte,\n",
    "                #et l'échantillon de mot du negative sample\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context_tuple_list est de cette forme si sample_negative(3) : \n",
    "\n",
    "    context_tuple_list[0] : ('austin', 'texas', ['absent', 'going', 'recovering'])\n",
    "    context_tuple_list[1] : ('austin', 'committee', ['concluded', 'loading', 'drawn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Echantillonnage des paires (*pair sampling*)\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">TODO</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le réseau\n",
    "\n",
    "La principale différence avec le réseau présenté ci-dessus réside dans le fait que nous n’avons plus besoin d’une distribution de probabilité sur les mots en sortie. Nous pouvons plutôt avoir une probabilité pour chaque mot. Pour obtenir cela, nous pouvons remplacer la sortie softmax out par une sigmoïde, prenant des valeurs comprises entre 0 et 1.\n",
    "\n",
    "L'autre principale différence est que la perte doit être calculée sur la sortie des observations uniquement, car nous fournissons la sortie attendue ainsi qu'un ensemble d'exemples négatifs. Pour ce faire, nous pouvons utiliser un logarithme négatif de la sortie en tant que fonction de perte.\n",
    "\n",
    "Pour un mot target $w_T$, un mot de contexte $w_C$ et un exemple négatif $w_N$, les mots-vecteurs (*embeddings*) respectifs sont définis comme étant $e_T$, $e_C$ et $e_N$. La fonction de perte (*loss*) l est définie comme suit :\n",
    "\n",
    "$$l = -log(\\sigma(e_T^T e_C)) - \\displaystyle\\sum_i log(\\sigma(- e_T^T e_{N,i}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_example):\n",
    "        emb_target = self.embeddings_target(target_word)\n",
    "        emb_context = self.embeddings_context(context_word)\n",
    "        emb_product = torch.mul(emb_target, emb_context)\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out = torch.sum(F.logsigmoid(emb_product))\n",
    "        emb_negative = self.embeddings_context(negative_example)\n",
    "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out += torch.sum(F.logsigmoid(-emb_product))\n",
    "        return -out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau de neurones est entraîné avec les paramètres suivants:\n",
    "\n",
    "* taille d'intégration : 200 (*embedding size*)\n",
    "* taille du lot : 2000 (*batch size*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-301-fc39e8b81444>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mcontext_tuple_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_tuple_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_tuple_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-223-ccaf7442a5ed>\u001b[0m in \u001b[0;36mget_batches\u001b[1;34m(context_tuple_list, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbatch_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontext_tuple_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#on récupère l'id du contexte\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]]) ## ???????\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mbatch_negative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext_tuple_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_tuple_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#si i est le dernier élément\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mtensor_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-223-ccaf7442a5ed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbatch_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontext_tuple_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#on récupère l'id du contexte\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]]) ## ???????\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mbatch_negative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext_tuple_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_tuple_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#si i est le dernier élément\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mtensor_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "## modifié car sinon trop long à tourner...\n",
    "\n",
    "import time\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#net = Word2Vec(embedding_size=200, vocab_size=vocabulary_size)\n",
    "net = Word2Vec(embedding_size=10, vocab_size=vocabulary_size)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    for i in range(len(context_tuple_batches)):\n",
    "        net.zero_grad()\n",
    "        target_tensor, context_tensor, negative_tensor = context_tuple_batches[i]\n",
    "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "    print(\"Loss: \", np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\">à débugger ci-dessus...</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le réseau entraîné, nous pouvons utiliser le mot-vecteur (*word embedding*) et calculer la similarité entre les mots. La fonction suivante calcule les n premiers mots les plus proches pour un mot donné. La similitude utilisée est le cosinus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = net.embeddings_target\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('knox', 1.4831182956695557),\n",
       " ('integration', 1.9943172931671143),\n",
       " ('severely', 2.1332778930664062),\n",
       " ('easier', 2.3286726474761963),\n",
       " ('officers', 2.3364176750183105)]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_word(\"love\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
