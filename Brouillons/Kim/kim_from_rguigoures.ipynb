{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel - Word2vec en utilisant Pytorch\n",
    "\n",
    "Ce notebook explique comment implémenter la technique de NLP, appelée word2vec, à l’aide de Pytorch. Word2vec a pour objectif principal de construire un \"word embedding\", c’est-à-dire une représentation de mots (\"latent and semantic free\") dans un espace continu. Pour ce faire, cette approche exploite un réseau de neurones peu profond, avec seulement 2 couches. Ce tutoriel explique : \n",
    "\n",
    "* comment générer l'ensemble de données adapté à word2vec\n",
    "* comment construire le réseau de neurones\n",
    "* comment accélérer l'approche\n",
    "\n",
    "\n",
    "## Les données\n",
    "\n",
    "Présentons les concepts de base du NLP : \n",
    "\n",
    "* Corpus : le corpus est la collection de textes définissant le jeu de données\n",
    "* vocabulaire: l'ensemble des mots contenu dans les données\n",
    "\n",
    "En guise d'exemple, nous utilisons le nouveau corpus contenu dans la base de données \"Brown\", disponible dans la librairie nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement des différentes bibliothèques... A faire une fois\n",
    "#nltk.download('universal_tagset') #Pour traduire les types de tags\n",
    "#nltk.download('brown')\n",
    "#nltk.download('words')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('names')\n",
    "#nltk.download('cmudict')\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexiques et listes de mots dans nltk\n",
    "\n",
    "Le package NLTK comprend également un certain nombre de lexiques et de listes de mots. Celles-ci sont accessibles comme des corpus de texte. Les exemples suivants illustrent l’utilisation des corpus de la liste de mots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'en-basic']\n",
      "['a', 'aa', 'aal', 'aalii']\n",
      "235886\n",
      "['azerbaijani', 'danish', 'dutch', 'english']\n",
      "['aux', 'avec', 'ce', 'ces']\n",
      "['female.txt', 'male.txt']\n",
      "['Abagail', 'Abbe', 'Abbey', 'Abbi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names, stopwords, words\n",
    "\n",
    "print(words.fileids())\n",
    "print(words.words('en')[1:5]) \n",
    "print(len(words.words('en')))\n",
    "print(stopwords.fileids()[1:5])\n",
    "print(stopwords.words('french')[1:5])\n",
    "print(names.fileids())\n",
    "print(names.words('female.txt')[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le corpus du dictionnaire de prononciation CMU contient des transcriptions de plus de 100 000 mots. Vous pouvez y accéder sous forme de liste d'entrées (chaque entrée étant composée d'un mot, d'un identifiant et d'une transcription) ou sous forme de dictionnaire de mots en listes de transcriptions. Les transcriptions sont codées sous forme de n-uplets de chaînes de phonèmes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('acetate', ['AE1', 'S', 'AH0', 'T', 'EY2', 'T'])\n",
      "[['K', 'IH1', 'M'], ['AA0', 'N', 'T', 'UW1', 'N', 'EH0', 'Z']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "print(cmudict.entries()[653]) \n",
    "# charger le corpus entier cmudict dans le dictionnaire python:\n",
    "transcr = cmudict.dict()\n",
    "print([transcr[w][0] for w in 'Kim Antunez'.lower().split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appropriation des données de type corpus\n",
    "\n",
    "Commençons par nous approprier le fonctionnement du format \"corpus\" dans Python...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROWN CORPUS\n",
      "\n",
      "A Standard Corpus of Present-Day Edited American\n",
      "English, for use with Digital Computers.\n",
      "\n",
      "by W. N. Francis and H. Kucera (1964)\n",
      "Department of Linguistics, Brown University\n",
      "Providence, Rhode Island, USA\n",
      "\n",
      "Revised 1971, Revised and Amplified 1979\n",
      "\n",
      "http://www.hit.uib.no/icame/brown/bcm.html\n",
      "\n",
      "Distributed with the permission of the copyright holder,\n",
      "redistribution permitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Description du corpus brown\n",
    "print(brown.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemin de la base de données Brown : \n",
      "<CategorizedTaggedCorpusReader in 'W:/AppData/nltk_data/corpora/brown'>\n",
      "\n",
      " 10 premiers noms de fichiers de la base Brown : \n",
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10']\n",
      "\n",
      " 100 premiers caractères du fichier ca01 la base Brown : \n",
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn\n",
      "\n",
      " 10 premiers mots du fichier ca01 la base Brown : \n",
      "['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
      "\n",
      " Mots de la base Brown en entier : \n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "print(\"Chemin de la base de données Brown : \")\n",
    "print(str(nltk.corpus.brown).replace('\\\\\\\\','/'))\n",
    "\n",
    "print(\"Nombre de mots du fichier ca01 la base Brown \")\n",
    "len(brown.words('ca01'))\n",
    "\n",
    "print(\"\\n 10 premiers noms de fichiers de la base Brown : \")\n",
    "print(brown.fileids()[:10])\n",
    "\n",
    "print(\"\\n 100 premiers caractères du fichier ca01 la base Brown : \")\n",
    "print(brown.raw('ca01')[:100])\n",
    "\n",
    "print(\"\\n 10 premiers mots du fichier ca01 la base Brown : \")\n",
    "print(brown.words('ca01')[1:10])\n",
    "\n",
    "#print(\"\\n Première phrase du fichier ca01 la base Brown : \")\n",
    "#print(brown.sents('ca01')[1:10])\n",
    "\n",
    "#print(\"\\n Premier paragraphe du fichier ca01 la base Brown : \")\n",
    "#print(brown.paras('ca01'))\n",
    "\n",
    "print(\"\\n Mots de la base Brown en entier : \")\n",
    "print(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base Brown a la particularité d'être annotée (pas seulement en texte plein). Elle est annotée avec des balises de partie de parole et définit des méthodes supplémentaires étiquetées_*(), dans lesquels les mots sont des nuplets (mot, balise), plutôt que de simples chaînes de mots. Ici les tags semblent correspondre au type des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(brown.tagged_words())\n",
    "print(brown.tagged_words(tagset='universal')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs corpus inclus dans NLTK contiennent des documents classés par sujet, genre, polarité, etc. Outre l’interface de corpus standard, ces corpus permettent d’accéder à la liste des catégories et à l'association entre les documents et leurs catégories (dans les deux sens). On peut accéder aux catégories à l'aide de la méthode categories(). Cette méthode a un argument facultatif qui spécifie un document ou une liste de documents, nous permettant d'associer (un ou plusieurs) documents vers (une ou plusieurs) catégories.\n",
    "\n",
    "Outre la mise en correspondance des catégories et des documents, ces corpus permettent un accès direct à leur contenu via les catégories. Au lieu d'accéder à un sous-ensemble d'un corpus en spécifiant un ou plusieurs ID de fichier, nous pouvons identifier une ou plusieurs catégories, ici la catégorie news. Notez que qu'il le faut pas préciser à la fois les documents et catégories, sinon cela renverait une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['belles_lettres', 'editorial', 'fiction', 'government']\n",
      "['editorial', 'news']\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories()[1:5]) \n",
    "print(brown.categories(['ca01','cb01']))\n",
    "print(brown.tagged_words(categories='news'))\n",
    "\n",
    "#Dans le contexte d'un système de catégorisation de texte,\n",
    "#nous pouvons facilement tester si la catégorie attribuée à un document est correcte :\n",
    "def classify(doc): return 'news'   #\n",
    "doc = 'ca01'\n",
    "print(classify(doc) in brown.categories(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparations de nos données dans brown\n",
    "\n",
    "Revenons au traitement de la base brown pour le NLP. \n",
    "\n",
    "Ici, les caractères autres que des lettres sont supprimés de la chaîne. De plus, le texte est mis en minuscule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of']\n"
     ]
    }
   ],
   "source": [
    "#print(brown.fileids('news')[1:5])\n",
    "#print(brown.sents('ca01')[0][1:5]) #5 premiers mots de la 1e phrase\n",
    "#raw_text = list(itertools.chain.from_iterable(brown.sents('ca01')))[1:5] #idem sans préciser de numéro de phrase\n",
    "#print(' '.join(raw_text))\n",
    "\n",
    "import itertools  #NEW KIM\n",
    "\n",
    "for cat in ['news']:  #On se restreint à la catégorie \"news\" des articles contenus dans brown. \n",
    "    for text_id in brown.fileids(cat): #On parcourt tous les fichiers de la catégorie news\n",
    "        raw_text = list(itertools.chain.from_iterable(brown.sents(text_id))) #on récupère les mots\n",
    "        text = ' '.join(raw_text) #justaxposer les mots en mettant des espaces\n",
    "        text = text.lower() #texte mis en minuscule. \n",
    "        text.replace('\\n', ' ') #on supprime les sauts de lignes\n",
    "        text = re.sub('[^a-z ]+', '', text) #on enlève les caractères non alphabétiques\n",
    "        corpus.append([w for w in text.split() if w != '']) #on intègre ces mots sous forme de \n",
    "        # liste de liste dans corpus\n",
    "\n",
    "print(corpus[0][1:10]) #Les 10 premiers mots du premier texte (ca01) de corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sous-échantillonnage des mots fréquents\n",
    "\n",
    "La première étape du prétraitement des données consiste à équilibrer les occurrences de mots dans les données. Pour ce faire, nous effectuons un sous-échantillonnage des mots fréquents. Appelons $p_i$ la proportion du mot i dans le corpus. Alors la probabilité $P(wi)$ de garder le mot dans le corpus est définie comme suit :\n",
    "\n",
    "\n",
    "\n",
    "$$P(w_i) = \\dfrac{10^{-3}}{p_i}\\left(\\sqrt{10^3 p_i} + 1\\right)$$\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">Formule à comprendre</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random, math\n",
    "\n",
    "# On fixe l'aléa pour bien pouvoir comprendre nos résultats\n",
    "random.seed(1)\n",
    "\n",
    "#Retourne un float aléatoire dans l'intervalle [0.0, 1.0).\n",
    "#print(random.random())\n",
    "\n",
    "# Fonction qui en entrée prend un corpus et renvoie un corpus filtré\n",
    "def subsample_frequent_words(corpus): #création d'une fonction qui prend un corpus en entrée\n",
    "    filtered_corpus = [] \n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus)))) #{'the': 159650, 'fulton': 350, ...\n",
    "    sum_word_counts = sum(list(word_counts.values())) #nombre total de mots\n",
    "    word_counts = {word: word_counts[word]/float(sum_word_counts) for word in word_counts} \n",
    "    # calculs de proportion dont la somme fait 100 {'the': 0.07339892418739369, 'fulton': 0.00016091214197048412,\n",
    "    for text in corpus: #on parcourt tous les articles du corpus\n",
    "        filtered_corpus.append([]) #on crée une sous liste à chaque nouvel article\n",
    "        for word in text: #et pour tous les mots de l'article\n",
    "            if random.random() < (1+math.sqrt(word_counts[word] * 1e3)) * 1e-3 / float(word_counts[word]):\n",
    "                #si une proportion tirée au hasard est inférieure à la probabilité de garder ce mot\n",
    "                filtered_corpus[-1].append(word) #on enlève ce mot. \n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\"> La condition random < P(wi) ne me semble pas intuitive</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On applique la fonction de filtrage au corpus\n",
    "corpus = subsample_frequent_words(corpus)\n",
    "vocabulary = set(itertools.chain.from_iterable(corpus)) # transforme le nouveau corpus au format\n",
    "#{'bernadines', 'removed', 'lining',\n",
    "\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)} # on associe un index a chaque mot\n",
    "#{'bernadines': 0, 'removed': 1,\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)} # on associe un mot a chaque index\n",
    "#{0: 'bernadines', 1: 'removed',\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\"> Je ne comprends pas pourquoi vocabulary est réordonné </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire des sacs de mots\n",
    "\n",
    "Word2vec est une approche de \"sac\" de mots. Pour chaque mot de l'ensemble de données, nous devons extraire les mots de contexte (*context words*), c'est-à-dire les mots voisins dans une certaine fenêtre (*window*) de longueur fixe. Par exemple, dans la phrase suivante :\n",
    "\n",
    "*My cat is lazy, it sleeps all day long*\n",
    "\n",
    "Si nous considérons le mot cible (*target word*) *lazy* et que nous choisissons une fenêtre de taille 2, les mots du contexte sont *cat*, *is*, *it* et *sleeps*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return type: <class 'enumerate'>\n",
      "[(0, 'eat'), (1, 'sleep'), (2, 'repeat')]\n",
      "[(2, 'g'), (3, 'e'), (4, 'e'), (5, 'k')]\n"
     ]
    }
   ],
   "source": [
    "#Petit aparté pour comprendre la fonction enumerate de Python\n",
    "l1 = [\"eat\",\"sleep\",\"repeat\"] \n",
    "s1 = \"geek\"\n",
    "obj1 = enumerate(l1) \n",
    "obj2 = enumerate(s1)   \n",
    "print(\"Return type:\",type(obj1))\n",
    "print(list(enumerate(l1))) #C'est plutôt cette fonction d'indiciation de mot qu'on utilise ci-dessous\n",
    "print(list(enumerate(s1,2))) #Ici on compte le nombre de lettres dans un mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 10702487 paires de mots cibles et de contexte\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#print(list(enumerate(corpus[0]))[:10])\n",
    "#[(0, 'fulton'), (1, 'county'),\n",
    "\n",
    "context_tuple_list = []\n",
    "w = 4 #taille de la fenêtre = 4\n",
    "\n",
    "# créer des pairs de mots cibles et contexte\n",
    "for text in corpus: #pour tous les articles du corpus (nettoyé par la méthodes ci-dessus)\n",
    "    for i, word in enumerate(text): #pour chaque association nb de mots (i) + mot (word)\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j]))\n",
    "                \n",
    "#print(context_tuple_list[0:10])    \n",
    "#[('fulton', 'county'), ('fulton', 'grand'), ('fulton', 'jury'), ('county', 'fulton')...\n",
    "print(\"Il y a {} paires de mots cibles et de contexte\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFFF00\"> Tester les autres types de nettoyage ? Negative et pair sampling </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire le réseau\n",
    "\n",
    "Il existe deux approches pour word2vec : \n",
    "\n",
    "* CBOW (*Continuous Bag Of Words* ou sac continu de mots). Il prédit le mot cible conditionnellement au contexte. En d'autres termes, les mots de contexte sont l'entrée et le mot cible est la sortie.\n",
    "\n",
    "* Skip-gram. Il prédit le contexte conditionnellement au mot cible. En d'autres termes, le mot cible est l'entrée et les mots de contexte sont la sortie.\n",
    "\n",
    "Le code suivant concerne la méthode CBOW.\n",
    "\n",
    "Le vocabulaire est représenté sous la forme d'un codage à une seule étape <span style=\"background-color: #FFFF00\">(traduction?)</span> (*one hot encoding*), ce qui signifie que la variable d'entrée est un vecteur de la taille du vocabulaire (de taille n si n mots). Pour un mot, ce vecteur vaut 0 partout sauf à l'endroit de l'indice du mot dans le vocabulaire (i pour le ième mot et $x_i$ = 1).\n",
    "\n",
    "Le codage à une étape est mappé <span style=\"background-color: #FFFF00\">(traduction?)</span> (*mapped*)  sur un *embedding* <span style=\"background-color: #FFFF00\">(traduction?)</span>, c'est-à-dire une représentation latente du mot en tant que vecteur contenant des valeurs continues et dont la taille est plus petit que le vecteur de codage *one-hot*.\n",
    "\n",
    "Pour chaque mot de contexte, une fonction softmax prend l'*embedding du mot*, produisant une distribution de probabilité du mot cible sur le vocabulaire.\n",
    "\n",
    "![alt text](https://rguigoures.github.io/images/cbow.png \"Schéma de la méthode CBOW\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelques précisions méthodologiques \n",
    "\n",
    "#### la fonction zero_grad\n",
    "\n",
    "La fonction  `zero_grad` initialise les gradients de tous les paramètres du modèle à zéro.\n",
    "\n",
    "Dans PyTorch, nous devons initialiser les gradients à 0 avant de commencer à effectuer une rétroprojection *backpropagation* <span style=\"background-color: #FFFF00\">(= descente de gradient ?)</span> , car PyTorch accumule les gradients à chaque nouveau passage. Ceci est pratique quand on entraîne des RNN <span style=\"background-color: #FFFF00\">(revoir RNN)</span>. Ainsi, l’action par défaut consiste à accumuler (c’est-à-dire à sommer) les gradients à chaque appel de `loss.backward()`.\n",
    "\n",
    "De ce fait, lorsque vous démarrez votre boucle d’entraînement, vous devez idéalement mettre à zéro les gradients afin de mettre à jour le paramètre correctement. Sinon, le gradient indiquerait une direction autre que la direction souhaitée vers le minimum (ou le maximum, dans le cas d'objectifs de maximisation).\n",
    "\n",
    "Voici un exemple pour comprendre : \n",
    "\n",
    "<span style=\"background-color: #FFFF00\">sauf que j'ai rien compris => Comprendre avant à quoi correspond une descente de gradient ! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], requires_grad=True)\n",
      "tensor([[0.]], grad_fn=<SinBackward>)\n",
      "None\n",
      "tensor([[6.]])\n",
      "tensor([[0.]], requires_grad=True)\n",
      "tensor([[1.]])\n",
      "tensor([[0.]], grad_fn=<SinBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "x = Variable(torch.Tensor([[0]]), requires_grad=True)\n",
    "print(x)\n",
    "print(x.sin())\n",
    "print(x.sin().backward())\n",
    "\n",
    "for t in range(5):\n",
    "    y = x.sin() \n",
    "    y.backward()\n",
    "    \n",
    "print(x.grad) # shows 5\n",
    "\n",
    "#Calling x.grad.data.zero_() before y.backward() can make sure x.grad is exactly\n",
    "#the same as current y’(x), not a sum of y’(x) in all previous iterations.\n",
    "\n",
    "x = Variable(torch.Tensor([[0]]), requires_grad=True) \n",
    "print(x)\n",
    "\n",
    "for t in range(5):\n",
    "    if x.grad is not None:\n",
    "        x.grad.data.zero_()\n",
    "    y = x.sin() \n",
    "    y.backward()\n",
    "\n",
    "print(x.grad) # shows 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### la fonction Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nntorch.nn  as  nn\n",
    "import torch.nn  as  nn #KIM\n",
    "#import torch.autogradtorch.aut  as autograd\n",
    "import torch.autograd  as  autograd #KIM\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context_word):\n",
    "        emb = self.embeddings(context_word)\n",
    "        hidden = self.linear(emb)\n",
    "        #out = F.log_softmax(hidden)\n",
    "        out = F.log_softmax(hidden, dim=1)# KIM For matrices, it’s 1. For others, it’s 0.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrêter l'algorithme avant la fin\n",
    "\n",
    "Avant de commencer l’apprentissage, introduisons le concept d’arrêt précoce (*early stopping*). Il vise à arrêter l'apprentissage lorsque la perte (*loss*) ne diminue pas de manière significative (paramètre  `min_percent_gain`) après un certain nombre d'itérations (paramètre  `patience`). Un arrêt précoce est généralement utilisé sur la perte de validation (*validation loss*), mais dans le cas de word2vec, il n’y a pas de validation car l’approche n’est pas supervisée. Nous appliquons plutôt l'arrêt précoce sur les données d'entraînement à la place (*training loss*).\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">mieux comprendre le concept de validation et d'apprentissage supervisé</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
    "        self.patience = patience\n",
    "        self.loss_list = []\n",
    "        self.min_percent_gain = min_percent_gain / 100.\n",
    "        \n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "    \n",
    "    def stop_training(self):\n",
    "        if len(self.loss_list) == 1:\n",
    "            return False\n",
    "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
    "        print(\"Loss gain: {}%\".format(round(100*gain,2)))\n",
    "        if gain < self.min_percent_gain:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage\n",
    "\n",
    "Pour l'apprentissage (*learning*), nous utilisons l'entropie croisée (*cross entropy*) comme fonction de loss. Le réseau de neurones est entraîné avec les paramètres suivants:\n",
    "\n",
    "* taille d'intégration : 200 (*embedding size*)\n",
    "* taille du lot : 2000A (*batch size*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modifié car sinon trop long à tourner...\n",
    "vocabulary_size = len(vocabulary) #12132\n",
    "net = Word2Vec(embedding_size=2, vocab_size=vocabulary_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "#early_stopping = EarlyStopping()\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
    "context_tensor_list = []\n",
    "\n",
    "## Etape 1 : On transforme les couple target context en couple de tensor (autre format)\n",
    "for target, context in context_tuple_list[0:10]: #[('fulton', 'county'), ('fulton', 'grand')\n",
    "#for target, context in context_tuple_list: \n",
    "    target_tensor = autograd.Variable(torch.LongTensor([word_to_index[target]])) #fulton devient tensor([6582])\n",
    "    context_tensor = autograd.Variable(torch.LongTensor([word_to_index[context]]))  #county devient tensor([6582])\n",
    "    context_tensor_list.append((target_tensor, context_tensor)) \n",
    "    #(tensor([6582]), tensor([8950])) ajouté à context_tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Etape 2 : \n",
    "while True:\n",
    "    losses = []\n",
    "    for target_tensor, context_tensor in context_tensor_list:\n",
    "        net.zero_grad()\n",
    "        log_probs = net(context_tensor)\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "    print(\"Loss: \", np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed up the approach\n",
    "\n",
    "The implementation introduced is pretty slow. But good news, there are solutions for speeding up the computation.\n",
    "\n",
    "### Batch learning\n",
    "\n",
    "In order to speed up the learning, we propose to use batches. This implies that a bunch of observations are forwarded through the network before doing the backpropagation. Besides being faster, this is also a good way to regularize the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_target, batch_context, batch_negative = [], [], []\n",
    "    for i in range(len(context_tuple_list)):\n",
    "        batch_target.append(word_to_index[context_tuple_list[i][0]])\n",
    "        batch_context.append(word_to_index[context_tuple_list[i][1]])\n",
    "        batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]])\n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
    "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
    "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
    "            tensor_negative = autograd.Variable(torch.from_numpy(np.array(batch_negative)).long())\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative examples\n",
    "\n",
    "The default word2vec algorithm exploits only positive examples and the output function is a softmax. However, using a softmax slows down the learning: softmax is normalized over all the vocabulary, then all the weights of the network are updated at each iteration. Consequently we decide using a sigmoid function as an output instead: only the weights involving the target word are updated. But then the network does not learn from negative examples anymore. That’s why we need to input artificially generated negative examples.\n",
    "\n",
    "Once we have built the data for the positive examples, i.e the words in the neighborhood of the target word, we need to build a data set with negative examples. For each word in the corpus, the probability of sampling a negative context word is defined as follows:\n",
    "\n",
    "$$P(w_i) = \\dfrac{\\mid w_i \\mid^{\\frac{3}{4}}}{\\displaystyle\\sum_{j=1}^n\\mid w_j \\mid^{\\frac{3}{4}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "\n",
    "def sample_negative(sample_size):\n",
    "    sample_probability = {}\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "    for word in word_counts:\n",
    "        sample_probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
    "    words = np.array(list(word_counts.keys()))\n",
    "    while True:\n",
    "        word_list = []\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                 word_list.append(words[index])\n",
    "        yield word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4643 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "## modifié car sinon trop long à tourner...\n",
    "\n",
    "import numpy as np\n",
    "context_tuple_list = []\n",
    "#w = 4\n",
    "w = 2\n",
    "#negative_samples = sample_negative(8)\n",
    "negative_samples = sample_negative(2)\n",
    "\n",
    "for text in corpus[1:2]:\n",
    "#for text in corpus:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j], next(negative_samples)))\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network\n",
    "\n",
    "The main difference from the network introduced above lies in the fact that we don’t need a probability distribution over words as an output anymore. We can instead have a probability for each word. To get that, we can replace the softmax out output by a sigmoid, taking values between 0 and 1.\n",
    "\n",
    "The other main difference is that the loss needs to be computed on the observe output only, since we provide the expected output as well as a set of negative examples. To do so, we can use a negative logarithm of the output as a loss function.\n",
    "\n",
    "For a target word $w_T$, a context word $w_C$ and a negative example $w_N$, respective embeddings are defined as $e_T$, $e_C$ and $e_N$. The loss function l is defined as follows:\n",
    "\n",
    "$$l = -log(\\sigma(e_T^T e_C)) - \\displaystyle\\sum_i log(\\sigma(- e_T^T e_{N,i}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_example):\n",
    "        emb_target = self.embeddings_target(target_word)\n",
    "        emb_context = self.embeddings_context(context_word)\n",
    "        emb_product = torch.mul(emb_target, emb_context)\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out = torch.sum(F.logsigmoid(emb_product))\n",
    "        emb_negative = self.embeddings_context(negative_example)\n",
    "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out += torch.sum(F.logsigmoid(-emb_product))\n",
    "        return -out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network in trained with the following parameters: \n",
    "\n",
    "* embedding size: 200\n",
    "* batch size: 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6034.86\n",
      "Loss:  5976.067\n",
      "Loss gain: 0.97%\n"
     ]
    }
   ],
   "source": [
    "## modifié car sinon trop long à tourner...\n",
    "\n",
    "import time\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#net = Word2Vec(embedding_size=200, vocab_size=vocabulary_size)\n",
    "net = Word2Vec(embedding_size=10, vocab_size=vocabulary_size)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    for i in range(len(context_tuple_batches)):\n",
    "        net.zero_grad()\n",
    "        target_tensor, context_tensor, negative_tensor = context_tuple_batches[i]\n",
    "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "    print(\"Loss: \", np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network trained, we can use the word embedding and compute the similarity between words. The following function computes the top n closest words for a given word. The similarity used is the cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_closest_word(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = net.embeddings_target\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
    "    word_distance.sort(key=lambda x: x[1])\n",
    "    return word_distance[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fastgrossing', 2.5288169384002686),\n",
       " ('vienna', 2.7083771228790283),\n",
       " ('before', 2.883903741836548),\n",
       " ('kowalskis', 2.9128899574279785),\n",
       " ('loosely', 2.92732834815979)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_word(\"mother\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
