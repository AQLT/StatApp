{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "os.chdir('C:/Users/Kim Antunez/Documents/Projets_autres')\n",
    "print(string.punctuation + \"'’\")\n",
    "def mise_en_forme_phrase (phrase):\n",
    "    phrase = phrase.lower()\n",
    "    # On elève la ponctuation mais ça peut se discuter (garder les @ et #?)\n",
    "    phrase = re.sub('( @[^ ]*)|(^@[^ ]*)',\"nickname\", phrase) #Remplace @... par nickname\n",
    "    #supprime toutes les ponctuations par défaut + les apostrophes bizarres\n",
    "    phrase = phrase.translate(str.maketrans('', '', string.punctuation + \"'’\"))\n",
    "    # On enlève les passages à la ligne\n",
    "    phrase = re.sub('\\\\n', ' ', phrase)\n",
    "    # On enlève les espaces multiples et les espaces à la fin des phrases\n",
    "    phrase = re.sub(' +', ' ', phrase)\n",
    "    phrase = re.sub(' +$', '', phrase)\n",
    "    return(phrase.split())\n",
    "#f = open('data/sample_3.txt')\n",
    "#raw = f.read()\n",
    "#print(type(raw))\n",
    "with open('data/sample_3.txt', encoding=\"utf-8\") as myfile:\n",
    "    phrases = [mise_en_forme_phrase(next(myfile)) for x in range(10000)]\n",
    "print(phrases[0:10])\n",
    "#raw = ''.join([''.join(phrase) for phrase in phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "# words = word_tokenize(raw) # Plus utile maintenant\n",
    "words = [item for sublist in phrases for item in sublist]\n",
    "print(type(words))\n",
    "## On enlève la ponctuation et on met en minuscule :\n",
    "#words = [word.lower() for word in words if word.isalpha()] # plus utile maintenant\n",
    "vocabulary = set(words)\n",
    "\n",
    "print(words[1:10])\n",
    "print(type(vocabulary))\n",
    "print(list(vocabulary)[1:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de mots :\", len(words))\n",
    "print(\"Taille du vocabulaire :\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pour changer la taille des graphiques :\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "fdist = nltk.FreqDist(words)\n",
    "print(\"Les 10 mots les plus communs sont :\")\n",
    "print(fdist.most_common(10))\n",
    "print(\"Les 2 premiers mots et leur occurrence sont :\")\n",
    "for cle, valeur in list(fdist.items())[0:2]:\n",
    "    print(\"{} ({} occurrences)\".format(cle, valeur))\n",
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling rate\n",
    "On va simplifier un peu le corpus en enlevant certains mots. Pour cela on va faire un sous-échantillonnage du corpus pour supprimer certains mots. \n",
    "\n",
    "Pour chaque mot $w_i$ on note $z(w_i)$ la proportion d'apparition de ce mot, c'est-à-dire le rapport entre le nombre de fois que ce mot apparait et le nombre total de mots. La probabilité de garder un mot le mot $w_i$ est :\n",
    "$$\n",
    "\\mathbb P(w_i) = \\left(\\sqrt{\\frac{z(w_i)}{q}} + 1 \\right)\n",
    "\\times\n",
    "\\frac{q}{z(w_i)}\n",
    "$$\n",
    "Le paramètre $q$ est appelé \"sample\" – échantillonnage – contrôle le nombre de sous-échantillonnages. La valeur par défaut est 0,001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calcul_proba(x):\n",
    "    result = (sqrt(x)+1)*(1/x)\n",
    "    return(result)\n",
    "calcul_proba_v = np.vectorize(calcul_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération de l'échantillon de test\n",
    "Comment on gère les doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_echantillon(phrases, vocabulary , probabilities_subsampling,  window = 2):\n",
    "    #Sub-sampling\n",
    "    nouveau_corpus = [] \n",
    "    for phrase in phrases: #on parcourt tous les articles du corpus\n",
    "        nouveau_corpus.append([]) #on crée une sous liste à chaque nouvel article\n",
    "        for word in phrase: #et pour tous les mots de l'article\n",
    "        # Les mots à supprimer sont les mots tels que la loi générée U([0,1]) soit > proba\n",
    "        # On garde donc les mots si U([0,1]) <= proba\n",
    "            proba_w = probabilities_subsampling[vocabulary.index(word)]\n",
    "            if np.random.uniform(low=0.0, high=1.0) <= proba_w:\n",
    "                nouveau_corpus[-1].append(word)\n",
    "    #On ne garde que les phrases de plus d'un mot. \n",
    "    phrases = [phrase for phrase in nouveau_corpus if len(phrase)>1]\n",
    "    test_sample = []\n",
    "    for phrase in phrases:\n",
    "        # On tire au hasard un mot focus et on récupère son index\n",
    "        focus = list(range(0, len(phrase)))\n",
    "        focus = random.choice(focus)\n",
    "        i = focus\n",
    "        index_i = vocabulary.index(phrase[i])\n",
    "        # On tire au hasard un mot contexte dans la fenêtre de ce mot focus et on récupère son index\n",
    "        i_contexte = list(range(max(i-window,0), min(i+window+1, len(phrase))))\n",
    "        i_contexte.remove(i)\n",
    "        i_contexte = random.choice(i_contexte)\n",
    "        j = i_contexte\n",
    "        index_j = vocabulary.index(phrase[j])\n",
    "        test_sample.append([index_i, index_j])\n",
    "    return(test_sample)\n",
    "# exemple de test_sample : [6175, 2149]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme avec softmax\n",
    "Si on note $\\theta$ le paramètre à estimer, $L(\\theta)$ la fonction de perte et $\\eta$ le taux d'apprentissage (*learning rate*) alors :\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "\n",
    "Pour le négative sampling, la probabilité de garder le mot $w_i$ est égale à :\n",
    "$$\n",
    "\\mathbb P(w_i) = \\frac{f(w_i)^{3/4}}{\n",
    "\\sum_{j=1}^n f(w_j)^{3/4}\n",
    "}\n",
    "$$\n",
    "Avec $f(w_j)$ la fréquence d'apparition du mot $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "def live_plot(data, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée le vocabulaire, on calcule les proba de subsampling et negative sampling\n",
    "sample = 0.01\n",
    "words = [item for sublist in phrases for item in sublist] #deja fait\n",
    "fdist = nltk.FreqDist(words) #deja fait\n",
    "vocabulary = list(set(words)) #deja fait\n",
    "proportion = np.array([(fdist[w]/ (len(words) * sample)) for w in vocabulary])\n",
    "p_subsampling = calcul_proba_v(proportion)\n",
    "p_negativesampling = np.array([(fdist[w]**(3/4)) for w in vocabulary])\n",
    "p_negativesampling /= p_negativesampling.sum()\n",
    "\n",
    "print(proportion[1:10])\n",
    "print(p_subsampling[1:10])\n",
    "print(p_negativesampling[1:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable ##KIM\n",
    "\n",
    "dim = 10\n",
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "#4 min\n",
    "\n",
    "# Attention: torch.rand génère une loi uniforme et torch.randn une loi normale\n",
    "input = torch.randn(len(vocabulary), dim) #On crée le tensor input de taille taille_voc x dim\n",
    "output = torch.randn(len(vocabulary), dim) #On crée le tensor output de taille taille_voc x dim\n",
    "#input = autograd.Variable(input, requires_grad=True) #On rajoute la possibilité de mettre un gradient\n",
    "input = Variable(torch.Tensor(input), requires_grad=True) #KIM\n",
    "print(\"input avant algo :\", input.data[1])\n",
    "#output = autograd.variable(output, requires_grad=True)\n",
    "output = Variable(torch.Tensor(output), requires_grad=True) #KIM\n",
    "print(\"output avant algo :\",output.data[1],\"\\n\")\n",
    "\n",
    "loss_tot = []\n",
    "\n",
    "start = time.time() #1576368774.8524628\n",
    "\n",
    "###### ETAPE 1 : créer l'échantillon\n",
    "\n",
    "#for i in range(1): #pour chaque époch...\n",
    "i = 0 #ici on se concentre que sur une epoch\n",
    "compteur = 0\n",
    "loss_val = 0\n",
    "#crée autant de couples target / contexte que de phrases\n",
    "test_sample = creer_echantillon(phrases, vocabulary, p_subsampling)\n",
    "\n",
    "print(\"ETAPE 1 : créer l'échantillon\")\n",
    "print(\"10 premiers couples de l'echantillon :\", test_sample[0:9])\n",
    "print(\"taille de l'echantillon (nb de couples) :\", len(test_sample))\n",
    "\n",
    "print(\"\\nETAPE 2 : \")\n",
    "\n",
    "##### ETAPE 2 \n",
    "\n",
    "for focus, context in test_sample[0:3]: #rajout 0:3\n",
    "        compteur+=1\n",
    "        print(\"\\ncompteur : \", compteur)\n",
    "        # Multiplication matricielle: data = 1 x taille_voc\n",
    "        data = torch.matmul(input[focus,], torch.t(output)) #input[focus,] = 1x dim FOIS t(ouput) = dim x taille_voc\n",
    "        print(\"3 premiers elements de data 1xtaille_voc (ce qui bouge) :\", data[:3])\n",
    "        #log_probs = F.log_softmax(data, dim=0)\n",
    "        #loss = F.nll_loss(log_probs.view(1,-1), torch.tensor([context]))\n",
    "        # Il semble que cela combine les deux précédentes fonctions : \n",
    "        # https://pytorch.org/docs/stable/nn.functional.html#cross-entropy\n",
    "        loss = F.cross_entropy(data.view(1,-1), torch.tensor([context]))\n",
    "        print(\"loss :\",loss)\n",
    "        loss_val += loss.data #prend la valeur de loss. Sert à quoi ???\n",
    "        # Pour ensuite dériver les matrices par rapport à la loss\n",
    "        loss.backward()\n",
    "        # Il faut modifier juste le .data pour ne pas perdre la structure\n",
    "        input.data = input.data - learning_rate * input.grad.data  \n",
    "        output.data = output.data - learning_rate * output.grad.data #matrice dim x taille voc avec des petits chiffres partout\n",
    "        input.grad.data.zero_()\n",
    "        output.grad.data.zero_() #### remet la matrice à zéro\n",
    "        loss_val = loss_val / len(vocabulary)\n",
    "        print(\"loss_val :\",loss_val)\n",
    "        loss_tot.append(loss_val)\n",
    "        print(\"loss_tot : \", loss_tot)\n",
    "        # live_plot(loss_tot)\n",
    "        end = time.time()\n",
    "        #print(round((end - start)/60, 2))\n",
    "       #print(input)        \n",
    "       #plt.plot(loss_tot)\n",
    "        \n",
    "        #UserWarning: torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\n",
    "        #UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "dim = 10\n",
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "#4 min\n",
    "\n",
    "# Attention: torch.rand génère une loi uniforme et torch.randn une loi normale\n",
    "input = torch.randn(len(vocabulary), dim)\n",
    "output = torch.randn(len(vocabulary), dim)\n",
    "input = autograd.Variable(input, requires_grad=True)\n",
    "output = autograd.variable(output, requires_grad=True)\n",
    "\n",
    "loss_tot = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "###### ETAPE 1 : créer l'échantillon\n",
    "\n",
    "#for i in range(1): #pour chaque époch...\n",
    "i = 0 #ici on se concentre que sur une epoch\n",
    "compteur = 0\n",
    "loss_val = 0\n",
    "#crée autant de couples target / contexte que de phrases\n",
    "test_sample = creer_echantillon(phrases, vocabulary, p_subsampling)\n",
    "\n",
    "print(\"ETAPE 1 : créer l'échantillon\")\n",
    "print(\"10 premiers couples de l'echantillon :\", test_sample[0:9])\n",
    "print(\"taille de l'echantillon (nb de couples) :\", len(test_sample))\n",
    "\n",
    "print(\"\\nETAPE 2 : \")\n",
    "\n",
    "##### ETAPE 2 \n",
    "\n",
    "for focus, context in test_sample[0:3]: #rajout 0:3\n",
    "        compteur+=1\n",
    "        print(\"\\ncompteur : \", compteur)\n",
    "        # Multiplication matricielle: data = 1 x taille_voc\n",
    "        data = torch.matmul(input[focus,], torch.t(output)) #input[focus,] = 1x dim FOIS t(ouput) = dim x taille_voc\n",
    "        print(\"3 premiers elements de data 1xtaille_voc (ce qui bouge) :\", data[:3])\n",
    "        #log_probs = F.log_softmax(data, dim=0)\n",
    "        #loss = F.nll_loss(log_probs.view(1,-1), torch.tensor([context]))\n",
    "        # Il semble que cela combine les deux précédentes fonctions : \n",
    "        # https://pytorch.org/docs/stable/nn.functional.html#cross-entropy\n",
    "        loss = F.cross_entropy(data.view(1,-1), torch.tensor([context]))\n",
    "        print(\"loss :\",loss)\n",
    "        loss_val += loss.data #prend la valeur de loss. Sert à quoi ???\n",
    "        # Pour ensuite dériver les matrices par rapport à la loss\n",
    "        loss.backward()\n",
    "        # Il faut modifier juste le .data pour ne pas perdre la structure\n",
    "        input.data = input.data - learning_rate * input.grad.data  \n",
    "        output.data = output.data - learning_rate * output.grad.data #matrice dim x taille voc avec des petits chiffres partout\n",
    "        input.grad.data.zero_()\n",
    "        output.grad.data.zero_() #### remet la matrice à zéro\n",
    "        loss_val = loss_val / len(vocabulary)\n",
    "        print(\"loss_val :\",loss_val)\n",
    "        loss_tot.append(loss_val)\n",
    "        print(\"loss_tot : \", loss_tot)\n",
    "        # live_plot(loss_tot)\n",
    "        end = time.time()\n",
    "        #print(round((end - start)/60, 2))\n",
    "       #print(input)        \n",
    "       #plt.plot(loss_tot)\n",
    "\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme avec subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tirage_neg_sampling(vocabulary, proba_negativesampling, focus, context, K = 5):\n",
    "    #proba_negativesampling[focus] = 0\n",
    "    #proba_negativesampling[context] = 0\n",
    "    liste_vocab = list(range(len(vocabulary)))\n",
    "    neg_sampling = np.random.choice(liste_vocab, size=K, p=proba_negativesampling)\n",
    "    #while( (focus in neg_sampling) | (context in neg_sampling)):\n",
    "    #    neg_sampling = np.random.choice(liste_vocab, size=K, p=proba_negativesampling)\n",
    "    return(neg_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2 KIM\n",
    "dim = 10\n",
    "epoch = 50\n",
    "learning_rate = 0.01\n",
    "K = 5 ###NEW\n",
    "\n",
    "input = torch.randn(len(vocabulary), dim)\n",
    "output = torch.randn(len(vocabulary), dim)\n",
    "input = autograd.Variable(input, requires_grad=True)\n",
    "output = autograd.variable(output, requires_grad=True)\n",
    "\n",
    "loss_tot = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "###### ETAPE 1 : créer l'échantillon\n",
    "\n",
    "#for i in range(1): #pour chaque époch...\n",
    "i = 0 #ici on se concentre que sur une epoch\n",
    "compteur = 0\n",
    "loss_val = 0\n",
    "#crée autant de couples target / contexte que de phrases\n",
    "test_sample = creer_echantillon(phrases, vocabulary, p_subsampling)\n",
    "\n",
    "print(\"ETAPE 1 : créer l'échantillon\")\n",
    "print(\"10 premiers couples de l'echantillon :\", test_sample[0:9])\n",
    "print(\"taille de l'echantillon (nb de couples) :\", len(test_sample))\n",
    "\n",
    "print(\"\\nETAPE 2 : \")\n",
    "\n",
    "##### ETAPE 2 \n",
    "\n",
    "\n",
    "for focus, context in test_sample[0:3]: #rajout 0:3   \n",
    "        compteur+=1\n",
    "        print(\"\\ncompteur : \", compteur)\n",
    "        neg_sample = tirage_neg_sampling(vocabulary, p_negativesampling,focus, context, K = K)#DIFF\n",
    "        vect_sample = np.append(context, neg_sample) #DIFF\n",
    "        # Multiplication matricielle: data = 1 x taille_voc\n",
    "        #data = torch.matmul(input[focus,], torch.t(output)) #DIFF #input[focus,] = 1x dim FOIS t(ouput) = dim x taille_voc\n",
    "        data = torch.matmul(input[focus,], torch.t(output[context,])) #DIFF\n",
    "        print(\"data étape 1 (1x1 au lieu de 1xvoc)\", data)\n",
    "        #loss = F.cross_entropy(data.view(1,-1), torch.tensor([context])) #DIFF\n",
    "        loss1 = - F.logsigmoid(data)\n",
    "        print(\"loss1 :\",loss1)\n",
    "        data = torch.matmul(input[focus,], torch.t(output[neg_sample,])) #DIFF\n",
    "        print(\"data étape 2 (1xvoc)\", data)\n",
    "        loss2 = - F.logsigmoid(-data).sum()#DIFF\n",
    "        #loss_val += loss.data #prend la valeur de loss. Sert à quoi ??? #DIFF\n",
    "        loss_val += loss1 + loss2 #DIFF\n",
    "        # Pour ensuite dériver les matrices par rapport à la loss\n",
    "        #loss.backward() #DIFF\n",
    "        (loss1+loss2).backward() #DIFF\n",
    "        # Il faut modifier juste le .data pour ne pas perdre la structure\n",
    "        input.data = input.data - learning_rate * input.grad.data  \n",
    "        output.data = output.data - learning_rate * output.grad.data #matrice dim x taille voc avec des petits chiffres partout\n",
    "        input.grad.data.zero_()\n",
    "        output.grad.data.zero_() #### remet la matrice à zéro\n",
    "        loss_val = loss_val / len(vocabulary)\n",
    "        print(\"loss_val :\",loss_val)\n",
    "        loss_tot.append(loss_val)\n",
    "        print(\"loss_tot : \", loss_tot)\n",
    "        # live_plot(loss_tot)\n",
    "        end = time.time()\n",
    "        #print(round((end - start)/60, 2))\n",
    "       #print(input)        \n",
    "       #plt.plot(loss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2\n",
    "dim = 10\n",
    "epoch = 3\n",
    "learning_rate = 0.01\n",
    "K = 5\n",
    "\n",
    "input = torch.randn(len(vocabulary), dim)\n",
    "output = torch.randn(len(vocabulary), dim)\n",
    "input = autograd.Variable(input, requires_grad=True)\n",
    "output = autograd.variable(output, requires_grad=True)\n",
    "\n",
    "loss_tot = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(epoch):\n",
    "    compteur = 0\n",
    "    #print(i)\n",
    "    loss_val = 0\n",
    "    test_sample = creer_echantillon(phrases, vocabulary, p_subsampling)\n",
    "    for focus, context in test_sample:\n",
    "        compteur+=1\n",
    "        neg_sample = tirage_neg_sampling(vocabulary, p_negativesampling,\n",
    "                                         focus, context,\n",
    "                                         K = K)\n",
    "        vect_sample = np.append(context, neg_sample)\n",
    "        data = torch.matmul(input[focus,], torch.t(output[context,]))\n",
    "        loss1 = - F.logsigmoid(data)\n",
    "\n",
    "        data = torch.matmul(input[focus,], torch.t(output[neg_sample,]))\n",
    "        loss2 = - F.logsigmoid(-data).sum()\n",
    "        #print(loss)\n",
    "        loss_val += loss1 + loss2\n",
    "        # Pour ensuite dériver les matrices par rapport à la loss\n",
    "        (loss1+loss2).backward()\n",
    "        \n",
    "        # Il faut modifier juste le .data pour ne pas perdre la structure\n",
    "        input.data = input.data - learning_rate * input.grad.data\n",
    "        output.data = output.data - learning_rate * output.grad.data\n",
    "        \n",
    "        input.grad.data.zero_()\n",
    "        output.grad.data.zero_()\n",
    "    loss_val = loss_val / len(vocabulary)\n",
    "    loss_tot.append(loss_val)\n",
    "    live_plot(loss_tot)\n",
    "end = time.time()\n",
    "print(round((end - start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = (input + output)/2\n",
    "mot_poids = {vocabulary[index]: poids.detach().numpy() for (index, poids) in enumerate(W3)}\n",
    "for cle, valeur in list(mot_poids.items())[:1]:\n",
    "    print(\"Le mot {} a pour coordonnées {}.\".format(cle, valeur))\n",
    "    \n",
    "\n",
    "def cos_distance(u, v):\n",
    "    return (np.dot(u, v)  / (math.sqrt(np.dot(u, u)) *  (math.sqrt(np.dot(v, v)))))\n",
    "def mot_plus_proche(word, n=10):\n",
    "    word_distance = {}\n",
    "    for mot in mot_poids:\n",
    "        if mot != word:\n",
    "            word_distance[mot] = (cos_distance(mot_poids[mot],(mot_poids[word])))\n",
    "    word_distance = sorted(word_distance.items(), key=lambda t: t[1],reverse=True)\n",
    "    return word_distance[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_plus_proche(\"grand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "liste = [\"homme\",\"femme\",\"chien\",\"chat\",\"vanille\",\"chocolat\",\n",
    "    \"poli\",\"courtois\",\"choqué\",\"horrifié\",\"propre\",\"sale\",\"canapé\",\"fauteuil\",\n",
    "    \"téléphone\",\"portable\",\"voiture\",\"camion\",\"grand\",\"petit\"]\n",
    "\n",
    "df_res = pd.DataFrame.from_dict(mot_poids).transpose()\n",
    "df_res = df_res.loc[liste]\n",
    "df_res['label'] = 'Autre' \n",
    "\n",
    "df_res.loc[\"homme\",'label'] = 'Couple 1'\n",
    "df_res.loc[\"femme\",'label'] = 'Couple 1'\n",
    "\n",
    "df_res.loc[\"chien\",'label'] = 'Couple 2'\n",
    "df_res.loc[\"chat\",'label'] = 'Couple 2'\n",
    "\n",
    "df_res.loc[\"vanille\",'label'] = 'Couple 3'\n",
    "df_res.loc[\"chocolat\",'label'] = 'Couple 3'\n",
    "\n",
    "df_res.loc[\"poli\",'label'] = 'Couple 4'\n",
    "df_res.loc[\"courtois\",'label'] = 'Couple 4'\n",
    "\n",
    "df_res.loc[\"choqué\",'label'] = 'Couple 5'\n",
    "df_res.loc[\"horrifié\",'label'] = 'Couple 5'\n",
    "\n",
    "df_res.loc[\"propre\",'label'] = 'Couple 6'\n",
    "df_res.loc[\"sale\",'label'] = 'Couple 6'\n",
    "\n",
    "df_res.loc[\"canapé\",'label'] = 'Couple 7'\n",
    "df_res.loc[\"fauteuil\",'label'] = 'Couple 7'\n",
    "\n",
    "df_res.loc[\"téléphone\",'label'] = 'Couple 8'\n",
    "df_res.loc[\"portable\",'label'] = 'Couple 8'\n",
    "\n",
    "df_res.loc[\"voiture\",'label'] = 'Couple 9'\n",
    "df_res.loc[\"camion\",'label'] = 'Couple 9'\n",
    "\n",
    "df_res.loc[\"grand\",'label'] = 'Couple 10'\n",
    "df_res.loc[\"petit\",'label'] = 'Couple 10'\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = [0, 1, 2, 3, 4, 5, 6, 7,8 ,9]\n",
    "# Separating out the features\n",
    "x = df_res.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = df_res.loc[:,['label']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['Axe 1', 'Axe 2'])\n",
    "\n",
    "df_res['mot'] = df_res.index.values\n",
    "df_res = df_res.reset_index()\n",
    "\n",
    "finalDf = pd.concat([principalDf, df_res[['label','mot']]],  axis = 1)\n",
    "\n",
    "\n",
    "# Représentation simple statique \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Axe 1', fontsize = 15)\n",
    "ax.set_ylabel('Axe 2', fontsize = 15)\n",
    "ax.set_title('ACP en 2 composantes', fontsize = 20)\n",
    "targets = list(set(df_res['label']))\n",
    "colors = ['blue', 'red', 'green', 'purple', 'pink', 'orange', 'black', 'brown', 'grey', 'magenta']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['label'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'Axe 1']\n",
    "               , finalDf.loc[indicesToKeep, 'Axe 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(input[focus,], torch.t(output[context,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "import random\n",
    "from random import randrange\n",
    "from random import sample \n",
    "\n",
    "from numpy.random import multinomial\n",
    "liste_couples = {\n",
    "    0:(\"homme\",\"femme\"),\n",
    "    1:(\"chien\",\"chat\"),\n",
    "    2:(\"vanille\",\"chocolat\"),\n",
    "    3:(\"poli\",\"courtois\"),\n",
    "    4:(\"choqué\",\"horrifié\"),\n",
    "    5:(\"propre\",\"sale\"),\n",
    "    6:(\"canapé\",\"fauteuil\"),\n",
    "    7:(\"téléphone\",\"portable\"),\n",
    "    8:(\"voiture\",\"camion\"),\n",
    "    9:(\"grand\",\"petit\")\n",
    "}\n",
    "\n",
    "couples_contexte = {\n",
    "(\"homme\",\"femme\") : [\"sexe\",\"genre\",\"mari\",\"épouse\",\"famille\",\"inégalités\",\"enfant\",\"amour\",\"couple\",\"différence\"],                  \n",
    "(\"chien\",\"chat\") : [\"animal\",\"domestique\",\"maison\",\"aboyer\",\"miauler\",\"labrador\",\"poil\",\"pattes\",\"spa\",\"ami\"],\n",
    "(\"vanille\",\"chocolat\") : [\"parfum\",\"goût\",\"fraise\",\"glace\",\"boule\",\"gâteau\",\"bonbon\",\"blanc\",\"noir\",\"préférence\"],\n",
    "(\"poli\",\"courtois\") : [\"bonjour\",\"merci\",\"pardon\",\"stp\",\"règles\",\"apprendre\",\"savoir-vivre\",\"savoir-être\",\"formule\",\"demander\"],\n",
    "(\"choqué\",\"horrifié\") : [\"peur\",\"terreur\",\"phobie\",\"clown\",\"noir\",\"araignée\",\"rat\",\"cri\",\"oh\",\"ah\"],\n",
    "(\"propre\",\"sale\") : [\"nettoyer\",\"vaisselle\",\"laver\",\"savon\",\"produit\",\"salissures\",\"traces\",\"net\",\"éclatant\",\"brillant\"],\n",
    "(\"canapé\",\"fauteuil\") : [\"maison\",\"salon\",\"assis\",\"confortable\",\"sofa\",\"convertible\",\"cuir\",\"télé\",\"accoudoir\",\"plaid\"],\n",
    "(\"téléphone\",\"portable\") : [\"appel\",\"allo\",\"décrocher\",\"sms\",\"numéro\",\"mobile\",\"orange\",\"sfr\",\"free\",\"bouygues\"],\n",
    "(\"voiture\",\"camion\") : [\"moto\",\"véhicule\",\"conduire\",\"rouler\",\"conducteur\",\"volant\",\"chauffeur\",\"permis\",\"vitesse\",\"passager\"],\n",
    "(\"grand\",\"petit\") : [\"taille\",\"géant\",\"nain\",\"s\",\"m\",\"l\",\"xl\",\"mesure\",\"énorme\"]\n",
    "}\n",
    "\n",
    "bruits = ['ALLATES','NEBULES','LONGTON','PAWESIN','SORENGO','SEXTEES','TAXIONS','ELEWIJT','EPEISME','APOTOME',\n",
    "'REMORVA','HOBOKEN','MALICES','SOMZEEN','NEDDOIS','RECLIVA','GILEREZ','GAULDOS','GOINFRA','RAYERES','BOTTEES','VAGITES','PRELEES','GARDERA','ANATASE','DATASSE',\n",
    "'BILLAGE','POUCAVE','REFUGES','REDIMAI','SOLANGE','EMBOISE','BACHAGA','FAMINER','ECUMONS','HARWELL','VEURDES','AZURERA','ENUQUAI','MAULAIS','MEVENIR','GAUDENT',\n",
    "'MENTANA','REHERSE','ARBECEY','FAUXAMI','BALADOU','REVERDI','BEDAVES','BORAMES','PONTIFE','ALESIEZ','REFIXER','IRRUIEZ','DRESSEZ','PYROIDE','GNAQUAI','LAVETON',\n",
    "'RERESTA','EPRISSE','FROLERA','RAVEURS','CITRINE','NAUNHOF','FEUTRIE','RETASEE','TIMBREE','GANTERA','HALENES','SUSIENS','DEGOUTE','BINAGES','VEAUTES','LAISSAI',\n",
    "'STERANE','RECERNE','RELIQUE','FLAGGER','DELAVAS','SUADERA','PINIERS','DOMPTER','CARRARE','ROSAZIA','LIMITAS','EUCLIDE','FARCANT','BLATERE','STRICTS','REFLOTS',\n",
    "'PAGELLE','BLAUZAC','BROCHER','TOPDOWN','UNIFERE','EHOUPER','ASININS','ASSAKIS','HOTELES','DENIOTE']\n",
    "\n",
    "i = 0\n",
    "for mot in bruits:\n",
    "    mot_min = mot.lower()\n",
    "    bruits[i] = mot_min\n",
    "    i += 1\n",
    "    \n",
    "t_corpus = []\n",
    "for i in range(10000):\n",
    "    indiceCouple = randrange(0,10)\n",
    "    membreCouple = randrange(0,2)\n",
    "    # On a notre mot clé\n",
    "    mot_cle = liste_couples[indiceCouple][membreCouple]\n",
    "    # On récupère les contextes, on va en tirer 5\n",
    "    liste_mot = couples_contexte[liste_couples[indiceCouple]]\n",
    "    ech_contexte = sample(liste_mot,5)\n",
    "    # On génère le bruit \n",
    "    ech_bruit = sample(bruits,3)\n",
    "    # On crée la phrase\n",
    "    phrase = []\n",
    "    phrase.append(mot_cle)\n",
    "    for mot in ech_contexte:\n",
    "        phrase.append(mot)\n",
    "    for bruit in ech_bruit:\n",
    "        phrase.append(bruit)\n",
    "    random.shuffle(phrase)\n",
    "    t_corpus.append(phrase)\n",
    "phrases = t_corpus\n",
    "#phrases = [' '.join(x) for x in t_corpus] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
