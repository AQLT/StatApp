{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'‚Äô\n",
      "[['il', 'm√©rite', 'd', '√™tre', 'bloquer', 'la', 'lettre', 'de', 'l', 'alphabet'], ['nickname', 'et', 'fi√®re', 'je', 't', 'en', 'voi', 'att', 'j', 'avais', 'oubli√©'], ['il', 'est', '1', 'heure'], ['eeeeh', 'jfais', 'la', 'go', 'qui', 'a', 'de', 'les', 'programmes', 'mais', 'j', 'ai', 'm√™me', 'pas', 'de', 'navigo', 'ptdddddr', 'üò≠'], ['en', 'tout', 'cas', 'la', 'demoiselle', 'a', 'bien', 'raison'], ['le', 'rathalos', 'est', 'un', 'gros', 'fils', 'de', 'pute', 'bonne', 'nuit'], ['nickname', 'et', 'puis', 'un', 'jour', 'pfffffffffus', 'rien', 'ne', 'fonctionne', 'l', 'humain', 'se', 'meurt', '√†', 'petit', 'feu'], ['nickname', 'je', 'veux', 'pas', 'en', 'savoir', 'sasuke', 'j', 'sais', 'pas', 'qui', 'c', 'est', 'mais', 'j', 'crois', 'il', 'est', 'sombre'], ['√†', 'partir', 'de', 'quand', 'peut', 'on', 'se', 'qualifier', 'de', 'fan'], ['go', 'profit√©', 'de', 'les', '6h20', 'de', 'sommeil', 'qu', 'il', 'me', 'reste']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "os.chdir('C:/Users/Kim Antunez/Documents/Projets_autres')\n",
    "print(string.punctuation + \"'‚Äô\")\n",
    "def mise_en_forme_phrase (phrase):\n",
    "    phrase = phrase.lower()\n",
    "    # On el√®ve la ponctuation mais √ßa peut se discuter (garder les @ et #?)\n",
    "    phrase = re.sub('( @[^ ]*)|(^@[^ ]*)',\"nickname\", phrase) #Remplace @... par nickname\n",
    "    #supprime toutes les ponctuations par d√©faut + les apostrophes bizarres\n",
    "    phrase = phrase.translate(str.maketrans('', '', string.punctuation + \"'‚Äô\"))\n",
    "    # On enl√®ve les passages √† la ligne\n",
    "    phrase = re.sub('\\\\n', ' ', phrase)\n",
    "    # On enl√®ve les espaces multiples et les espaces √† la fin des phrases\n",
    "    phrase = re.sub(' +', ' ', phrase)\n",
    "    phrase = re.sub(' +$', '', phrase)\n",
    "    return(phrase.split())\n",
    "#f = open('data/sample_3.txt')\n",
    "#raw = f.read()\n",
    "#print(type(raw))\n",
    "with open('data/sample_3.txt', encoding=\"utf-8\") as myfile:\n",
    "    phrases = [mise_en_forme_phrase(next(myfile)) for x in range(10000)]\n",
    "print(phrases[0:10])\n",
    "#raw = ''.join([''.join(phrase) for phrase in phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['m√©rite', 'd', '√™tre', 'bloquer', 'la', 'lettre', 'de', 'l', 'alphabet']\n",
      "<class 'set'>\n",
      "['grossissent', 'ortenburg', 'wish', 'avoir', '35', 'lance', 'lyoko', 'multivitamin√©s', 'compl√©t√©']\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, pprint\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "# words = word_tokenize(raw) # Plus utile maintenant\n",
    "words = [item for sublist in phrases for item in sublist]\n",
    "print(type(words))\n",
    "## On enl√®ve la ponctuation et on met en minuscule :\n",
    "#words = [word.lower() for word in words if word.isalpha()] # plus utile maintenant\n",
    "vocabulary = set(words)\n",
    "\n",
    "print(words[1:10])\n",
    "print(type(vocabulary))\n",
    "print(list(vocabulary)[1:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots : 109479\n",
      "Taille du vocabulaire : 13546\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de mots :\", len(words))\n",
    "print(\"Taille du vocabulaire :\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 10 mots les plus communs sont :\n",
      "[('nickname', 4457), ('de', 3938), ('le', 2548), ('est', 2144), ('je', 2037), ('les', 1813), ('j', 1785), ('√†', 1721), ('c', 1693), ('la', 1620)]\n",
      "Les 2 premiers mots et leur occurrence sont :\n",
      "il (868 occurrences)\n",
      "m√©rite (10 occurrences)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20a41220e88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour changer la taille des graphiques :\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "fdist = nltk.FreqDist(words)\n",
    "print(\"Les 10 mots les plus communs sont :\")\n",
    "print(fdist.most_common(10))\n",
    "print(\"Les 2 premiers mots et leur occurrence sont :\")\n",
    "for cle, valeur in list(fdist.items())[0:2]:\n",
    "    print(\"{} ({} occurrences)\".format(cle, valeur))\n",
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling rate\n",
    "On va simplifier un peu le corpus en enlevant certains mots. Pour cela on va faire un sous-√©chantillonnage du corpus pour supprimer certains mots. \n",
    "\n",
    "Pour chaque mot $w_i$ on note $z(w_i)$ la proportion d'apparition de ce mot, c'est-√†-dire le rapport entre le nombre de fois que ce mot apparait et le nombre total de mots. La probabilit√© de garder un mot le mot $w_i$ est :\n",
    "$$\n",
    "\\mathbb P(w_i) = \\left(\\sqrt{\\frac{z(w_i)}{q}} + 1 \\right)\n",
    "\\times\n",
    "\\frac{q}{z(w_i)}\n",
    "$$\n",
    "Le param√®tre $q$ est appel√© \"sample\" ‚Äì √©chantillonnage ‚Äì contr√¥le le nombre de sous-√©chantillonnages. La valeur par d√©faut est 0,001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calcul_proba(x):\n",
    "    result = (sqrt(x)+1)*(1/x)\n",
    "    return(result)\n",
    "calcul_proba_v = np.vectorize(calcul_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G√©n√©ration de l'√©chantillon de test\n",
    "Comment on g√®re les doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_echantillon(phrases, vocabulary , probabilities_subsampling,  window = 2):\n",
    "    #Sub-sampling\n",
    "    nouveau_corpus = [] \n",
    "    for phrase in phrases: #on parcourt tous les articles du corpus\n",
    "        nouveau_corpus.append([]) #on cr√©e une sous liste √† chaque nouvel article\n",
    "        for word in phrase: #et pour tous les mots de l'article\n",
    "        # Les mots √† supprimer sont les mots tels que la loi g√©n√©r√©e U([0,1]) soit > proba\n",
    "        # On garde donc les mots si U([0,1]) <= proba\n",
    "            proba_w = probabilities_subsampling[vocabulary.index(word)]\n",
    "            if np.random.uniform(low=0.0, high=1.0) <= proba_w:\n",
    "                nouveau_corpus[-1].append(word)\n",
    "    #On ne garde que les phrases de plus d'un mot. \n",
    "    phrases = [phrase for phrase in nouveau_corpus if len(phrase)>1]\n",
    "    test_sample = []\n",
    "    for phrase in phrases:\n",
    "        # On tire au hasard un mot focus et on r√©cup√®re son index\n",
    "        focus = list(range(0, len(phrase)))\n",
    "        focus = random.choice(focus)\n",
    "        i = focus\n",
    "        index_i = vocabulary.index(phrase[i])\n",
    "        # On tire au hasard un mot contexte dans la fen√™tre de ce mot focus et on r√©cup√®re son index\n",
    "        i_contexte = list(range(max(i-window,0), min(i+window+1, len(phrase))))\n",
    "        i_contexte.remove(i)\n",
    "        i_contexte = random.choice(i_contexte)\n",
    "        j = i_contexte\n",
    "        index_j = vocabulary.index(phrase[j])\n",
    "        test_sample.append([index_i, index_j])\n",
    "    return(test_sample)\n",
    "# exemple de test_sample : [6175, 2149]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme avec softmax\n",
    "Si on note $\\theta$ le param√®tre √† estimer, $L(\\theta)$ la fonction de perte et $\\eta$ le taux d'apprentissage (*learning rate*) alors :\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "\n",
    "Pour le n√©gative sampling, la probabilit√© de garder le mot $w_i$ est √©gale √† :\n",
    "$$\n",
    "\\mathbb P(w_i) = \\frac{f(w_i)^{3/4}}{\n",
    "\\sum_{j=1}^n f(w_j)^{3/4}\n",
    "}\n",
    "$$\n",
    "Avec $f(w_j)$ la fr√©quence d'apparition du mot $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "def live_plot(data, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00091342 0.00091342 0.00091342 0.10047589 0.00091342 0.00182683\n",
      " 0.00091342 0.00091342 0.00091342]\n",
      "[1127.87761097 1127.87761097 1127.87761097   13.10741629 1127.87761097\n",
      "  570.79147409 1127.87761097 1127.87761097 1127.87761097]\n",
      "[2.46091758e-05 2.46091758e-05 2.46091758e-05 8.35875475e-04\n",
      " 2.46091758e-05 4.13875355e-05 2.46091758e-05 2.46091758e-05\n",
      " 2.46091758e-05]\n"
     ]
    }
   ],
   "source": [
    "# On cr√©e le vocabulaire, on calcule les proba de subsampling et negative sampling\n",
    "sample = 0.01\n",
    "words = [item for sublist in phrases for item in sublist] #deja fait\n",
    "fdist = nltk.FreqDist(words) #deja fait\n",
    "vocabulary = list(set(words)) #deja fait\n",
    "proportion = np.array([(fdist[w]/ (len(words) * sample)) for w in vocabulary])\n",
    "p_subsampling = calcul_proba_v(proportion)\n",
    "p_negativesampling = np.array([(fdist[w]**(3/4)) for w in vocabulary])\n",
    "p_negativesampling /= p_negativesampling.sum()\n",
    "\n",
    "print(proportion[1:10])\n",
    "print(p_subsampling[1:10])\n",
    "print(p_negativesampling[1:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme avec subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tirage_neg_sampling(vocabulary, proba_negativesampling, focus, context, K = 5):\n",
    "    #proba_negativesampling[focus] = 0\n",
    "    #proba_negativesampling[context] = 0\n",
    "    liste_vocab = list(range(len(vocabulary)))\n",
    "    neg_sampling = np.random.choice(liste_vocab, size=K, p=proba_negativesampling)\n",
    "    #while( (focus in neg_sampling) | (context in neg_sampling)):\n",
    "    #    neg_sampling = np.random.choice(liste_vocab, size=K, p=proba_negativesampling)\n",
    "    return(neg_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAE9CAYAAAC1Lk0zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhU5cH+8e8zmUkmO4SQBcK+Q9gMJCouYFVAUfTVWhcQELTWWq2tP1tfq7Vqtctb2/q2Vi0ICFZsq1WrAoqCVqsJQRHCvioIYScLJCEJz++PGXnTGCCBmZw5yf25rlxOzjkzuXM8cHNmznkeY61FREQkknmcDiAiInIyKisREYl4KisREYl4KisREYl4KisREYl4KisREYl4Xqd+cGpqqu3atetpv86hQ4eIj48//UDNxG15wX2Z3ZYX3JdZecPPbZlDlXfZsmV7rbXtv7bCWuvIV05Ojg2FxYsXh+R1movb8lrrvsxuy2ut+zIrb/i5LXOo8gKFtoHO0NuAIiIS8VRWIiIS8VRWIiIS8VRWIiIS8VRWIiIS8VRWIiIS8VRWIiIS8VRWIiIS8VRWIiIS8VxdVu+t38OGA7VOxxARkTBzbVnVHrX8/I3VPL6sktU7Sp2OIyIiYeTasoryGJ6dPJxYr+HGZ/PZsveQ05FERCRMXFtWAFlt47h7mJ+jFiZMz2fHwQqnI4mISBi4uqwAOiR4eO6mXEorqpkwI5995VVORxIRkRBzfVkBZHdMZsbk4Xx5oIJJMwsorax2OpKIiIRQiygrgNxuKTw1IYe1O8uYNquQiiO6SlBEpKVoMWUFMKpvGr/91hCWfr6f255fxpGao05HEhGREGhRZQVw2eAO/PyKgSxet4cf/HU5tUet05FEROQ0eZ0OEA7X53WmtLKaX8xfS6Lfx6NXZmOMcTqWiIicohZZVgC3nt+Dkopq/rRkE8mxPn48tq/TkURE5BS12LICuGd0H0orqnnqvU0kxXq5bWRPpyOJiMgpaNFlZYzh4fHZlFXW8KsF60jy+5hwZhenY4mISBO16LIC8HgMv7lmMOVVNdz/ahGJfi/jh3R0OpaIiDRBi7sasCG+KA9P3nAGuV1T+OFfP+PdtbucjiQiIk3QKsoKwO+LYvqkYfTLTOI7cz/h4837nI4kIiKN1GrKCiDR72P2Tbl0Solj2uxCVm4vcTqSiIg0QqsqK4CU+GjmTM0lOdbHjc/ms3F3mdORRETkJFpdWQFkJsfy/LQ8ojweJkwvYNv+w05HEhGRE2iVZQXQNTWeOVNzOXykhokz8tldVul0JBEROY5WW1YA/TKTmDkll12lVdw4o4CSw5paREQkErXqsgLI6dKWZ27MYfOeQ0yZVcDhIzVORxIRkXpafVkBnNurPU9cN4Tl2w7y7TnLqKrRXFgiIpFEZRU0JjuTX141iH9t2MudLyynplZzYYmIRAqVVR3fHNaJB8b1Z8GqYu59eSVHNReWiEhEaPFjAzbVTed0o6Simt+/s4FEv4/7x/XTXFgiIg5TWTXg+xf2oqSimmc/3EJyrI87L+zldCQRkVZNZdUAYwwPjOtPWWUNv120nqRYL1NGdHM6lohIq6WyOg6Px/DLqwZSXlXNz/65mkS/j6tzspyOJSLSKukCixPwRnn4/bVDGdGzHT96aQULVxU7HUlEpFVSWZ2E3xfFMxOHMbBjMt/7y6d8uHGv05FERFodlVUjxMd4mTVlON1S47n5uUI++eKA05FERFoVlVUjtYkLTC3SPjGGKTOXsra41OlIIiKthsqqCdKS/Mydmoff52HijAI+33fI6UgiIq1Co8rKGLPVGLPSGLPcGFPYwPobjDErgl//NsYMDn3UyNApJY65U/OoqT3KDdPzKS7R1CIiIuHWlDOrUdbaIdbaYQ2s2wKcb60dBDwMPBOSdBGqV3ois2/K5eDhaibOyOfAoSNORxIRadFC8jagtfbf1tqvrjr4GGjxNyQNymrDn28cxuf7DzNpZgFllZoLS0QkXBpbVhZ4yxizzBhzy0m2nQrMP71Y7nBWj3Y8ef0ZrNpRys3PFVJZralFRETCwVh78pHFjTEdrLU7jDFpwNvA96y17zew3SjgSeAca+2+BtbfAtwCkJ6enjNv3rzTzU95eTkJCQmn/Tqn46MdNTyzoorB7aO4fWgMXs/xB76NhLxN5bbMbssL7susvOHntsyhyjtq1KhlDX7cZK1t0hfwIHB3A8sHAZuA3o15nZycHBsKixcvDsnrnK7nPtpqu/zodXvHC5/Y2tqjx90uUvI2hdsyuy2vte7LrLzh57bMocoLFNoGOuOkYwMaY+IBj7W2LPj4YuChett0Bl4GJlpr159WrbrUxDO7UFpRza8XriPJ7+Oh8QM0tYiISIg0ZiDbdOAfwb94vcBfrLULjDG3AlhrnwIeANoBTwa3q7ENXzXYot02sgelFdU8/f5mkmN93D26j9ORRERahJOWlbV2M/C1+6aCJfXV42nAtNBGcx9jDD8e25fSymr+sHgjSbFebjmvh9OxRERcT1OEhJgxhkeuGEhpZQ2PvrmWJL+Pa3M7Ox1LRMTVVFZhEOUx/PaaIZRX1nDvP1aS4PcyblAHp2OJiLiWxgYMk2ivh6cm5DCsS1vuenE5S9btdjqSiIhrqazCKDY6ihmTh9M7PZFb5y5j6db9TkcSEXEllVWYJfl9zL4plw7Jsdw0cymfl2qUCxGRplJZNYPUhBjmTMsj0e/lfwor2bSn3OlIIiKuorJqJh3bxDJ3Wh4AE6fn8+XBCocTiYi4h8qqGXVvn8Ddw/yUVdUwcXo+e8urnI4kIuIKKqtm1iUpipmTh7OjpIIbZxRQUqGpRURETkZl5YBhXVN4akIOG3aXMXXWUiqO6KILEZETUVk5ZGSfNH73raF88sUBbp27jCM1R52OJCISsVRWDrp0UCaPXjmQ99bv4a4Xl1N79ORzi4mItEYabslh1+Z2pqyyhp+/uYZEv5fH/mugphYREalHZRUBbj6vOyUVX43U7uPesX1VWCIidaisIsQPL+5NaWU1zwTnwvruqJ5ORxIRiRgqqwhhjOHBywZQVlkTnG3Yy8SzujodS0QkIqisIojHY/jV1YMoq6zm/ldXkej3ccXQjk7HEhFxnK4GjDC+KA9/uP4Mzurejh/+7TMWrd7ldCQREceprCKQ3xfFnycNI7tDErf95RM+2rTP6UgiIo5SWUWohBgvs6bk0iUljmmzl/LZtoNORxIRcYzKKoK1jY9mztQ8UhKimTSzgPW7ypyOJCLiCJVVhMtI9jN3ah6+KA8TZ+Szbf9hpyOJiDQ7lZULdGkXz9ypeVRWH+WG6fnsLq10OpKISLNSWblEn4xEZk0Zzt7yKibOKODg4SNORxIRaTYqKxcZ2rktf75xGFv2HmLyzKUcqqpxOpKISLNQWbnMiJ6p/O/1Q1n5ZQm3zCmkslpzYYlIy6eycqHRAzL41VWD+HDjPu544VNqajUXloi0bCorl7oqJ4sHL+vPW6t3cc9LKziqubBEpAXT2IAuNnlEN0ora3j87fUk+X389LL+mlpERFoklZXLfe+CnpRUVDPjgy0kx/q466LeTkcSEQk5lZXLGWP4yaX9KK2o5vfvbCAp1sfUc7o5HUtEJKRUVi2AMYbH/msg5VU1PPz6ahL9Xq4Z1snpWCIiIaMLLFoIb5SH3107hHN7pfLjl1Ywf+VOpyOJiISMyqoFifFG8fTEHIZ2bsud85bzrw17nI4kIhISKqsWJi7ay7OThtO9fTy3PLeMZZ8fcDqSiMhpU1m1QMlxPuZMzSM9KYYpMwtYs7PU6UgiIqdFZdVCtU+MYe60POJjvEycUcCWvYecjiQicspUVi1YVts45kzN46i1TJiez86SCqcjiYicEpVVC9czLYHZU3IpqahmwvR89pVXOR1JRKTJVFatwMCsZGZMGsb2AxVMnrmUsspqpyOJiDSJyqqVyOvejqcm5LBmZylTZ2tqERFxF5VVKzKqbxqPf2sIS7fu57bnP6FaU4uIiEuorFqZywd34JErsnl37W5+8NfPqNXUIiLiAhobsBW6Ia8LpRU1/HLBWhL9Xn5+RbamFhGRiNaosjLGbAXKgFqgxlo7rN56A/weuAQ4DEy21n4S2qgSSt8Z2YPSymr+tGQTybE+fjSmr9ORRESOqylnVqOstXuPs24s0Cv4lQf8KfhfiWD3jO5DaUWgsJL8Pr4zsofTkUREGhSqtwHHA89Zay3wsTGmjTEm01qrob8jmDGGh8ZnU1oZeEswKdbLDXldnI4lIvI1jS0rC7xljLHA09baZ+qt7whsq/P99uAylVWEi/IYHr9mMIeqavjJK0Uk+n1cPriD07FERP6DCZwMnWQjYzpYa3cYY9KAt4HvWWvfr7P+DeAxa+0Hwe/fAe6x1i6r9zq3ALcApKen58ybN++0f4Hy8nISEhJO+3WaS6TmPVJr+U1hJRsPHuWOM2IY3P7//h0TqZmPx215wX2ZlTf83JY5VHlHjRq1rP51EQBYa5v0BTwI3F1v2dPAdXW+Xwdknuh1cnJybCgsXrw4JK/TXCI5b2nFEXvpE+/b3ve9aT/etPfY8kjO3BC35bXWfZmVN/zcljlUeYFC20BnnPQ+K2NMvDEm8avHwMVAUb3NXgNuNAFnAiVWn1e5TqLfx+wpuWS1jWXa7EKKvixxOpKICNC4m4LTgQ+MMZ8BBcAb1toFxphbjTG3Brd5E9gMbAT+DNwWlrQSdu0SAlOLJMX6uPHZAjbuLnc6kojIyS+wsNZuBgY3sPypOo8t8N3QRhOnZCbHMndaHt986iMmzsjnh0N0w7CIOEvDLUmDuqXGM2dqLoeqavj10kr2lGlqERFxjspKjqtfZhIzp+RyoMoycUY+JYc1tYiIOENlJSeU06Utdwz1s3nPIabMKuDwkRqnI4lIK6SykpPKTo3iieuGsHzbQb49ZxlVNZoLS0Sal8pKGmVMdia/uGoQ/9qwl+/PW06N5sISkWakspJGu2ZYJ+4f15/5RcX89z9WfnUDuIhI2Gk+K2mSqed0o6Simife2UCi38dPLu2nubBEJOxUVtJkd13Yi9KKamZ8sIXkWB93fKOX05FEpIVTWUmTGWN4YFx/Siurefzt9ST5vUwe0c3pWCLSgqms5JR4PIZfXTWI8soaHvznahL9Pq7KyXI6loi0ULrAQk6ZN8rDE9cN5ewe7bjnpRW8tarY6Ugi0kKprOS0+H1RPHPjMAZ2TOb2v3zKhxv3Oh1JRFoglZWctoQYL7OmDKdbajw3P1fIp18ccDqSiLQwKisJiTZx0cyZmktqQgyTZy5lXXGZ05FEpAVRWUnIpCX5eX5aHn6fh4kz8vl83yGnI4lIC6GykpDqlBLHnKl5HKk9yoQZ+ewqrXQ6koi0ACorCbne6YnMnpLL/vIjTJiez4FDR5yOJCIup7KSsBjcqQ3TJw3n8/2HmTyzgPIqTS0iIqdOZSVhc1aPdjx5/RkU7Sjl5tmFVFZrahEROTUqKwmrC/un85tvDubjLfu4/S+fUq2pRUTkFKisJOyuGNqRhy4fwKI1u7jn7ys4elRTi4hI02hsQGkWE8/qSklFNf/z1noS/V5+dvkATS0iIo2mspJm891RPSmtrOGZ9zeTHOvjhxf3cTqSiLiEykqajTGGe8f2pbSimv99dyNJfh83n9fd6Vgi4gIqK2lWxhh+fuVAyipr+Pmba0iK9fKt4Z2djiUiEU5lJc0uymP47beGUF5Vw70vryQhxselgzKdjiUiEUxXA4ojor0enpqQwxmd2/L9Fz9lybrdTkcSkQimshLHxEZHMWPycHqlJXLr3GUUbt3vdCQRiVAqK3FUcqyP56bm0iE5limzlrJqR4nTkUQkAqmsxHGpCTHMmZZHYoyXG2cUsHlPudORRCTCqKwkInRsE8ucaXkATJiez46DFQ4nEpFIorKSiNGjfQKzb8qlrLKGCTPy2Vte5XQkEYkQKiuJKNkdk3l2ynB2HKxg0rMFlFZWOx1JRCKAykoizvCuKTw1IYf1u8qYOmspFUc0tYhIa6eykog0sk8av/3WEAo/P8B3nl/GkRpNLSLSmqmsJGKNG9SBx64cyJJ1e7jrr8up1dQiIq2WhluSiHZtbmdKK6t59M21JPm9PHrlQE0tItIKqawk4t1yXg9KKqr54+JNJPl9/HhsXxWWSCujshJXuPviPpRW1PD0+5tJivXx3VE9nY4kIs1IZSWuYIzhZ5cPoKyyml8vXEdSrI+JZ3ZxOpaINBOVlbiGx2P49TcHU15VwwOvFpHk9zJ+SEenY4lIM9DVgOIqvigPf7j+DPK6pfCDv37GO2t2OR1JRJqBykpcx++LYvqk4QzokMRtz3/CR5v2OR1JRMJMZSWulBDjZdaUXDqnxDFt9lJWbD/odCQRCSOVlbhWSnw0c6bm0TY+mknPFrBhV5nTkUQkTBpdVsaYKGPMp8aY1xtY19kYszi4foUx5pLQxhRpWEayn+en5eGN8jBhRj7b9h92OpKIhEFTzqzuBNYcZ91PgL9aa4cC1wJPnm4wkcbq0i6eOVNzqaw+yoQZ+Rys1DiCIi1No8rKGJMFXApMP84mFkgKPk4Gdpx+NJHG65uRxKwpw9lTVsX/FFZy8PARpyOJSAgZa08+OKgx5u/AY0AicLe1dly99ZnAW0BbIB640Fq7rIHXuQW4BSA9PT1n3rx5p/0LlJeXk5CQcNqv01zclhfclXn1vloeL6ygc1IU9wz34/e6Y1gmN+1jUN7m4LbMoco7atSoZdbaYV9bYa094RcwDngy+Hgk8HoD2/wA+GHw8VnAasBzotfNycmxobB48eKQvE5zcVtea92X+X/mvW273/uGvf7PH9nK6hqn4zSK2/ax8oaf2zKHKi9QaBvojMa8DTgCuNwYsxWYB1xgjJlbb5upwF+D5fcR4AdSm1SnIiGSk+7lV1cN4sON+7jjhU+pqdVnWCJud9Kystbea63NstZ2JXDxxLvW2gn1NvsC+AaAMaYfgbLaE+KsIo12VU4WP72sPwtX7eJHL63kqObCEnG1Ux4b0BjzEIHTtdeAHwJ/NsbcReBii8nB0zkRx0wZ0Y3Sihp+u2g9SbFeHhjXX1OLiLhUk8rKWrsEWBJ8/ECd5asJvF0oElHu+EZPSiqqefbDLSTH+vj+hb2djiQip0CjrkuLZozhJ5f2o7Symt8t2kCS38dN53RzOpaINJHKSlo8j8fwi/8aSHllDQ+9vppEv5dvDuvkdCwRaQKNDSitgjfKw++vG8K5vVL50UsrWFC00+lIItIEKitpNWK8UTw9MYchndpwxwvL+WDDXqcjiUgjqaykVYmL9jJzci7d28dzy5xCln1+wOlIItIIKitpdZLjfDw3NZe0xBimzCxgzc5SpyOJyEmorKRVSkv0M2dqHnHRXibOKGDr3kNORxKRE1BZSavVKSWOudNyOWotN0zPZ2dJhdORROQ4VFbSqvVMS2T2lFxKKqqZOKOA/Yc0tYhIJFJZSas3MCuZ6ZOGsW3/YSY9W0BZZbXTkUSkHpWVCHBm93b8acIZrNlZyrTZhVRW1zodSUTqUFmJBF3QN53fXDOYgq37ue35T6jW1CIiEUNlJVLH+CEdeXh8Nu+u3c3df/tMU4uIRAiNDShSz4Qzu1BaWc2vFqwj0e/l4fHZmlpExGEqK5EG3DayJ6UVNTz13iaS/D7uGdPX6UgirZrKSuQ4fjSmD6WV1Ty5ZBNJsT5uPb+H05FEWi2VlchxGGN4eHw2pRXV/GL+WpL8Pq7P6+x0LJFWSWUlcgJRHsPj1wzhUFUN972ykkS/l8sGd3A6lkiro6sBRU4i2uvhyRtyGN4lhbteXM7itbudjiTS6qisRBohNjqK6ZOH0TczkVvnLqNgy36nI4m0KiorkUZK8vuYPSWXrLaxTJ21lKIvS5yOJNJqqKxEmqBdQgxzpuaRFOvjxmcL2Li73OlIIq2CykqkiTq0iWXutDw8BibOyGf7gcNORxJp8VRWIqegW2o8z92UR3lVDRNnFLCnrMrpSCItmspK5BT175DErCnDKS6p5MZnCyip0NQiIuGishI5DTldUnh6Yg4bd5dx06ylHD5S43QkkRZJZSVyms7r3Z4nrh3Kp18c4NtzllFVo7mwREJNZSUSAmMHZvKL/xrEvzbs5a4Xl1OrqUVEQkrDLYmEyDXDO1FaWc0jb6whMWYlv7hqoKYWEQkRlZVICE07tzulFdU88e5GEv1e7ru0nwpLJARUViIhdtdFvSmtrGH6B1tIjvXxvW/0cjqSiOuprERCzBjDA+P6U1pRzW/eXk9SrI9JZ3d1OpaIq6msRMLA4zH86upBlFXV8NPXVpEU6+XKoVlOxxJxLV0NKBIm3igP/3vdUM7u0Y67/7aCt1YVOx1JxLVUViJh5PdF8cyNw8jumMztL3zKvzftdTqSiCuprETCLCHGy+wpw+naLo6bZxeyfNtBpyOJuI7KSqQZtImLZs7UPNolxDB5ZgHrisucjiTiKiorkWaSnuTn+Wl5REd5mDgjny/2aWoRkcZSWYk0o04pccydlseR2qPcMONjdpVWOh1JxBVUViLNrHd6IrOm5LK//AgTZ+Rz4NARpyOJRDyVlYgDhnRqw58nDWPrvsNMnlnAhgO1HNXgtyLHpbISccjZPVL54/VnsG5XGT/Pr+TMx97h/leK+HDjXmpqjzodTySiaAQLEQdd1D+dgvsu5MmX3+Pzo235+7LtzPn4c9rE+bioXzpjB2YwomcqMd4op6OKOEplJeKwJL+PMzt4+fHIHCqO1PLe+j0sXFXMglXF/G3ZdhJivFzQN40x2RmM7NOeuGj9sZXWp9FHvTEmCigEvrTWjmtg/TXAg4AFPrPWXh+qkCKtRWx0FGOyMxiTncGRmqP8e9NeFhQV89bqXbz22Q5ivB7O792esQMzuKBvOsmxPqcjizSLpvwT7U5gDZBUf4UxphdwLzDCWnvAGJMWonwirVa018PIPmmM7JPGI1ccZenWA4EzrmB5eT2Gs3umMjY7g4v6p5OaEON0ZJGwaVRZGWOygEuBnwM/aGCTm4E/WmsPAFhrd4csoYjgjfJwVo92nNWjHQ+M689n2w+yoKiY+UXF3PvySu77x0qGd005dlaWmRzrdGSRkDLWnvxyWWPM34HHgETg7vpvAxpjXgHWAyOAKOBBa+2CBl7nFuAWgPT09Jx58+ad9i9QXl5OQkLCab9Oc3FbXnBfZrflhVPPbK1lW9lRlu2qpXBXDV+WB/48d0/2MCw9ipx0L+nxob/o12372G15wX2ZQ5V31KhRy6y1w+ovP2lZGWPGAZdYa28zxoyk4bJ6HagGrgGygH8B2dba447YOWzYMFtYWNjkX6S+JUuWMHLkyNN+nebitrzgvsxuywuhy7xpTzkLiopZuKqYFdtLAOibkciY7AzGZmfSOz0BY8xp/xy37WO35QX3ZQ5VXmNMg2XVmLcBRwCXG2MuAfxAkjFmrrV2Qp1ttgMfW2urgS3GmHVAL2DpaScXkUbr0T6B747qyXdH9WT7gcMsXLWLBUU7+f07G/jdog10S40PvFU4IINBWckhKS6R5nDSsrLW3kvg4gnqnFlNqLfZK8B1wCxjTCrQG9gc2qgi0hRZbeOYek43pp7Tjd1llby1ahcLVxXzzPub+dOSTXRI9jM6eMaV06UtUR4Vl0SuU75hwxjzEFBorX0NWAhcbIxZDdQC/89auy9EGUXkNKUl+plwZhcmnNmFg4ePsGjNbhYUFfN8/hfM/HArqQkxXDwgnTEDMjirRzt8URrcRiJLk8rKWrsEWBJ8/ECd5ZbAVYINXSkoIhGkTVw0V+dkcXVOFuVVNSxeu5sFq4p55dMv+Uv+FyT5vVzYP52x2Zmc2ysVv0+jZ4jzdCu8SCuWEOPlssEduGxwByqra/nXhsBNyIvW7OLlT74kLjqKUX3TGDMgg1F900iI0V8Z4gwdeSICgN8XxUX907mofzrVtUf5ePM+5hcV89aqYt5YsZNor4fzeqUyekAGsUc0Qrw0L5WViHyNL8rDub3ac26v9jw8Pptlnx84dkn8ojW78RiY90U+Y7IzuHhAOmmJfqcjSwunshKRE4ryGHK7pZDbLYX7x/Vj5ZclPP1GAasPVvCTV4q4/9UihnVpy+gBgdEzstrGOR1ZWiCVlYg0mjGGQVlt+GafaM4//3zW7yoPDvu0k0feWMMjb6xhYMfkY8M+9WjvnhEYJLKprETklBhj6JORSJ+MRO68sBdb9x5iQXCg3V8vXMevF66jd3oCYwZkMCY7k36ZiboJWU6ZykpEQqJrajy3nt+DW8/vwY6DFby1KjDQ7h8Wb+SJdzfSpV0cYwZkMDo7gyFZbfDoJmRpApWViIRchzaxTB7RjckjurG3vIq3V+9iQVExz364haff30xGkp/RA9IZk53J8K5t8eomZDkJlZWIhFVqQgzX5XbmutzOlFRU8+7aXcxfWcy8pduY/dHnpMRHc3H/dEZnZzCiRyrRXhWXfJ3KSkSaTXKsjyuHZnHl0CwOH6nhvXV7mF9UzOsrdjJv6TYSY7x8o18aY7IzOb93e2KjNXqGBKisRMQRcdFexg7MZOzATKpqavlwY2D0jLdX7+KV5TuI9UUxsk97xmRncEHfNBL9Pqcji4NUViLiuBhvFBf0TeeCvunU1B6lYMt+5gdvQp5fVEx0lIcRPdsxJjuDi/pnkBIf7XRkaWYqKxGJKN4oD2f3TOXsnqn87PIBfLrtQPBermIWv7SSe19eSV63dowdmMHF/TPISNboGa2BykpEIpbHY8jpkkJOlxT++5J+rNpRyoKiYhasKuaBV1fxwKurOKNzm+CEkpl0bqfRM1oqlZWIuIIxhuyOyWR3TObu0X3YuLvs2BnXo2+u5dE319I/M4mxwdEzeqUnOh1ZQkhlJSKu1DMtkdsvSOT2C3qxbf/hY2dcv3l7Pb95ez092sczJjgTcmDKPXEzlZWIuF6nlDhuPq87N5/XnV2llcdGz3jqvc38cfEmUmMNVxxazZjsDM7o3FajZ7iQykpEWpT0JD8Tz+rKxLO6sv/QERat3sXz76/iuY8+Z69SlWMAAA7ASURBVPoHW2ifGMPoAYGZkHO7peDT6BmuoLISkRYrJT6aa4Z3Iu3QJnLOHMG7a3ezoKiYl5Z9ydyPv6BNnI+L+qUzJjuDc3qlEuPVTciRSmUlIq1Cot/H+CEdGT+kIxVHanlv/R4Wrgp8zvW3ZdtJiPEyqm8aY7MzOL93e+Jj9NdjJNH/DRFpdWKjo47NuXWk5ij/3hQYPeOt1bv452c7iPF6OK93e8ZmZ/CNfukkx2r0DKeprESkVYv2ehjZJ42RfdJ45IqjFH4euAn5q6GfvB7D2T1TGTMgg4sHpJOaEON05FZJZSUiEuSN8nBm93ac2b0dD4zrz2fbDx6bUPK//7GSn7yykmFdUxibncHoARl0aBPrdORWQ2UlItIAj8cwtHNbhnZuy4/H9GVtcVlgvMKiYn72z9X87J+rGdypDWMGZDA2O4OuqfFOR27RVFYiIidhjKFfZhL9MpP4wUW92byn/NgZ1y8XrOWXC9bSNyPx2OdgfdITMUb3coWSykpEpIm6t0/gtpE9uW1kT7YfOMzCVbtYWFTM79/ZwO8WbaBbajyjg2dcg7KSVVwhoLISETkNWW3jmHpON6ae043dZZW8vXoXC4qKmf6vzTz13iY6JPsZnZ3BmAEZDOuaQpRGzzglKisRkRBJS/RzQ14XbsjrwsHDR1i0JnAT8vP5XzDzw62kJkRzUf/AGddZPdpp9IwmUFmJiIRBm7hors7J4uqcLMqraliybjfzi4p5dfmXvFDwBUl+Lxf2T2fMgAzO690ev0+jZ5yIykpEJMwSYryMG9SBcYM6UFldy782BG5CXrRmFy9/8iVx0VGM6pPGmOwMRvVNI0GjZ3yN9oiISDPy+6K4qH86F/VPp7r2KB9v3sf8omLeWlXMGyt3Eu31cF6vVLpEVTP40BHaxkc7HTkiqKxERBzii/Jwbq/2nNurPQ+Pz2ZZcPSMhauKWXTwCLNWL+Ks7u0YnZ3B6AHppCX6nY7sGJWViEgEiPIYcrulkNsthfvH9WPWa++yO6YjC4qKuf+VIh54tYiczm0ZExw9o1NKnNORm5XKSkQkwhhj6JYcxZSRfblndB/W7yo/NhPyI2+s4ZE31jCwY/Kxm5B7tE9wOnLYqaxERCKYMYY+GYn0yUjkzgt7sXXvIRYGZ0L+9cJ1/HrhOnqlJQTGK8zOoH9mUou8CVllJSLiIl1T4/n2+T349vk92FlSwcLgGdcfFm/kiXc30jkl7tgZ15CsNnhayE3IKisREZfKTI5l8ohuTB7Rjb3lVSxavYv5RcXM/HALz7y/mYwkP6MHpDM6O4Pcril4XXwTsspKRKQFSE2I4drczlyb25mSimreXRsY9unFwm3M/uhzUuKjuahfOmMGZnB2j3bEeN11E7LKSkSkhUmO9XHl0CyuHJrF4SM1vLduD/OLAvdxvVi4jcQYLxf0S2Nsdgbn904jNjryi0tlJSLSgsVFexk7MJOxAzOpqqnlw417j82C/OryHfh9Hkb2DoyecUG/NJL8PqcjN0hlJSLSSsR4o7igbzoX9E2npvYoBVv2H5uXa8GqYnxRhhE9UxmbncGF/dJplxDjdORjVFYiIq2QN8rD2T1TObtnKg9eNoBPtx1kQdFO5hcV86OXVuIxK8nr1u7YTcgZyc6OnqGyEhFp5TweQ06XtuR0act/X9KPVTtKj93L9dPXVvHT11YxtHMbxmZnMGZAJp3bNf/oGSorERE5xhhDdsdksjsm88OL+7Bxd9mxtwkffXMtj765lv6ZSYzJDszL1TMtoVluQm50WRljooBC4Etr7bjjbHM18DdguLW2MDQRRUTEKT3TErn9gkRuv6AX2/YfPnbG9fjb63n87fV0bx/P2OwM2lfWhjVHU86s7gTWAEkNrTTGJAJ3APkhyCUiIhGmU0oc087tzrRzu7OrtJK3VgXOuJ56bzMd4g2Tx4fvZzfqdmZjTBZwKTD9BJs9DPwKqAxBLhERiWDpSX4mntWV56edydL7LmTawPDOu9XYsTd+B9wDHG1opTFmKNDJWvt6qIKJiIg7pMRH0yUpvDcWG2vtiTcwZhxwibX2NmPMSODuup9ZGWM8wLvAZGvtVmPMkuA2X/vMyhhzC3ALQHp6es68efNO+xcoLy8nIcE9w+O7LS+4L7Pb8oL7Mitv+Lktc6jyjho1apm1dtjXVlhrT/gFPAZsB7YCxcBhYG6d9cnA3uD6rQTeBtwBDDvR6+bk5NhQWLx4cUhep7m4La+17svstrzWui+z8oaf2zKHKi9QaBvojJO+DWitvddam2Wt7QpcC7xrrZ1QZ32JtTbVWts1uM3HwOVWVwOKiEiInPJ48caYh4wxl4cyjIiISEOadFOwtXYJsCT4+IHjbDPydEOJiIjU5d6ZuEREpNVQWYmISMRTWYmISMRTWYmISMRTWYmISMQ76QgWYfvBxuwBPg/BS6USuCnZLdyWF9yX2W15wX2ZlTf83JY5VHm7WGvb11/oWFmFijGm0DY0NEeEcltecF9mt+UF92VW3vBzW+Zw59XbgCIiEvFUViIiEvFaQlk943SAJnJbXnBfZrflBfdlVt7wc1vmsOZ1/WdWIiLS8rWEMysREWnhIrqsjDFjjDHrjDEbjTE/bmB9jDHmxeD6fGNM1zrr7g0uX2eMGR0heX9gjFltjFlhjHnHGNOlzrpaY8zy4NdrEZJ3sjFmT51c0+qsm2SM2RD8mtQceRuZ+bd18q43xhyss86JffysMWa3MaboOOuNMeaJ4O+zwhhzRp11zb6PG5H3hmDOFcaYfxtjBtdZt9UYszK4f5tliqBG5B1pjCmp8//9gTrrTngsOZj5/9XJWxQ8blOC65zYx52MMYuNMWuMMauMMXc2sE34j+OGJrmKhC8gCtgEdAeigc+A/vW2uQ14Kvj4WuDF4OP+we1jgG7B14mKgLyjgLjg4+98lTf4fXkE7t/JwB8aeG4KsDn437bBx20jIXO97b8HPOvUPg7+zPOAM4Ci46y/BJgPGOBMIN/hfXyyvGd/lQMY+1Xe4PdbgdQI278jgddP91hqzsz1tr2MwByCTu7jTOCM4ONEYH0Df1eE/TiO5DOrXGCjtXaztfYIMA8YX2+b8cDs4OO/A98wxpjg8nnW2ipr7RZgY/D1HM1rrV1srT0c/PZjICvMmU6kMfv3eEYDb1tr91trDwBvA2PClLOupma+DnihGXIdl7X2fWD/CTYZDzxnAz4G2hhjMnFoH58sr7X238E84Pwx3Jj9ezync/yfliZmjoRjeKe19pPg4zJgDdCx3mZhP44juaw6AtvqfL+dr++gY9tYa2uAEqBdI58bak39mVMJ/EvkK35jTKEx5mNjzBXhCFhPY/NeFTyt/7sxplMTnxtqjf65wbdYuwHv1lnc3Pu4MY73Ozm1j5ui/jFsgbeMMcuMMbc4lKkhZxljPjPGzDfGDAgui/j9a4yJI/AX+0t1Fju6j03go5ahQH69VWE/jps0+WIzMw0sq3/p4vG2acxzQ63RP9MYMwEYBpxfZ3Fna+0OY0x34F1jzEpr7aYw5DwWo4Fl9fP+E3jBWltljLmVwFnsBY18bjg05edeC/zdWltbZ1lz7+PGiKRjuNGMMaMIlNU5dRaPCO7fNOBtY8za4FmEkz4hMHxPuTHmEuAVoBcRvn+DLgM+tNbWPQtzbB8bYxIIFOf3rbWl9Vc38JSQHseRfGa1HehU5/ssYMfxtjHGeIFkAqfXjXluqDXqZxpjLgTuAy631lZ9tdxauyP4380EZmMeGs6wNCKvtXZfnYx/BnIa+9wwacrPvZZ6b584sI8b43i/k1P7+KSMMYOA6cB4a+2+r5bX2b+7gX8Q/rfeT8paW2qtLQ8+fhPwGWNSieD9W8eJjuFm3cfGGB+BonreWvtyA5uE/zhuzg/qmvihnpfAh3Hd+L8PQAfU2+a7/OcFFn8NPh7Af15gsZnwX2DRmLxDCXyo26ve8rZATPBxKrCBMH/Y28i8mXUeXwl8bP/vQ9Mtwdxtg49TIuGYCG7Xh8AH0cbJfVznZ3fl+BcAXMp/fjBd4OQ+bkTezgQ+Az673vJ4ILHO438DYyIgb8ZXxwGBv9i/CO7rRh1LTmQOrv/qH97xTu/j4P56DvjdCbYJ+3HcLP9jTmMnXULgypNNwH3BZQ8ROCsB8AN/C/7hKQC613nufcHnrQPGRkjeRcAuYHnw67Xg8rOBlcE/MCuBqRGS9zFgVTDXYqBvnefeFNzvG4EpkXJMBL9/EPhFvec5tY9fAHYC1QT+lTkVuBW4NbjeAH8M/j4rgWFO7uNG5J0OHKhzDBcGl3cP7tvPgsfMfRGS9/Y6x/DH1CnZho6lSMgc3GYygYvE6j7PqX18DoG37lbU+f9+SXMfxxrBQkREIl4kf2YlIiICqKxERMQFVFYiIhLxVFYiIhLxVFYiIhLxVFYiLhAcPfx1p3OIOEVlJSIiEU9lJRJCxpgJxpiC4HxDTxtjoowx5caY3xhjPjGBeczaB7cdEhxUd4Ux5h/GmLbB5T2NMYuCg69+YozpEXz5hOCAwmuNMc8HZxgQaRVUViIhYozpB3yLwGCjQ4Ba4AYCQ+N8Yq09A3gP+GnwKc8BP7LWDiJw1/9Xy58H/mitHUxg5I2dweVDge8TmK+tOzAi7L+USISI5FHXRdzmGwQG+10aPOmJBXYDR4EXg9vMBV42xiQDbay17wWXzwb+ZoxJBDpaa/8BYK2tBAi+XoG1dnvw++UExpf7IPy/lojzVFYioWOA2dbae/9joTH319vuRGOcneitvao6j2vRn19pRfQ2oEjovANcHZxrCGNMSnASSA9wdXCb64EPrLUlwAFjzLnB5ROB92xgnqDtX00OaYyJCU7CJ9Kq6V9mIiFirV1tjPkJgZlcPQRG1f4ucAgYYIxZRmA2628FnzIJeCpYRpuBKcHlE4GnjTEPBV/jm834a4hEJI26LhJmxphya22C0zlE3ExvA4qISMTTmZWIiEQ8nVmJiEjEU1mJiEjEU1mJiEjEU1mJiEjEU1mJiEjEU1mJiEjE+//hbWhkzIjbpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.01\n"
     ]
    }
   ],
   "source": [
    "# Test 2 √† faire X min\n",
    "dim = 10\n",
    "epoch = 3\n",
    "learning_rate = 0.01\n",
    "K = 5\n",
    "\n",
    "input = torch.randn(len(vocabulary), dim)\n",
    "output = torch.randn(len(vocabulary), dim)\n",
    "input = autograd.Variable(input, requires_grad=True)\n",
    "output = autograd.variable(output, requires_grad=True)\n",
    "\n",
    "loss_tot = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(epoch):\n",
    "    compteur = 0\n",
    "    #print(i)\n",
    "    loss_val = 0\n",
    "    test_sample = creer_echantillon(phrases, vocabulary, p_subsampling)\n",
    "    for focus, context in test_sample:\n",
    "        compteur+=1\n",
    "        neg_sample = tirage_neg_sampling(vocabulary, p_negativesampling,\n",
    "                                         focus, context,\n",
    "                                         K = K)\n",
    "        vect_sample = np.append(context, neg_sample)\n",
    "        data = torch.matmul(input[focus,], torch.t(output[context,]))\n",
    "        loss1 = - F.logsigmoid(data)\n",
    "\n",
    "        data = torch.matmul(input[focus,], torch.t(output[neg_sample,]))\n",
    "        loss2 = - F.logsigmoid(-data).sum()\n",
    "        #print(loss)\n",
    "        loss_val += loss1 + loss2\n",
    "        # Pour ensuite d√©river les matrices par rapport √† la loss\n",
    "        (loss1+loss2).backward()\n",
    "        \n",
    "        # Il faut modifier juste le .data pour ne pas perdre la structure\n",
    "        input.data = input.data - learning_rate * input.grad.data\n",
    "        output.data = output.data - learning_rate * output.grad.data\n",
    "        \n",
    "        input.grad.data.zero_()\n",
    "        output.grad.data.zero_()\n",
    "    loss_val = loss_val / len(vocabulary)\n",
    "    loss_tot.append(loss_val)\n",
    "    live_plot(loss_tot)\n",
    "end = time.time()\n",
    "print(round((end - start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le mot signature a pour coordonn√©es [ 0.99930257  0.57583964  0.15630364  0.9032513  -0.2750622   0.40516764\n",
      " -0.05151942  0.20612009 -0.8610595  -0.13646814].\n"
     ]
    }
   ],
   "source": [
    "W3 = (input + output)/2\n",
    "mot_poids = {vocabulary[index]: poids.detach().numpy() for (index, poids) in enumerate(W3)}\n",
    "for cle, valeur in list(mot_poids.items())[:1]:\n",
    "    print(\"Le mot {} a pour coordonn√©es {}.\".format(cle, valeur))\n",
    "    \n",
    "\n",
    "def cos_distance(u, v):\n",
    "    return (np.dot(u, v)  / (math.sqrt(np.dot(u, u)) *  (math.sqrt(np.dot(v, v)))))\n",
    "def mot_plus_proche(word, n=10):\n",
    "    word_distance = {}\n",
    "    for mot in mot_poids:\n",
    "        if mot != word:\n",
    "            word_distance[mot] = (cos_distance(mot_poids[mot],(mot_poids[word])))\n",
    "    word_distance = sorted(word_distance.items(), key=lambda t: t[1],reverse=True)\n",
    "    return word_distance[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alorsnickname', 0.9064449826922154),\n",
       " ('niaw', 0.884765600277602),\n",
       " ('yapas', 0.8728791103466244),\n",
       " ('squatt√©e', 0.8680008393595477),\n",
       " ('boul', 0.8631014736258095),\n",
       " ('frager', 0.8593539023424053),\n",
       " ('waen', 0.8556218463844105),\n",
       " ('surveiller', 0.853363892731487),\n",
       " ('bronzer', 0.8314212211658233),\n",
       " ('pente', 0.8300833694761184)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot_plus_proche(\"grand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation par le \"human judgement\" \n",
    "\n",
    "### D√©finition et exemple de calcul d'un spearman rank\n",
    "\n",
    "R√©sum√© de la page wikip√©dia fran√ßaise : https://fr.wikipedia.org/wiki/Corr%C3%A9lation_de_Spearman\n",
    "\n",
    "En statistique, la corr√©lation de Spearman ou rho de Spearman, est une mesure de d√©pendance statistique non param√©trique entre deux variables.\n",
    "\n",
    "La corr√©lation de Spearman est √©tudi√©e lorsque deux variables statistiques semblent corr√©l√©es sans que la relation entre les deux variables soit de type affine. Elle consiste √† trouver un coefficient de corr√©lation, non pas entre les valeurs prises par les deux variables mais entre les rangs de ces valeurs. Elle estime √† quel point la relation entre deux variables peut √™tre d√©crite par une fonction monotone. S'il n'y a pas de donn√©es r√©p√©t√©es, une corr√©lation de Spearman parfaite de +1 ou -1 est obtenue quand l'une des variables est une fonction monotone parfaite de l'autre. \n",
    "\n",
    "Pour un √©chantillon de taille n, les variables de rang ${\\displaystyle \\operatorname {rg} X_{i},\\operatorname {rg} Y_{i}}$ sont calcul√©es √† partir des donn√©es ${\\displaystyle X_{i},Y_{i}}$.\n",
    "\n",
    "La corr√©lation de Spearman est d√©finie par :\n",
    "\n",
    "${\\displaystyle r_{s}={\\frac {\\operatorname {cov} (\\operatorname {rg} _{X},\\operatorname {rg} _{Y})}{\\sigma _{\\operatorname {rg} _{X}}\\sigma _{rg_{Y}}}}} $\n",
    "\n",
    "O√π\n",
    "\n",
    " ${\\displaystyle \\operatorname {cov} (\\operatorname {rg} _{X},\\operatorname {rg} _{Y})}$ est la covariances de variables de rang,\n",
    "\n",
    "   ${\\displaystyle \\sigma _{\\operatorname {rg} _{Y}}} {\\displaystyle \\sigma _{\\operatorname {rg} _{Y}}}$ sont les √©carts-type des variables de rang.\n",
    "\n",
    "On constate que cette d√©finition correspond √† la corr√©lation de Pearson des variables de rang. Le coefficient de Spearman permet de d√©tecter des tendances monotones. Lorsque la tendance est affine, il se comporte de fa√ßon similaire au coefficient de Pearson. En revanche, il sera plus √©lev√© que la corr√©lation de Pearson si la tendance est monotone mais non affine. Plus la tendance monotone est marqu√©e, plus la valeur du coefficient est proche de 1 ou -1.\n",
    "\n",
    "De fa√ßon similaire au coefficient de Pearson, le coefficient de Spearman aura une valeur positive lorsque la tendance est croissante et n√©gative lorsqu'elle est d√©croissante.\n",
    "\n",
    "Lorsque la tendance n'est pas monotone, il aura une valeur proche de 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les √©chantillons sont corr√©l√©s (rejet de H0) p=0.000 / Valeur de la corr√©lation : 1.000\n",
      "Les √©chantillons sont corr√©l√©s (rejet de H0) p=0.000 / Valeur de la corr√©lation : 0.894\n",
      "Les √©chantillons sont non-corr√©l√©s (non rejet de H0) p=0.629 / Valeur de la corr√©lation : 0.015\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# On veut tester la corr√©lation de ces vecteurs\n",
    "# (Ce sera √† terme les vecteurs de corr√©lations entre focus et contextes)\n",
    "x = rand(1000) * 20\n",
    "x_corr = 2*x #Ce vecteur est cens√© √™tre ENTIEREMENT corr√©l√© √† X\n",
    "x_moycorr = x + (rand(1000) * 10) #Ce vecteur est cens√© √™tre FORTEMENT corr√©l√© √† X\n",
    "x_uncorr=[1] #Ce vecteur est cens√© √™tre NON corr√©l√© √† X\n",
    "for j in range(999):\n",
    "    x_uncorr.append(2)\n",
    "\n",
    "#On fait des tests √† 5 %\n",
    "alpha = 0.05\n",
    "\n",
    "corr1, p_value1 = spearmanr(x, x_corr)\n",
    "if p_value1 > alpha:\n",
    "\tprint('Les √©chantillons sont non-corr√©l√©s (non rejet de H0) p=%.3f' % p_value1)\n",
    "else:\n",
    "\tprint('Les √©chantillons sont corr√©l√©s (rejet de H0) p=%.3f' % p_value1,'/ Valeur de la corr√©lation : %.3f'% corr1)\n",
    "\n",
    "corr2, p_value2 = spearmanr(x, x_moycorr)\n",
    "if p_value2 > alpha:\n",
    "\tprint('Les √©chantillons sont non-corr√©l√©s (non rejet de H0) p=%.3f' % p_value2)\n",
    "else:\n",
    "\tprint('Les √©chantillons sont corr√©l√©s (rejet de H0) p=%.3f' % p_value2,'/ Valeur de la corr√©lation : %.3f'% corr2)\n",
    "\n",
    "corr3, p_value3 = spearmanr(x, x_uncorr)\n",
    "if p_value3 > alpha:\n",
    "\tprint('Les √©chantillons sont non-corr√©l√©s (non rejet de H0) p=%.3f' % p_value3,'/ Valeur de la corr√©lation : %.3f'% corr3)\n",
    "else:\n",
    "\tprint('Les √©chantillons sont corr√©l√©s (rejet de H0) p=%.3f' % p_value3,'/ Valeur de la corr√©lation : %.3f'% corr3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des donn√©es de human judegment et ajouter la distance cosinus du word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>corde</td>\n",
       "      <td>sourire</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>midi</td>\n",
       "      <td>ficelle</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>coq</td>\n",
       "      <td>p√©riple</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fruit</td>\n",
       "      <td>fournaise</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>autographe</td>\n",
       "      <td>rivage</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>coussin</td>\n",
       "      <td>oreiller</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>cimeti√®re</td>\n",
       "      <td>cimeti√®re</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>automobile</td>\n",
       "      <td>auto</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>joyau</td>\n",
       "      <td>bijou</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>midi</td>\n",
       "      <td>d√Æner</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word1      word2  corr\n",
       "0        corde    sourire  0.00\n",
       "1         midi    ficelle  0.00\n",
       "2          coq    p√©riple  0.06\n",
       "3        fruit  fournaise  0.11\n",
       "4   autographe     rivage  0.00\n",
       "..         ...        ...   ...\n",
       "60     coussin   oreiller  3.00\n",
       "61   cimeti√®re  cimeti√®re  4.00\n",
       "62  automobile       auto  3.94\n",
       "63       joyau      bijou  3.22\n",
       "64        midi      d√Æner  2.17\n",
       "\n",
       "[65 rows x 3 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/Kim Antunez/Documents/Projets_autres/StatApp/data_bis/word_similarity.csv', sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_mots(word1,word2):\n",
    "    if word1 in mot_poids.keys() and word2 in mot_poids.keys():\n",
    "        word_distance = (cos_distance(mot_poids[word1],(mot_poids[word2])))\n",
    "    else:\n",
    "        word_distance = float('nan')\n",
    "    return word_distance\n",
    "distance_mots_v = np.vectorize(distance_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>corr</th>\n",
       "      <th>corr_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>gar√ßon</td>\n",
       "      <td>sage</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.035004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>auto</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.493026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>verre</td>\n",
       "      <td>bijou</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.575871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>fr√®re</td>\n",
       "      <td>gars</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-0.200650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>voyage</td>\n",
       "      <td>p√©riple</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.131820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>gar√ßon</td>\n",
       "      <td>gars</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-0.172915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>midi</td>\n",
       "      <td>d√Æner</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.582349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word1    word2  corr  corr_word2vec\n",
       "20  gar√ßon     sage  0.29      -0.035004\n",
       "33    auto   voyage  0.33      -0.493026\n",
       "35   verre    bijou  0.56      -0.575871\n",
       "38   fr√®re     gars  2.00      -0.200650\n",
       "53  voyage  p√©riple  2.59      -0.131820\n",
       "59  gar√ßon     gars  3.83      -0.172915\n",
       "64    midi    d√Æner  2.17       0.582349"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"corr_word2vec\"] = distance_mots_v(df[\"word1\"],df[\"word2\"])\n",
    "print(len(df))\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le r√©sultat de word2vec est diff√©rent de celui du human judgement (non rejet de H0 = non corr√©lation) p=0.645 / Valeur de la corr√©lation : 0.214\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "#On fait des tests √† 5 %\n",
    "alpha = 0.05\n",
    "\n",
    "corr, p_value = spearmanr(df[\"corr\"], df[\"corr_word2vec\"])\n",
    "if p_value > alpha:\n",
    "\tprint('Le r√©sultat de word2vec est diff√©rent de celui du human judgement (non rejet de H0 = non corr√©lation) p=%.3f' % p_value,'/ Valeur de la corr√©lation : %.3f'% corr)\n",
    "else:\n",
    "\tprint('Le r√©sultat de word2vec est semblable celui du human judgement (rejet de H0 = non corr√©lation) p=%.3f' % p_value,'/ Valeur de la corr√©lation : %.3f'% corr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
