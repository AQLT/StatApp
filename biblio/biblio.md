# Bibliographie 

https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf  

https://arxiv.org/pdf/1301.3781.pdf

# Blog
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

# Code 

https://pytorch.org/get-started/locally/  

https://rguigoures.github.io/word2vec_pytorch/

# Negative sampling

https://medium.com/towardsdatascience/word2vec-negative-sampling-made-easy-7a1a647e07a4 

https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling (Alain)

# Softmax vs sigmoid
https://dejanbatanjac.github.io/2019/07/04/softmax-vs-sigmoid.html (Alain)

# Parall√©lisation python 
https://joblib.readthedocs.io/en/latest/parallel.html (Alain)

# Window
A paper by Levy & Goldberg, "Dependency-Based Word Embeddings", speaks a bit about the qualitative effect of window-size:

https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf

They find:

Larger windows tend to capture more topic/domain information: what other words (of any type) are used in related discussions? Smaller windows tend to capture more about word itself: what other words are functionally similar? (Their own extension, the dependency-based embeddings, seems best at finding most-similar words, synonyms or obvious-alternatives that could drop-in as replacements of the origin word.)

# ACP et TSNE 

Cours sur l'ACP : Analyse en Composantes Principales (ACP) par Marie Chavrent
TSNE : Page wiki en anglais
