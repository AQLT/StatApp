\babel@toc {french}{}
\contentsline {section}{Introduction}{2}{section*.2}%
\contentsline {section}{\numberline {1}Implémentation du modèle \emph {Word2Vec}}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}Le modèle \emph {Word2vec}, un modèle de \emph {word-embedding}}{2}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}Historique : de la sémantique vectorielle à \emph {Word2Vec}}{2}{subsubsection.1.1.1}%
\contentsline {subsubsection}{\numberline {1.1.2}\emph {Word2Vec}, un modèle d'apprentissage «\nobreakspace {}auto-supervisé\nobreakspace {}»}{3}{subsubsection.1.1.2}%
\contentsline {subsection}{\numberline {1.2}L'algorithme Skip-Gram}{4}{subsection.1.2}%
\contentsline {subsubsection}{\numberline {1.2.1}Construction de la base d'entraînement}{4}{subsubsection.1.2.1}%
\contentsline {subsubsection}{\numberline {1.2.2}Descente de gradient}{6}{subsubsection.1.2.2}%
\contentsline {paragraph}{\numberline {1.2.2.1}Version softmax}{6}{paragraph.1.2.2.1}%
\contentsline {paragraph}{\numberline {1.2.2.2}Version \emph {negative sampling}}{7}{paragraph.1.2.2.2}%
\contentsline {section}{\numberline {2}Évaluation du modèle implémenté}{8}{section.2}%
\contentsline {subsection}{\numberline {2.1}Évaluation sur un corpus fictif}{8}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Choix des meilleurs hyperparamètres pour le modèle}{8}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Nombre d'epochs, taille de fenêtre et taux d'apprentissage}{10}{subsubsection.2.2.1}%
\contentsline {paragraph}{\numberline {2.2.1.1}Le nombre d'epochs}{10}{paragraph.2.2.1.1}%
\contentsline {paragraph}{\numberline {2.2.1.2}Le taux d'apprentissage}{11}{paragraph.2.2.1.2}%
\contentsline {paragraph}{\numberline {2.2.1.3}La taille de la fenêtre}{11}{paragraph.2.2.1.3}%
\contentsline {subsubsection}{\numberline {2.2.2}Dimension des vecteurs-mots}{11}{subsubsection.2.2.2}%
\contentsline {subsection}{\numberline {2.3}Évaluation sur le corpus final}{12}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Avec \FB@og notre \FB@fg \relax {} modèle}{12}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Avec le modèle \texttt {Gensim}}{13}{subsubsection.2.3.2}%
\contentsline {section}{\numberline {3}Construction d'un indice mensuel de sentiments moyens des tweets}{15}{section.3}%
\contentsline {subsection}{\numberline {3.1}Prédire le sentiment d'un tweet à partir des \emph {word-embeddings}}{16}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Prédiction à partir du sentiment moyen des mots}{16}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Prédiction à partir des \emph {word-embeddings}}{17}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Les limites des modèles utilisés}{18}{subsubsection.3.1.3}%
\contentsline {paragraph}{\numberline {3.1.3.1}Les mots inconnus}{18}{paragraph.3.1.3.1}%
\contentsline {paragraph}{\numberline {3.1.3.2}Le processus d'annotation utilisé par la SNCF}{18}{paragraph.3.1.3.2}%
\contentsline {paragraph}{\numberline {3.1.3.3}Le domain-shift}{19}{paragraph.3.1.3.3}%
\contentsline {paragraph}{\numberline {3.1.3.4}Test d'une modification de la source de la base de test}{20}{paragraph.3.1.3.4}%
\contentsline {subsection}{\numberline {3.2}Sentiments des tweets et enquête de conjoncture auprès des ménages {[}Alain{]}}{20}{subsection.3.2}%
\contentsline {section}{Conclusion {[}sûrement Kim{]}}{20}{section*.15}%
\contentsline {section}{\numberline {A}Comment évaluer le modèle ?}{21}{appendix.A}%
\contentsline {subsection}{\numberline {A.1}Distance entre deux mots}{21}{subsection.A.1}%
\contentsline {subsection}{\numberline {A.2}Analyse en Composantes Principales}{21}{subsection.A.2}%
\contentsline {subsection}{\numberline {A.3}Algorithme \emph {t-distributed Stochastic Neighbor Embedding}}{23}{subsection.A.3}%
\contentsline {subsubsection}{\numberline {A.3.1}Jugement humain}{24}{subsubsection.A.3.1}%
