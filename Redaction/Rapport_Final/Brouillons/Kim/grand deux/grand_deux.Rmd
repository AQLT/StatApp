---
title: |
    | Analyse statistique et empirique des modèles
    | de Word Embedding sur Twitter
author: |
    | Kim Antunez, Romain Lesauvage, Alain Quartier-la-Tente
    | sous l'encadrement de Benjamin Muller (Inria)
automaticcontents: true
output:
    pdf_document:
        toc: false
        toc_depth: 2
        number_sections: true
        fig_width: 7
        fig_height: 6
        fig_caption: true
        highlight: default
        template: default.tex
        keep_tex: yes
themeoptions: "coding=utf8,language=english"
classoption: 'french'
fontsize: 11pt
geometry: margin=0.95in
lang: "french"
documentclass: "article"
header-includes:
- \usepackage{caption}
- \usepackage{graphicx}
- \usepackage{natbib}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                        fig.path = "img/markdown-",
                      cache = FALSE)
options(enable_print_style = FALSE)
```

```{r, warning=FALSE, echo=FALSE}
library(reticulate)
```

# Évaluation du modèle implémenté

## Comment évaluer le modèle ?
\label{sec:comment_evaluer}

Malgré l'utilisation généralisée des *word embedding*, très peu de travaux théoriques expliquent ce qui est réellement capturé par ces représentations de mots. 

C'est pourquoi ce modèle est principalement évalué à l'aide de méthodes empiriques. Nous allons décrire dans cette partie \ref{sec:comment_evaluer} quelques méthodes que nous avons retenues pour évaluer la qualité des vecteurs-mots obtenus. 

### Distance entre deux mots

L'un des enjeux principaux du modèle étant de pouvoir estimer la proximité entre deux vecteurs-mots, nous pouvons tout d'abord mesurer cette dernière par des calculs de distance. 

Il existe différents types de distances. Chacune d'elles possède des propriétés intéressantes et s'adaptent plus ou moins bien au problème traité. Nous avons ici retenu deux distances classiquement utilisées :  

\begin{itemize}
\item \textbf{la distance euclidienne} $ d_{e}(\vec{u},\vec{v}) = \left\| \vec{u} - \vec{v}  \right\|_2$

La longueur du vecteur mot, captée dans le cas de la distance euclidienne, est positivement corrélée à la fréquence d'apparition du mot (\cite{Schakel}). Cette information peut s'avérer utile dans l'analyse de la signification des mots, notamment lorsque l'on effectue des opérations sur les vecteurs (comme l'exemple de $\overrightarrow{Paris} - \overrightarrow{France} + \overrightarrow{Italie} = \overrightarrow{Rome}$ dans \cite{Mikolov})

Toutefois, cette dépendance à la fréquence d'apparition peut également fausser l'analyse. C'est pourquoi nous avons choisi, par la suite, de normaliser les vecteurs. 

$ d_{e}(\vec{u},\vec{v}) = \left\| \frac{\vec{u}}{\left\| \vec{u} \right\|_2} - \frac{\vec{v}}{\left\| \vec{v} \right\|_2}  \right\|_2$


\item \textbf{la similarité cosinus} $ d_{c}(\vec{u}, \vec{v}) = \frac{\vec{u}.\vec{v}}{\left\| \vec{u} \right\|_2  \left\| \vec{v} \right\|_2 }$.

La similarité cosinus correspond au produit scalaire entre les deux vecteurs normalisés. Elle mesure ainsi l'angle formé entre deux vecteurs-mots.

C'est la distance que de nombreux papiers fondateurs de la méthode \emph{Word2Vec} (comme \cite{Mikolov} ou \cite{Levy}) utilisent avec l'argument selon lequel les mots apparaissant dans des contextes similaires sont groupés dans la même direction durant l'entraînement. 
Une similarité est proche de +1 si deux mots sont positivement reliés (proches), de -1 s'ils sont négativements reliés (éloignés) et de 0 s'ils ne sont pas "reliés". 

Il est toutefois délicat d'interpréter une similarité proche de -1. On pourrait intuitivement penser à des antonymes, comme "grand" et "petit", mais dans les faits, les antonymes sont susceptibles d'apparaître dans des contextes semblables et sont donc bien souvent positivement corrélés. 

 
\end{itemize}


### Analyse en Composantes Principales

Une fois le modèle *Word2Vec* entraîné, nous obtenons des *word-embeddings* pour chacun de nos mots, représentés par des vecteurs de grandes dimensions (20, 50 ou même supérieures à 100). 

Dès lors, il devient complexe de bien observer la proximité entre deux mots. C'est pourquoi il devient utile de mobiliser des méthodes de réduction de dimensions comme l'analyse en composantes principales (ACP). L'objectif premier de cette méthode est en effet de projeter un nuage de points sur un espace de dimension inférieure afin de rendre l'information moins redondante et plus visuelle, tout en étant le plus proche possible de la réalité. 

Considérons le cas où nous disposons de $n$ individus (dans notre cas les mots) et de $p$ variables (dans notre cas, leurs composantes ou dimensions issues du modèle *Word2Vec*). On note $X = (x_{ij})$ la matrice de taille $(n,p)$ des données brutes, où $x_{ij}$ représente la valeur de la $j$-ème variable pour le $i$-ème individu. Afin de donner à chaque individu le même poids, nous centrons et réduisons les colonnes de notre matrice de données. On notera par la suite $Z = (z_{ij})$ la matrice des données centrées et réduites.

La construction des axes de l'ACP est faite par projection orthogonale. Nous utilisons ici le produit scalaire $<x,y> = x\,^t N y$ avec la métrique $N = diag(\frac{1}{n},...,\frac{1}{n})$. Ainsi, la projection orthogonale d'un individu i (vecteur ligne) $z_i$ sur une droite de vecteur directeur $v$ vaut $^tz_iv$ et les coordonnées de projection des $n$ individus valent $Zv$.

Les vecteurs directeurs des axes sont définis de manière à maximiser la dispersion du nuage (son inertie) des individus projetés et conserver ainsi au mieux les distances entre les individus. L'inertie se définit comme 

$$I(Z) = \frac{1}{n} \sum \limits_{i = 1}^n d^2(z_i,\bar{z}) = \sum \limits_{i = 1}^n var(z^j) = p$$


avec $d(z_i,z_{i'})$ la distance euclidienne entre deux individus $z_i$ et $z_{i'}$ : $d(z_i,z_{i'}) = \sum \limits_{j=1}^p (z_{ij} - z_{i'j})^2$^[Nous travaillons ici dans le cadre d'une ACP normée où la matrice $X$ a été centrée puis réduite. La réduction de $X$ a modifié les distances initiales entre individus ($d(z_i,z_{i'}) \neq d(x_i,x_{i'})$). Cela n'aurait pas été le cas si la matrice $Y$ avait été uniquement centrée (ACP non normée).]. 


 


On trouve tout d'abord le vecteur directeur $v_1$ qui orientera le premier axe de l'ACP grâce au programme suivant : 
$$v_1 =\underset{ \left\| v  \right\| = 1}{\mathrm{argmin~}} Var(Zv) =\underset{ \left\| v  \right\| = 1}{\mathrm{argmin~}} v\,^t R v $$ où $R = Var(Z) = \frac{1}{n} Z\,^t Z$ est la matrice des corrélations entre les p variables.











Puis, on choisit $v_2$ orthogonal à $v_1$ tel que l'inertie soit toujours maximisée
$$v_2 =\underset{ \left\| v  \right\| = 1, v \perp v_1}{\mathrm{argmin}} Var(Zv) $$
En procédant de manière séquentielle, on obtient $q < r$ axes orthogonaux avec $r = rg(Z)$ et $q$ choisi par le statisticien^[Différentes méthodes existent afin de déterminer le $q$ optimal, comme la règle de Kaiser ou encore celle du coude.].

On peut montrer que $\forall k < q$ :


\begin{itemize}
\item $v_k$ est un vecteur propre associé à la k\ieme{} valeur propre $\lambda_k$ de $R$
\item la composante principale $Zv_k$ est centrée et $V(Zv_k)= \lambda_k$
\item Les $Zv_k$ ne sont pas corrélés entre eux
\end{itemize}
On obtient alors la matrice $F = ZV$ des nouvelles coordonnées factorielles des individus, avec $V = (v_1,\dots,v_q)$  la matrice des vecteurs propres. 


Nous utilisons ici l'ACP en vue d'identifier les individus (ici, nos mots) qui sont proches. Pour ce faire, il suffit de représenter les coordonnées factorielles de la matrice $F$ dans des repères, en général en 2 dimensions pour une question de lisibilité. Deux mots apparaissant dans des contextes similaires seront proches sur ce repère et orientés dans la même direction.  

Enfin, pour juger de la qualité de la réduction de dimension, on calcule souvent la proportion de l'inertie totale expliquée par les $q$ premières composantes principales. 

$$ \frac{V(F)}{I(Z)} = \frac{\sum \limits_{i = 1}^q \lambda_i}{p}$$


### Algorithme t-distributed Stochastic Neighbor Embedding

Bien que l'ACP soit une première manière de résumer l'information contenue dans nos vecteurs, elle présente des limites, notamment dans les vecteurs aux trop grandes dimensions, pour lesquels l'inertie des premiers axes de l'ACP peut se révéler faible. 


Pour combler ces lacunes, un autre algorithme de réduction de dimension peut être utilisé, celui dit du t-distributed Stochastic Neighbor Embedding. 
Contraitement à l'ACP, cet algorithme est stochastique et non-linéaire et il favorise l’apparition de groupes de mots proches. 
Sa philosophie demeure cependant identique : représenter dans un espace à dimension réduite notre nuage de points de manière à repérer les mots proches.

La première étape de l'algorithme consiste en la transformation des distances euclidiennes entre les vecteurs $(X_1,...,X_n)$ en probabilités conditionnelles qui représenteront également les similarités entre nos vecteurs. Pour cela, on note :

$$ p_{j|i} = \frac{\exp{-\frac{\left\| \vec{X_i} - \vec{X_j}  \right\|^2}{2\sigma_i^2}}}{\sum_{k \neq i}{\exp{-\frac{\left\| \vec{X_i} - \vec{X_k}  \right\|^2}{2\sigma_i^2}}}}$$

La similarité entre les points $X_i$ et $X_j$ correspond à la probabilité conditionnelle que $X_i$ choisirait $X_j$ comme voisin si ces derniers étaient choisis selon une loi gaussienne centrée en $X_i$. 
Ici, $\sigma_i$ est la variance de cette gaussienne centrée en $X_i$, calculer de manière itérative de sorte à maximiser la perplexité du modèle (mesure du modèle de probabilités).

Dans une seconde étape, il faut réaliser les mêmes calculs mais dans l'espace de projection, où nous aurons nos points $(Y_1,..., Y_n)$. 
Cependant, puisqu'en dimension réduite, la gaussienne à tendance à concentrer les points, l'algorithme propose d'utilise une distribution de *Student*, d'où le nom de l'algorithme, et ainsi de calculer :

$$ q_{j|i} = \frac{(1+\left\| \vec{Y_i} - \vec{Y_j}  \right\|^2)^{-1}}{\sum_{k \neq i}{(1+\left\| \vec{Y_i} - \vec{Y_k}  \right\|^2)^{-1}}}$$

Il ne reste plus désormais qu'à déterminer les vecteurs $(Y_1,...,Y_n)$ qui minimise l'écart entre les deux distributions de probabilité. 
Pour cela, l'algorithme se base sur la divergence de Kullback–Leibler entre les distributions P et Q :

$$KL(P,Q) = \sum_{i \neq j} { p_{ij} \log{\frac{p_{ij}}{q_{ij}}}}$$

Avec $$p_{ij} = \frac{p_{i|j} + p_{j|i}}{2n}$$ où $n$ est le nombre de vecteurs.

Ensuite, on minimise grâce à une descente de gradient. On obtient alors les résultats de l'algorithme t-SNE, qu'il faut cependant analyser avec précaution : il ne s’agît pas d’un algorithme linéaire équivalent à l'ACP, on ne peut donc pas interpréter directement des éléments tels que la taille des clusters obtenus ou leur distance relative.


### Human judgement

We evaluated the word representations on four dataset, covering
word similarity and relational analogy tasks. We used two datasets to evaluate pairwise word similarity:
Finkelstein et al.’s WordSim353 [13] and Bruni et al.’s MEN [4]. These datasets contain word
pairs together with human-assigned similarity scores. The word vectors are evaluated by ranking
the pairs according to their cosine similarities, and measuring the correlation (Spearman’s ) with
the human ratings.


## Choix des « meilleurs » paramètres pour le modèle 

Expliquer ici notre cheminement (éventuellement foireux) pour choisir les différents paramètres du modèle avec 100k tweets : en particulier learning rate + window. Parler aussi des autres paramètres qui peuvent jouer mais dont on n’évalue pas l’effet.  Expliquer le rôle central joué par la seed et pourquoi et la manière dont nous procédons pour créer des pseudos intervalles de confiance pour répondre à ce problème. Bien indiquer les limites de cette partie en raison du temps de calcul (à préciser) et de la puissance machine et bien dire que l’objectif était de nous initier à la sélection de paramètres sans avoir les capacités techniques de pouvoir réaliser un travail très poussé. Voir éventuellement plus tard si on peut tester paramètres aussi sur corpus fictif

## Evaluation sur un corpus fictif

Kim : Montrer les 3 évaluations du corpus fictif (tous sauf human judgement) dans l’objectif de montrer que notre modèle semble bien implémenté et robuste !

Avant de nous attaquer au jeu de données complet, nous avons évalué un premier corpus fictif. Nous avons associé dix couples (du type [voiture, camion]), à dix mots contexte différents ([véhicule, moto, \dots]). 
Le corpus fictif est formé de 10 000 phrases composées chacune d'un mot d'un couple, de cinq mots du contexte et de trois mots bruits, tous tirés aléatoirement. 

Nous avons donc mis en oeuvre ces trois techniques sur notre corpus fictif. 
Les résultats semblent concluants : la similarité cosinus montre bien une forte corrélation entre les mots focus et contexte du corpus initial et l'ACP et l'algorithme t-SNE permettent également de montrer graphiquement cette proximité (figure).


\footnotesize
```{r, eval=FALSE}
mot_plus_proche("grand", n = 50)
#[('énorme', 0.9914256427481623),
# ('taille', 0.9905528713008166),
# […]
# ('vanille', 0.06068283530950071),
# ('salissures', 0.0539210063101789)]

```
\normalsize


## Evaluation sur le corpus final [PLUS TARD]
Montrer les 4 évaluation du corpus final de 1 000 000 de tweets. Dire en quoi le modèle semble bon, en quoi il semble pas bon (du style bon pour prédire les jours de la semaine et les chiffres de l’an dernier). 




\nocite{*}
\begin{thebibliography}{999}
\bibitem[Bortoli, C., \emph{et al} (2015)]{Schakel} Schakel, A. M., Wilson, B. J. (2015). Measuring Word Significanceusing Distributed Representations of Words. arXiv:1508.02297. \url{https://arxiv.org/pdf/1508.02297v1.pdf}.
\bibitem[Levy, O., Golberg, Y. (2015)]{Levy} Levy, O., Golberg, Y. (2015). Neural Word Embedding as Implicit Matrix Factorization.
\url{https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf}.
\bibitem[Mikolov, T., \emph{et al} (2013)]{Mikolov} Mikolov, T.,  Chen, K., Corrado, G., Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781. \url{https://arxiv.org/pdf/1301.3781.pdf}.
\end{thebibliography}

