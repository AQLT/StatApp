---
title: "*Word-Embedding* et sentiments des ménages avec Twitter"
subtitle: "Projet de statistique appliquée, Ensae"
author: "Kim Antunez, Romain Lesauvage et Alain Quartier-la-Tente"
departement: "Ensae --- 2019-2020 "
division: |
    | encadrement : \textsc{Benjamin Muller} (Inria)
    | 11/06/2020
logo: "img/LOGO-ENSAE.png"
automaticcontents: false
output:
    beamer_presentation:
        template: template.tex
        keep_tex: yes
        theme: TorinoTh
        slide_level: 3
header-includes:
- \usepackage{wrapfig}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{animate}
- \usepackage{fontawesome5}
- \usepackage{caption}
- \usepackage{graphicx}
- \usepackage{tikz}
- \usetikzlibrary{fit,arrows.meta}
- \usepackage{pifont}
- \usepackage[shortlabels]{enumitem}
- \usepackage{dsfont}
- \setlist[itemize,1]{label = --}
- \setlist[itemize,2]{label = $\circ$}
- \setlist[enumerate,1]{label={\arabic*}}
- \usepackage{lmodern}
- \usepackage{cancel}
- \usepackage{array}
- \usepackage{xspace}

themeoptions: "coding=utf8,language=french"
classoption: 'usepdftitle=false,french'
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE)
library(knitr)
library(kableExtra)
library(clue)
library(mvtnorm)
library(plot3D)
#library(portes)
library(shiny)
library(shinyjs)
library(shinyWidgets)
```

###  Introduction (1/2)

- *Word2vec* = modèle de NLP développé par Google (Mikolov et al (2013)). 

- **Objectif** = *word-embedding* : donner une représentation vectorielle aux mots.
    * \faMinusSquare{} vecteurs difficilement interprétables
    * \faPlusSquare{} tâches d'apprentissage facilitées

\pause

\bigskip

-  \faArrowCircleRight{} Réseau de neurones à 2 couches permettant de traiter des grandes bases de données. 
    * mots apparaissant dans un même contexte = représentations vectorielles proches
    *
$$
\overrightarrow{Paris} - \overrightarrow{France} + \overrightarrow{Italie} = \overrightarrow{Rome}
$$

### Introduction (2/2)

\resizebox{\textwidth}{!}{\input{img/schema_recap.tex}}

# Le modèle *word2vec*

### Sommaire
\tableofcontents[currentsection, hideothersubsections]

## L'approche Skip-gram

### L'approche Skip-gram


\begin{columns}
\begin{column}{0.65\textwidth}
Approche retenue : Skip-gram
\begin{itemize}
\item étant donné un mot \emph{focus} quels pourraient être ses voisins (\emph{contextes}) ?
\item les \emph{contextes} dépendent d'un paramètre : la \highlight{fenêtre} $w$
\end{itemize}
\medskip

\medskip \onslide<2->{
Exemple:
\begin{quote}
\LARGE \textbf{``}\normalsize \emph{Espérons que la présentation sous Teams se passe bien!!!} \LARGE \textbf{''}\normalsize
\end{quote}
Voisins(\texttt{passe}, $w=1$) =  \texttt{[se,\ bien]} 
Voisins(\texttt{passe}, $w=2$) =  \texttt{[Teams,\ se,\ bien]} 
}
\end{column}
\begin{column}{0.4\textwidth} 
\includegraphics[width=\textwidth]{img/skip_gram.png}

\end{column}
\end{columns}



## Construction de la base d'entraînement

### Construction de la base d'entraînement (1/2)

\bcoutil À partir de couples `[focus, contexte]`, on met itérativement à jour deux matrices $W_e$ et $W_s$. Représentation vectorielle finale :
$$
\frac{W_e+W_s}{2}=\underbrace{
\begin{pmatrix}
\text{représentation mot 1} \\ \vdots
\\
\text{représentation mot }n 
\end{pmatrix}}_{\text{dimension }[n\times dim]}
$$
\pause
Pour chaque phrase on :

- supprime la ponctuation, met tout en minuscule

- renomme les mots rares en « lowfrequency » .

- effectue un sous-échantillonnage des mots (\highlight{subsampling})

- tire au hasard un mot *focus* et un mot *contexte* associé

\faArrowCircleRight{} on parcourt la base \highlight{epochs} fois

### Construction de la base d'entraînement (2/2)

Exemple avec $w=2$: 

\begin{quote}
\LARGE \textbf{``}\normalsize \emph{Espérons que la présentation sous Teams se passe bien!!!} \LARGE \textbf{''}\normalsize
\end{quote}

- on supprime la ponctuation, met tout en minuscule, renomme les mots rares 
\faArrowCircleRight{} `[espérons, que, la, présentation, sous, teams, se, passe, bien]` \pause

- on effectue un sous-échantillonnage des mots (\highlight{subsampling})  
\faArrowCircleRight{} `[espérons, X, X, présentation, X, teams, se, passe, X]`
\pause
- on tire au hasard un mot *focus* et un mot *contexte* associé  
\faArrowCircleRight{} On tire un couple au hasard parmi `[présentation, teams]`, `[teams, présentation]`,  `[teams, se]`,  `[teams, passe]`,  `[se, teams]`, \dots

## *softmax* et *negative sampling*

### Actualisation de $W_e$ et $W_s$

Pour chaque couple `[focus, contexte]` : actualisation de $W_e$ et $W_s$ par descente de gradient :
$$
\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta Loss(\theta^{(t)})
$$
$\eta$ \highlight{taux d'apprentissage} et $Loss(\theta)$ fonction de perte

\pause
Deux approches :

1.  *softmax* : pour un mot `focus` on estime la probabilité que les autres mots soient voisins (classification multiclasse) 
$$
\mathbb P(w_{contexte}\vert w_{focus}) = ?
$$
 \pause
2. *negative sampling* : pour chaque couple `[focus, mot2]` on estime la probabilité que `mot2` soit voisin de `focus` (classification binaire)
$$
\mathbb P(D=1\vert w_{focus},w_{mot2}) = ?
$$

### *softmax* et *negative sampling*

Pour chaque couple `[focus, contexte]` :


1.  *softmax* : on maximise
$$
\mathbb P(w_{contexte}\vert w_{focus}) = 
\frac{
\exp(W_{e,w_{focus}}\times {}^tW_{s,w_{contexte}})
}{
\sum_{i=1}^n\exp(W_{e,w_{focus}}\times {}^tW_{s,w_{i}})
}
$$
\bcsmmh Complexité : $\mathcal O (n)$ et $n\simeq$ 70 000 \pause

2. *negative sampling* : on tire $K=5$ mots "négatifs" $(w_{neg,\,i})_{i=1..K}$ a priori non liés à `[focus, contexte]`  
On maximise $\mathbb P(D=1\vert w_{focus},w_{contexte})$ et $\mathbb P(D=0\vert w_{focus},w_{neg,\,i})$
$$
\begin{cases}
\mathbb P(D=1\vert w_{focus},w_{contexte})&=\sigma(W_{e,w_{focus}}{}^tW_{s,w_{contexte}})  \\
\mathbb P(D=0\vert w_{focus},w_{neg,\,i})&=\sigma(-W_{e,w_{focus}}{}^tW_{s,w_{neg,\,i}}) 
\\\sigma(x)=\frac{1}{1+\exp(-x)}
\end{cases}
$$
\bcsmbh Complexité : $\mathcal O (K)$ 


# Évaluation du modèle

### Sommaire
\tableofcontents[currentsection, hideothersubsections]

## Évaluation sur un corpus fictif

### Comment évaluer le modèle ?

<!-- \textbf{Problème} : utilisation généralisée des *word-embeddings* mais peu de travaux théoriques expliquent ce qui est capturé, comment évaluer le modèle ? -->

Les vecteurs-mots sont de grande dimension : comment juger de leur qualité et de leurs proximités ?

- \textbf{ACP et t-SNE} :  réduire la dimension et analyser les proximités.

- \textbf{Similarité cosinus} : distance entre vecteurs-mots.

- \textbf{Jugement humain} : corrélations entre les proximités de nos vecteurs-mots et une base de proximités de mots construites par le jugement d'individus.

\bigskip

\faArrowCircleRight{} Évaluer sur un corpus fictif puis sur l'ensemble des tweets



### Évaluation sur un corpus fictif (1/2)
\textbf{Idée} : construire un corpus fictif pour lequel on connaît le résultat attendu.

\textbf{En pratique} :
\begin{itemize}
\item On génère 10 groupes de mots composés d'un couple de référence et de 10 autres mots contexte.
\item On construit 10 000 phrases en tirant au hasard :
\begin{itemize}
\item 1 des groupes de mots ;
\item 1 des 2 mots \og références \fg{} du groupe ;
\item 5 mots contextes ;
\item 3 mots bruits parmi une liste de 100 mots.
\end{itemize}
\item On mélange les 9 mots de chaque phrase.
\end{itemize}

### Évaluation sur un corpus fictif (2/2)

\begin{columns}
\begin{column}{0.5\textwidth} \footnotesize
\centering
\begin{tabular}{|c|>{\centering\arraybackslash}p{2.5cm}|}
    \hline
    mot & similarité cosinus avec \og grand \fg{} \tabularnewline
    \hline
    longueur & 0,982   \tabularnewline
    petit & 0,981   \tabularnewline
    s & 0,979   \tabularnewline
    $\vdots$ & $\vdots$    \tabularnewline
    allates & $-0,784$ \tabularnewline
    \hline
 \end{tabular}

 \begin{tabular}{|c|>{\centering\arraybackslash}p{2.5cm}|}
    \hline
    mot & similarité cosinus avec \og petit \fg{} \tabularnewline
    \hline
    taille & 0,987   \tabularnewline
    longueur & 0,983   \tabularnewline
    grand & 0,981   \tabularnewline
    $\vdots$ & $\vdots$    \tabularnewline
    allates & $-0,810$ \tabularnewline
    \hline
 \end{tabular}

\end{column}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=1\textwidth]{img/tsne}
\end{center}
\end{column}
\end{columns}

\footnotesize 
\emph{Paramètres utilisés : ep = 50 / lr = 0,01 / w = 5 / dim = 10.}
\normalsize

\medskip

\faArrowCircleRight{} implémentation semble validée (résultats conformes aux attendus) 

<!-- Kim : mettre le graphique du tableau de droite ?  -->

## Choix des meilleurs hyperparamètres

### Déterminer les hyperparamètres

- \emph{Word2vec} se base sur différents choix d'hyperparamètres :
    * taille de la fenêtre (\emph{w})
    * nombre d'epochs (\emph{ep})
    * taux d'apprentissage (\emph{lr})
    * dimension des \emph{word-embeddings} (\emph{dim})

\medskip

\pause

- Détermination empirique des hyperparamètres :
    * corrélation de Spearman entre nos vecteurs-mots et une base de jugement humain
    * chronophage (il faut relancer le modèle à chaque fois). 
  
\medskip
  
  \faArrowCircleRight{} Utilisation complémentaire de \texttt{Gensim} puis validation avec notre implémentation.



### Exemple : epochs, fenêtre et taux d'apprentissage

\begin{figure}[htp]
\begin{center}
\includegraphics[width=1\textwidth]{img/test_parametres.png}
\end{center}
\vspace{-0.3cm}
\footnotesize
\emph{Paramètre utilisé : dim = 50}
\end{figure}

### Valeurs retenues pour les hyperparamètres

\begin{itemize}

\item \textbf{Nombre d'epochs} : qualité des résultats croît avec le nombre d'epochs

\faArrowCircleRight{} \textbf{ep = 100}.

\item \textbf{Taille de fenêtre} : capte des informations sémantiques différentes selon sa valeur

\faArrowCircleRight{} \textbf{w = 4}.

\item \textbf{Taux d'apprentissage} : 0,02 donne de meilleurs résultats

\faArrowCircleRight{} \textbf{lr = 0,02}.

\item \textbf{Dimension} : qualité des résultats croît avec la dimension jusqu'à 300 puis décroît. Peu de différences entre 100 et 300.

\faArrowCircleRight{} \textbf{dim = 100}.

\end{itemize}

## Évaluation sur le corpus de tweets

### Évaluation sur le corpus de tweets (1/2)

\begin{figure}
\begin{minipage}{.4\textwidth}


« Notre » modèle

\medskip

\footnotesize
\textbf{ Spearman : } 0,57 (p-v : 4,1 \%)
\normalsize

\medskip

\faArrowCircleRight{} \textbf{bons} résultats


\end{minipage}%
\begin{minipage}{.6\textwidth}
\footnotesize

\begin{table}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
    \hline
\textbf{bonjour} & \textbf{femme} & \textbf{1} \tabularnewline
\emph{(669)} & \emph{(264)} & \emph{(765)} \tabularnewline
       \hline
\includegraphics[height=2mm]{img/emojis/1.png} (0,59) & quelle (0,49) & 5 (0,55) \tabularnewline
\includegraphics[height=2mm]{img/emojis/2.png} (0,59) & cette (0,46) & mois (0,51) \tabularnewline
merci (0,54) & une (0,44) & 10 (0,49) \tabularnewline
nuit (0,48) & vie (0,44) & 2 (0,48) \tabularnewline
bisous (0,47) & grippe (0,44) & top (0,48) \tabularnewline
    \hline
 \end{tabular}
\captionsetup{margin=0cm,format=hang,justification=justified}

\end{center}
\tiny
\emph{ep = 80 / w = 4 / lr = 0,02 / dim = 100 / base : 100 000 tweets}

\end{table}
\normalsize


\end{minipage}
\end{figure}

\pause 

\bigskip

\begin{figure}
\begin{minipage}{.4\textwidth}

Modèle \texttt{Gensim}

\medskip

\footnotesize
\textbf{ Spearman : } 0,50 (p-v : 0,0 \%)
\normalsize

\medskip

\faArrowCircleRight{} \textbf{très bons} résultats

\end{minipage}%
\begin{minipage}{.6\textwidth}
\footnotesize

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
    \hline
\textbf{bonjour} & \textbf{femme} & \textbf{1} \tabularnewline
\emph{(17 043)} & \emph{(6 177)} & \emph{(21 055)} \tabularnewline
       \hline
bonsoir (0,85) & fille (0,86) & 2 (0,65)  \tabularnewline
bjr (0,75) & copine (0,74) & 3 (0,64) \tabularnewline
hello (0,71) & meuf (0,71) & 6 (0,63) \tabularnewline
salut (0,66) & demoiselle (0,66) & 4 (0,62) \tabularnewline
coucou (0,55) & nana (0,66) & 7 (0,60) \tabularnewline
    \hline
 \end{tabular}
\captionsetup{margin=0cm,format=hang,justification=justified}
\end{center}
\tiny
\emph{ep = 100 / w = 4 / lr = 0,02 / dim = 100 / base : ensemble des tweets}

\medskip

\footnotesize{5 plus proches voisins par similarité cosinus}
\end{table}

\normalsize

\end{minipage}
\end{figure}

### Évaluation sur le corpus de tweets (2/2)

\begin{figure}
\begin{minipage}{.5\textwidth}

\begin{center}
\includegraphics[width=0.95\textwidth]{img/acp_gensim.png}
\end{center}
\emph{ACP sur un corpus réduit de mots.}

\pause

\end{minipage}%
\begin{minipage}{.5\textwidth}

  \centering
  \includegraphics[width=\linewidth]{img/acp_reine.png}
  \emph{$\protect\overrightarrow{Roi} - \protect\overrightarrow{Homme} + \protect\overrightarrow{Femme} = $ ?}

\end{minipage}

\end{figure}

\medskip

\faArrowCircleRight{} Réduction de dimension des vecteurs-mots et (parfois) opérations sur les mots \textbf{convaincants} 

# Indice de sentiments

### Sommaire
\tableofcontents[currentsection, hideothersubsections]

## Prédire le sentiment d'un tweet

### Prédire le sentiment d'un tweet

- \textbf{Idée} : associer à chaque tweet un sentiment
    * 1 s'il est positif
    * 0 s'il est négatif

\medskip

- Base de 23 000 tweets annotés sur les transports urbains :
    * **base d'entraînement** : 16 000 tweets
    * **base de test** : 7 000 tweets

\medskip

- 2 approches :
    * **Modèle lexical** : utiliser l'information des tweets annotés pour construire un sentiment moyen par mot.
    * **Modèle logit** : utiliser les *word-embeddings* comme prédicteurs d'une régression logistique.



### Modèle lexical : sentiment moyen des mots

Le sentiment prédit d'un tweet $t$ composé de $n$ mots sera : 
<!-- problème indicatrice -->
$$S_{1,\gamma}(t) = \mathds{1}\left\{ \frac{1}{n} \sum \limits_{i=1}^n \alpha_i \geq \gamma\right\}  \qquad \in \{ 0,1 \}$$
\begin{itemize}
\item $\gamma \in [-1,1]$ un seuil fixé ;
\item $\alpha_i = \frac{nb_+(i) - nb_-(i)}{nb_+(i) + nb_-(i)} \in [-1,1]$  sentiment moyen du mot $i$ calculé à partir du nombre de tweets positifs ($nb_+(i)$) et négatifs ($nb_-(i)$) dans lesquels il apparaît. 
\end{itemize}

\faArrowCircleRight{} \emph{Accuracy}\footnote{Taux de tweets dont le sentiment est bien prédit.} =  89,1 \% ($\gamma^* = -0,14$).

### Modèle logit : prédiction grâce aux *word-embeddings*

Le sentiment prédit d'un tweet $t$ sera :
 $$S_{2,\gamma}(t) =\mathds{1}\left\{   \mathbb{P}(Y_i = 1 | X_{i}) \ge \gamma\right\} \qquad \quad \in \{ 0,1 \}$$
Avec : 
$$Y_i = \mathds{1}\left\{ \sum_{i = 1}^n \beta_i X_{i,j} + \varepsilon_i \geq 0 \right\} \quad  \mathbb{P}(Y_i = 1 | X_{i}) = F_{\varepsilon}\left(\sum_{i = 1}^n \beta_i X_{i,j}\right)$$



<!-- $$ -->
<!-- Y_i = 1\left\{ \sum_{i = 1}^n \beta_i X_{i,j} + \varepsilon_i \geq 0 \right\}  -->
<!-- $$ -->
<!-- $$ -->
<!-- \mathbb{P}(Y_i = 1 | X_{i}) = F_{\varepsilon}\left(\sum_{i = 1}^n \beta_i X_{i,j}\right) -->
<!-- $$ -->

\normalsize
\begin{itemize}
\item $Y_i$ le sentiment du tweet $i$ ;
\item $X_{i,1}, \dots, X_{i,n}$ les coordonnées de la \emph{sentence-embedding} du tweet $i$ ;
\item $\varepsilon_i$ le résidu de notre modèle, de fonction de répartition $F_{\varepsilon}$ qui vaudra $F_{\varepsilon}(x) = \frac{1}{1 + e^{-x}}$ dans le cas d'un modèle logit et $F_{\varepsilon}(x) = \Phi(x)$ (fonction de répartition d'une loi $\mathcal{N}(0, 1)$) dans le cas d'un modèle probit. 
\end{itemize}

### Spécifications du modèle logit

Plusieurs points à traiter :

- Doit-on garder les *stop-words* ? \textsc{oui}

- Comment traiter les mots inconnus ? \textsc{affecter le vecteur-mot lowfrequency}

- Modèle probit ou logit ? \textsc{logit}

\bigskip

\bigskip


\faArrowCircleRight{} \emph{Accuracy} =  69,8 \% ($\gamma^* \simeq 0,5$).

### Limites des modèles utilisés

**Modèle lexical \underline{ici} meilleur que le modèle logit** car \dots

1. Davantage de mots inconnus dans le modèle logit (4,6 \% des mots contre 1,4 \% dans le modèle lexical).

\pause

2. Le processus d’annotation utilisé pour les tweets sur les transports urbains reproduit en partie par le modèle lexical ?

\pause

3. Le *domain shift*.  

\pause

\bigskip

\faArrowCircleRight{} Utilisation d'une nouvelle base de test pour neutraliser certains de ces effets. 

**Modèle logit \underline{alors} meilleur que le modèle lexical**\newline(*Accuracy* de 61,9 \% contre 55,9 \%).



## Sentiments des tweets et enquête de conjoncture auprès des ménages

### Sentiments des tweets et enquête Camme

\centering\includegraphics[width =\textwidth]{img/rmd-graphSentiments-1}

\raggedright  \pause
\begin{columns}
\begin{column}{0.5\textwidth} \bcsmmh
\begin{itemize}
\item Indicateurs relativement éloignés de l'enquête Camme
\item Similarité (DTW) avec indicateur Camme plus proche avec modèle lexical que modèle word-embedding 
\end{itemize}

\end{column}\pause
\begin{column}{0.5\textwidth} \bcsmbh
\begin{itemize}
\item Modèle word-embedding utile pour prévoir indicateur Camme (causalité de Granger) $\ne$ modèle lexical
\item Modèle indicateur \textbf{avancé} des sentiments des ménages
\end{itemize}

\end{column}
\end{columns}


# Conclusion {.unnumbered}

### Conclusion (1/2) 

- *Word2vec* \dots

	* capture **très bien** la \underline{sémantique des mots} dans un texte
	* prédit **assez bien** le \underline{sentiment d’une phrase}
	* est **potentiellement utile** pour \underline{prédire l’indicateur synthétique de}  \underline{confiance des ménages de l’Insee} \dots
	* \dots mais demeure **très différent** de cet indicateur (en évolution)

\bigskip

\pause

- Pourquoi très différent ?
    * Principalement en raison de leurs **différentes philosophies** \footnotesize (sujets spécifiques de Camme VS positivité ou non des tweets pour notre indice) \normalsize \dots
    * \dots{} mais aussi à cause des **limites de la base d’entraînement** de tweets annotés \footnotesize (domain-shift, processus d’annotation, mots inconnus) \normalsize

### Conclusion (2/2) 

Pistes d'amélioration ?

-  disposer d’une **base de tweets traitant de sujets divers, et bien annotés** \footnotesize (gradation de sentiments, modèles de type BERT, analyse approfondie du contenu et des auteurs des tweets \dots) \normalsize


-  améliorer le **prétraitement des tweets** \footnotesize (orthographe des mots, modèle à séquences d’unités de sous-mots type *fasttext* \dots)\normalsize


-  utiliser des **modèles d’analyse de sentiment plus élaborés** \footnotesize  (type réseaux de
neurones récurrents)\normalsize





### Merci pour votre attention

\href{https://github.com/ARKEnsae/TweetEmbedding}{\faGithub{} ARKEnsae/TweetEmbedding}  

\href{https://arkensae.github.io//TweetEmbedding/Redaction/Rapport_Final/Rapport.pdf}{\faEdit{} Rapport du projet}  

\begin{center}
\includegraphics[width = 2.5cm]{img/LOGO-ENSAE.png}
\end{center}



