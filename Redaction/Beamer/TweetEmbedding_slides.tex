\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[10pt,xcolor=table,color={dvipsnames,usenames},ignorenonframetext,usepdftitle=false,french]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{caption}
\captionsetup{skip=0pt,belowskip=0pt}
%\setlength\abovecaptionskip{-15pt}
\usepackage{lmodern}
\usepackage{amssymb,amsmath,mathtools,multirow}
\usepackage{float,hhline}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{mathtools}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[coding=utf8,language=french,
,titlepagelogo=img/LOGO-ENSAE.png
]{TorinoTh}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Word-Embedding et sentiments des ménages avec Twitter},
            pdfauthor={Kim Antunez, Romain Lesauvage et Alain Quartier-la-Tente},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\newif\ifbibliography
% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \let\insertpartnumber\relax
  \let\partname\relax
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \begin{frame}{Sommaire}
    \tableofcontents[currentsection, hideothersubsections]
    \end{frame}
  \fi
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  %\setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  }
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{animate}
\usepackage{fontawesome5}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{fit,arrows.meta}
\usepackage{pifont}
\usepackage[shortlabels]{enumitem}
\usepackage{dsfont}
\setlist[itemize,1]{label = --}
\setlist[itemize,2]{label = $\circ$}
\setlist[enumerate,1]{label={\arabic*}}
\usepackage{lmodern}
\usepackage{cancel}
\usepackage{array}

\title{\emph{Word-Embedding} et sentiments des ménages avec Twitter}
\ateneo{Projet de statistique appliquée, Ensae}
\author{Kim Antunez, Romain Lesauvage et Alain Quartier-la-Tente}
\date{}


\setrellabel{}

\setcandidatelabel{}

\rel{}
\division{11/06/2020}

\departement{Ensae — 2019-2020}
\makeatletter
\let\@@magyar@captionfix\relax
\makeatother

\DeclareMathOperator{\Cov}{Cov}
\newcommand{\E}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\V}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\cov}[2]{\Cov\left( #1\,,\,#2 \right)}

\begin{document}
\begin{frame}[plain,noframenumbering]
\titlepage
\end{frame}

\begin{frame}{Introduction (1/2)}
\protect\hypertarget{introduction-12}{}

\begin{itemize}
\item
  modèle de NLP développé par Google (Mikolov et al (2013))
\item
  \textbf{objectif} = \emph{word-embedding} : donner une représentation
  vectorielle aux mots
\end{itemize}

\pause

\begin{itemize}
\item
  \bcloupe réseau de neurones à deux couches permettant de traiter des
  grandes bases de données \pause
\item
  les mots avec le même contexte ont des représentations vectorielles
  proches : \[
  \overrightarrow{Paris} - \overrightarrow{France} + \overrightarrow{Italie} = \overrightarrow{Rome}
  \]
\end{itemize}

\end{frame}

\begin{frame}{Introduction (2/2)}
\protect\hypertarget{introduction-22}{}

\resizebox{\textwidth}{!}{\input{img/schema_recap.tex}}

\end{frame}

\hypertarget{word2vec}{%
\section{\texorpdfstring{\emph{word2vec}}{word2vec}}\label{word2vec}}

\hypertarget{lapproche-skip-gram}{%
\subsection{L’approche Skip-gram}\label{lapproche-skip-gram}}

\begin{frame}{L’approche Skip-gram}
\protect\hypertarget{lapproche-skip-gram-1}{}

\begin{columns}
\begin{column}{0.6\textwidth}
Approche retenue : Skip-gram
\begin{itemize}
\item étant donné un mot \emph{focus} quels pourraient être ses voisins (\emph{contextes}) ?
\item les \emph{contextes} dépendent d'un paramètre : la \highlight{fenêtre} $w$
\end{itemize}
\medskip

\medskip \onslide<2->{
Exemple $w=2$:
\begin{quote}
\LARGE \textbf{``}\normalsize \emph{Espérons que la présentation sous Teams se passe bien} \LARGE \textbf{''}\normalsize
\end{quote}
Voisins(\texttt{passe}) =  \texttt{[Teams,\ se,\ bien]} 
}
\end{column}
\begin{column}{0.4\textwidth} 
\includegraphics[width=\textwidth]{img/skip_gram.png}

\end{column}
\end{columns}

\onslide<3->{
Approche CBOW (non retenue) : objectif inverse}

\end{frame}

\hypertarget{construction-de-la-base-dentrauxeenement}{%
\subsection{Construction de la base
d’entraînement}\label{construction-de-la-base-dentrauxeenement}}

\begin{frame}[fragile]{Construction de la base d’entraînement (1/2)}
\protect\hypertarget{construction-de-la-base-dentrauxeenement-12}{}

\bcoutil À partir de couples \texttt{{[}focus,\ contexte{]}}, on met
itérativement à jour deux matrices \(W_e\) et \(W_s\). Représentation
vectorielle finale : \[
\text{vocabulaire}
\begin{cases}\\\\\\
\end{cases}\frac{W_e+W_s}{2}=\underbrace{
\begin{pmatrix}
\text{représentation mot 1} \\
\\
\text{représentation mot }n 
\end{pmatrix}}_{\text{dimension }dim}
\] \pause Pour chaque phrase on :

\begin{itemize}
\item
  supprime la ponctuation, met tout en minuscule
\item
  effectue un sous-échantillonnage des mots (\highlight{subsampling})
\item
  tire au hasard un mot \emph{focus} et un mot \emph{contexte} associé
\end{itemize}

\faArrowCircleRight{} on parcourt la base \highlight{epochs} fois

\end{frame}

\begin{frame}[fragile]{Construction de la base d’entraînement (2/2)}
\protect\hypertarget{construction-de-la-base-dentrauxeenement-22}{}

Exemple avec \(w=2\):

\begin{quote}
\LARGE \textbf{``}\normalsize \emph{Espérons que la présentation sous Teams se passe bien!!!} \LARGE \textbf{''}\normalsize
\end{quote}

\begin{itemize}
\item
  on supprime la ponctuation, met tout en minuscule\\
  \faArrowCircleRight{}
  \texttt{{[}espérons,\ que,\ la,\ présentation,\ sous,\ teams,\ se,\ passe,\ bien{]}}
  \pause
\item
  on effectue un sous-échantillonnage des mots
  (\highlight{subsampling})\\
  \faArrowCircleRight{}
  \texttt{{[}espérons,\ X,\ X,\ présentation,\ X,\ teams,\ se,\ passe,\ X{]}}
  \pause
\item
  on tire au hasard un mot \emph{focus} et un mot \emph{contexte}
  associé\\
  \faArrowCircleRight{} On tire un mot au hasard parmi
  \texttt{{[}présentation,\ teams{]}},
  \texttt{{[}teams,\ présentation{]}}, \texttt{{[}teams,\ se{]}},
  \texttt{{[}teams,\ passe{]}}, \texttt{{[}se,\ teams{]}}, \dots
\end{itemize}

\end{frame}

\hypertarget{softmax-et-negative-sampling}{%
\subsection{\texorpdfstring{\emph{softmax} et \emph{negative
sampling}}{softmax et negative sampling}}\label{softmax-et-negative-sampling}}

\begin{frame}[fragile]{Actualisation de \(W_e\) et \(W_s\)}
\protect\hypertarget{actualisation-de-w_e-et-w_s}{}

Pour chaque couple \texttt{{[}focus,\ contexte{]}} : actualisation de
\(W_e\) et \(W_s\) par descente de gradient : \[
\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta Loss(\theta^{(t)})
\] \(\eta\) \highlight{taux d'apprentissage} et \(Loss(\theta)\)
fonction de perte

\pause

Deux approches :

\begin{enumerate}
[1.]
\tightlist
\item
  \emph{softmax} : pour un mot \texttt{focus} on estime la probabilité
  que les autres mots soient voisins (classification multiclasse) \[
  \mathbb P(w_{contexte}\vert w_{focus}) = ?
  \] \pause
\item
  \emph{negative sampling} : pour chaque couple
  \texttt{{[}contexte,\ mot2{]}} on estime la probabilité que
  \texttt{mot2} soit voisin de \texttt{contexte} (classification
  binaire) \[
  \mathbb P(D=1\vert w_{focus},w_{mot2}) = ?
  \]
\end{enumerate}

\end{frame}

\begin{frame}[fragile]{\emph{softmax} et \emph{negative sampling}}
\protect\hypertarget{softmax-et-negative-sampling-1}{}

Pour chaque couple \texttt{{[}focus,\ contexte{]}} :

\begin{enumerate}
[1.]
\item
  \emph{softmax} : on maximise \[
  \mathbb P(w_{contexte}\vert w_{focus}) = 
  \frac{
  \exp(W_{e,w_{focus}}\times {}^tW_{s,w_{contexte}})
  }{
  \sum_{i=1}^n\exp(W_{e,w_{focus}}\times {}^tW_{s,w_{i}})
  }
  \] \bcsmmh Complexité : \(\mathcal O (n)\) et \(n\simeq\) 70 000
  \pause
\item
  \emph{negative sampling} : on tire \(K=5\) mots “négatifs”
  \((w_{neg,\,i})_{i=1..K}\) a priori non liés à
  \texttt{{[}focus,\ contexte{]}}\\
  On maximise \(\mathbb P(D=1\vert w_{focus},w_{contexte})\) et
  \(\mathbb P(D=0\vert w_{focus},w_{neg,\,i})\) \[
  \begin{cases}
  \mathbb P(D=1\vert w_{focus},w_{contexte})&=\sigma(W_{e,w_{focus}}{}^tW_{s,w_{contexte}})  \\
  \mathbb P(D=0\vert w_{focus},w_{neg,\,i})&=\sigma(-W_{e,w_{focus}}{}^tW_{s,w_{neg,\,i}}) 
  \\\sigma(x)=\frac{1}{1+\exp()}
  \end{cases}
  \] \bcsmbh Complexité : \(\mathcal O (K)\)
\end{enumerate}

\end{frame}

\hypertarget{uxe9valuation-du-moduxe8le}{%
\section{Évaluation du modèle}\label{uxe9valuation-du-moduxe8le}}

\hypertarget{uxe9valuation-sur-un-corpus-fictif}{%
\subsection{Évaluation sur un corpus
fictif}\label{uxe9valuation-sur-un-corpus-fictif}}

\begin{frame}{Comment évaluer le modèle ?}
\protect\hypertarget{comment-uxe9valuer-le-moduxe8le}{}

Les vecteurs-mots sont de grande dimension : comment juger de leur
qualité et de leurs proximités ?

\begin{itemize}
\item
  \textbf{Similarité cosinus} : distance entre vecteurs-mots.
\item
  \textbf{ACP et t-SNE} : réduire la dimension et analyser les
  proximités.
\item
  \textbf{Jugement humain} : corrélations entre les proximités de nos
  vecteurs-mots et une base de proximités de mots construites par le
  jugement d’individus
\end{itemize}

\end{frame}

\begin{frame}{Évaluation sur un corpus fictif (1/2)}
\protect\hypertarget{uxe9valuation-sur-un-corpus-fictif-12}{}

\textbf{Idée} : construire un corpus fictif pour lesquels on connaît le
résultat attendu.

\textbf{En pratique} :

\begin{itemize}
\item On génère 10 groupes de mots composés d'un couple de référence et de 10 autres mots contexte.
\item On construit 10 000 phrases en tirant au hasard :
\begin{itemize}
\item 1 des groupes de mots ;
\item 1 des 2 mots \og références \fg{} du groupe ;
\item 5 mots contextes ;
\item 3 mots bruits parmi une liste de 100 mots.
\end{itemize}
\item On mélange les 9 mots de chaque phrase.
\end{itemize}

\end{frame}

\begin{frame}{Évaluation sur un corpus fictif (2/2)}
\protect\hypertarget{uxe9valuation-sur-un-corpus-fictif-22}{}

\begin{table}[!h]
\begin{center}
\begin{tabular}{|c|>{\centering\arraybackslash}p{3cm}|}
    \hline
    mot & similarité cosinus avec \og grand \fg{} \tabularnewline
    \hline
    longueur & 0,982   \tabularnewline
    petit & 0,981   \tabularnewline
    s & 0,979   \tabularnewline
    $\vdots$ & $\vdots$    \tabularnewline
    susiens & $- 0,735$ \tabularnewline
    allates & $-0,784$ \tabularnewline
    %produit & 0,100   \tabularnewline
    %voiture & 0,097   \tabularnewline
    \hline
 \end{tabular}
\begin{tabular}{|c|>{\centering\arraybackslash}p{3cm}|}
    \hline
    mot & similarité cosinus avec \og petit \fg{} \tabularnewline
    \hline
    taille & 0,987   \tabularnewline
    longueur & 0,983   \tabularnewline
    grand & 0,981   \tabularnewline
    $\vdots$ & $\vdots$    \tabularnewline
    alesiez & $- 0,745$ \tabularnewline
    allates & $-0,810$ \tabularnewline
    %citrine & 0,129   \tabularnewline
    %voiture & 0,121   \tabularnewline
    \hline
 \end{tabular}
\end{center}
\footnotesize
\emph{Paramètres utilisés : ep = 50 / lr = 0,01 / w = 5 / dim = 10.}
\end{table}

\faArrowCircleRight{} implémentation semble validée (résultats conformes
aux attendus)

\end{frame}

\hypertarget{choix-des-meilleurs-hyperparamuxe8tres}{%
\subsection{Choix des meilleurs
hyperparamètres}\label{choix-des-meilleurs-hyperparamuxe8tres}}

\begin{frame}{Déterminer les hyperparamètres}
\protect\hypertarget{duxe9terminer-les-hyperparamuxe8tres}{}

\begin{itemize}
\tightlist
\item
  \emph{Word2vec} se base sur différents choix d’hyperparamètres :

  \begin{itemize}
  \tightlist
  \item
    taille de la fenêtre (\emph{w})
  \item
    nombre d’epochs (\emph{ep})
  \item
    taux d’apprentissage (\emph{lr})
  \item
    dimension des \emph{word-embeddings} (\emph{dim})
  \end{itemize}
\end{itemize}

\medskip

\begin{itemize}
\tightlist
\item
  Détermination empirique des hyperparamètres :

  \begin{itemize}
  \tightlist
  \item
    corrélation de Spearman entre nos vecteurs-mots et une base de
    jugement humain
  \item
    chronophage (il faut relancer le modèle à chaque fois).
  \end{itemize}
\end{itemize}

\medskip

\faArrowCircleRight{} Utilisation complémentaire de \texttt{Gensim} puis
validation avec notre implémentation.

\end{frame}

\begin{frame}{Exemple : epochs, fenêtre et taux d’apprentissage}
\protect\hypertarget{exemple-epochs-fenuxeatre-et-taux-dapprentissage}{}

\begin{figure}[htp]
\begin{center}
\includegraphics[width=1\textwidth]{img/test_parametres.png}
\end{center}
\vspace{-0.3cm}
\footnotesize
\emph{Paramètre utilisé : dim = 50}
\end{figure}

\end{frame}

\begin{frame}{Valeurs retenues pour les hyperparamètres}
\protect\hypertarget{valeurs-retenues-pour-les-hyperparamuxe8tres}{}

\begin{itemize}

\item \textbf{Nombre d'epochs} : qualité des résultats croît avec le nombre d'epochs

\faArrowCircleRight{} \textbf{ep = 100}.

\item \textbf{Taille de fenêtre} : capte des informations sémantiques différentes selon sa valeur

\faArrowCircleRight{} \textbf{w = 4}.

\item \textbf{Taux d'apprentissage} : 0,02 donne de meilleurs résultats

\faArrowCircleRight{} \textbf{lr = 0,02}.

\item \textbf{Dimension} : qualité des résultats croît avec la dimension jusqu'à 300 puis décroît. Peu de différences entre 100 et 300.

\faArrowCircleRight{} \textbf{dim = 100}.

\end{itemize}

\end{frame}

\hypertarget{uxe9valuation-sur-le-corpus-de-tweets}{%
\subsection{Évaluation sur le corpus de
tweets}\label{uxe9valuation-sur-le-corpus-de-tweets}}

\begin{frame}{Évaluation sur le corpus de tweets (1/2)}
\protect\hypertarget{uxe9valuation-sur-le-corpus-de-tweets-12}{}

\begin{figure}
\begin{minipage}{.4\textwidth}


« Notre » modèle

\medskip

\footnotesize
\textbf{ Spearman : } 0,57 (p-v : 4,1 \%)
\normalsize

\medskip

\faArrowCircleRight{} \textbf{bons} résultats


\end{minipage}%
\begin{minipage}{.6\textwidth}
\tiny

\begin{table}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
    \hline
\textbf{bonjour} & \textbf{femme} & \textbf{1} & \textbf{samedi} \tabularnewline
\emph{(669)} & \emph{(264)} & \emph{(765)} & \emph{(203)} \tabularnewline
       \hline
\includegraphics[height=2mm]{img/emojis/1.png} (0,59) & quelle (0,49) & 5 (0,55) & soir (0,57) \tabularnewline
\includegraphics[height=2mm]{img/emojis/2.png} (0,59) & cette (0,46) & mois (0,51) & vivement (0,51) \tabularnewline
merci (0,54) & une (0,44) & 10 (0,49) & demain (0,50) \tabularnewline
nuit (0,48) & vie (0,44) & 2 (0,48) & end (0,48) \tabularnewline
bisous (0,47) & grippe (0,44) & top (0,48) & weekend (0,47) \tabularnewline
bonne (0,47) & belle (0,43) & depuis (0,47) & matin (0,45) \tabularnewline
\includegraphics[height=2mm]{img/emojis/3.png} (0,46) & ma (0,43) & saison (0,46) & jeudi (0,45) \tabularnewline
vous (0,46) & magnifique (0,43) & ans (0,44) & prochain (0,43) \tabularnewline
plaisir (0,44) & nouvelle (0,43) & jours (0,43) & week (0,43) \tabularnewline
allez (0,43) & vidéo (0,39) & 3 (0,43) & \includegraphics[height=2mm]{img/emojis/4.png} (0,42) \tabularnewline
    \hline
 \end{tabular}
\captionsetup{margin=0cm,format=hang,justification=justified}

\end{center}
\emph{ep = 80 / w = 4 / lr = 0,02 / dim = 100 / base : 100 000 tweets}

\end{table}
\normalsize


\end{minipage}
\end{figure}

\pause

\begin{figure}
\begin{minipage}{.4\textwidth}

Modèle \texttt{Gensim}

\medskip

\footnotesize
\textbf{ Spearman : } 0,50 (p-v : 0,0 \%)
\normalsize

\medskip

\faArrowCircleRight{} \textbf{très bons} résultats

\end{minipage}%
\begin{minipage}{.6\textwidth}
\tiny

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
    \hline
\textbf{bonjour} & \textbf{femme} & \textbf{1} & \textbf{samedi} \tabularnewline
\emph{(17 043)} & \emph{(6 177)} & \emph{(21 055)} & \emph{(4 917)} \tabularnewline
       \hline
bonsoir (0,85) & fille (0,86) & 2 (0,65) & vendredi (0,88) \tabularnewline
bjr (0,75) & copine (0,74) & 3 (0,64) & jeudi (0,86) \tabularnewline
hello (0,71) & meuf (0,71) & 6 (0,63) & lundi (0,83) \tabularnewline
salut (0,66) & demoiselle (0,66) & 4 (0,62) & mercredi (0,83) \tabularnewline
coucou (0,55) & nana (0,66) & 7 (0,60) & dimanche (0,83) \tabularnewline
transmets (0,49) & nièce (0,66) & 5 (0,58) & mardi (0,76) \tabularnewline
désagrément (0,48) & sœur (0,65) & 9 (0,58) & demain (0,72) \tabularnewline
avezvous (0,48) & barbe (0,65) & 8 (0,56) & barathon (0,56) \tabularnewline
bettembourg (0,48) & maman (0,64) & 1e (0,55) & 22h45 (0,55) \tabularnewline
hey (0,47) & princesse (0,64) & 34 (0,53) & 20h (0,54) \tabularnewline
    \hline
 \end{tabular}
\captionsetup{margin=0cm,format=hang,justification=justified}
\end{center}
\emph{ep = 100 / w = 4 / lr = 0,02 / dim = 100 / base : ensemble des tweets}

\medskip

\footnotesize{10 plus proches voisins par similarité cosinus}
\end{table}

\normalsize

\end{minipage}
\end{figure}

\end{frame}

\begin{frame}{Évaluation sur le corpus de tweets (2/2)}
\protect\hypertarget{uxe9valuation-sur-le-corpus-de-tweets-22}{}

\begin{figure}
\begin{minipage}{.5\textwidth}

\begin{center}
\includegraphics[width=0.95\textwidth]{img/acp_gensim.png}
\end{center}
\emph{ACP sur un corpus réduit de mots.}


\end{minipage}%
\begin{minipage}{.5\textwidth}

  \centering
  \includegraphics[width=\linewidth]{img/acp_reine.png}
  \emph{$\protect\overrightarrow{Roi} - \protect\overrightarrow{Homme} + \protect\overrightarrow{Femme} = $ ?}

\end{minipage}

\end{figure}

\medskip

\faArrowCircleRight{} Réduction de dimension des vecteurs-mots et
(parfois) opérations sur les mots \textbf{convaincants}

\end{frame}

\hypertarget{indice-de-sentiments}{%
\section{Indice de sentiments}\label{indice-de-sentiments}}

\hypertarget{pruxe9dire-le-sentiment-dun-tweet}{%
\subsection{Prédire le sentiment d’un
tweet}\label{pruxe9dire-le-sentiment-dun-tweet}}

\begin{frame}{Prédire le sentiment d’un tweet}
\protect\hypertarget{pruxe9dire-le-sentiment-dun-tweet-1}{}

\begin{itemize}
\tightlist
\item
  \textbf{Idée} : associer à chaque tweet un sentiment :

  \begin{itemize}
  \tightlist
  \item
    1 s’il est positif
  \item
    0 s’il est négatif.
  \end{itemize}
\end{itemize}

\medskip

\begin{itemize}
\tightlist
\item
  base de 23 000 tweets annotés sur les transports urbains

  \begin{itemize}
  \tightlist
  \item
    \textbf{base d’entraînement} : 16 000 tweets
  \item
    \textbf{base de test} : 7 000 tweets
  \end{itemize}
\end{itemize}

\medskip

\begin{itemize}
\tightlist
\item
  2 approches :

  \begin{itemize}
  \tightlist
  \item
    \textbf{Modèle lexical} : utiliser l’information des tweets
    labelisés pour construire un sentiment moyen par mot.
  \item
    \textbf{Modèle logit} : utiliser les \emph{word-embeddings} comme
    prédicteurs d’une régression logistique.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Modèle lexical : sentiment moyen des mots}
\protect\hypertarget{moduxe8le-lexical-sentiment-moyen-des-mots}{}

Le sentiment prédit d’un tweet \(t\) composé de \(n\) mots sera :
\[S_{1,\gamma}(t) = \mathds{1}\left\{ \frac{1}{n} \sum \limits_{i=1}^n \alpha_i \geq \gamma\right\}  \qquad \in \{ 0,1 \}\]

\begin{itemize}
\item $\gamma \in [-1,1]$ un seuil fixé ;
\item $\alpha_i = \frac{nb_+(i) - nb_-(i)}{nb_+(i) + nb_-(i)} \in [-1,1]$  sentiment moyen du mot $i$ calculé à partir du nombre de tweets positifs ($nb_+(i)$) et négatifs ($nb_-(i)$) dans lesquels il apparaît. 
\end{itemize}

\faArrowCircleRight{}
\emph{Accuracy}\footnote{Taux de tweets dont le sentiment est bien prédit.}
= 89,1 \% (\(\gamma^* = -0,14\)).

\end{frame}

\begin{frame}{Modèle logit : prédiction grâce aux
\emph{word-embeddings}}
\protect\hypertarget{moduxe8le-logit-pruxe9diction-gruxe2ce-aux-word-embeddings}{}

Le sentiment prédit d’un tweet \(t\) sera :
\[S_{2,\gamma}(t) =\mathds{1}\left\{   \mathbb{P}(Y_i = 1 | X_{i}) \ge \gamma\right\} \qquad \quad \in \{ 0,1 \}\]
Avec :
\[Y_i = \mathds{1}\left\{ \sum_{i = 1}^n \beta_i X_{i,j} + \varepsilon_i \geq 0 \right\} \quad  \mathbb{P}(Y_i = 1 | X_{i}) = F_{\varepsilon}\left(\sum_{i = 1}^n \beta_i X_{i,j}\right)\]

\normalsize
\begin{itemize}
\item $Y_i$ le sentiment du tweet $i$ ;
\item $X_{i,1}, \dots, X_{i,n}$ les coordonnées de la \emph{sentence-embedding} du tweet $i$ ;
\item $\varepsilon_i$ le résidu de notre modèle, de fonction de répartition $F_{\varepsilon}$ qui vaudra $F_{\varepsilon}(x) = \frac{1}{1 + e^{-x}}$ dans le cas d'un modèle logit et $F_{\varepsilon}(x) = \Phi(x)$ (fonction de répartition d'une loi $\mathcal{N}(0, 1)$) dans le cas d'un modèle probit. 
\end{itemize}

\end{frame}

\begin{frame}{Spécifications du modèle logit}
\protect\hypertarget{spuxe9cifications-du-moduxe8le-logit}{}

Plusieurs points à traiter :

\begin{itemize}
\item
  Doit-on inclure les \emph{stop-words} ? \textsc{oui}
\item
  Comment traiter les mots inconnus ?
  \textsc{affecter le vecteur-mot lowfrequency}
\item
  Modèle probit ou logit ? \textsc{logit}
\end{itemize}

\bigskip

\bigskip

\faArrowCircleRight{} \emph{Accuracy} = 69,8 \%
(\(\gamma^* \simeq 0,5\)).

\end{frame}

\begin{frame}{Limites des modèles utilisés}
\protect\hypertarget{limites-des-moduxe8les-utilisuxe9s}{}

\textbf{Modèle lexical \underline{ici} meilleur que le modèle logit} car
\dots

\begin{enumerate}
[1.]
\tightlist
\item
  Davantage de mots inconnus dans le modèle logit (36,2 \% du
  vocabulaire contre 13,2 \% dans le modèle lexical)
\end{enumerate}

\pause

\begin{enumerate}
[1.]
\setcounter{enumi}{1}
\tightlist
\item
  Le processus d’annotation utilisé pour les tweets sur les transports
  urbains reproduit en partie par le modèle lexical ( ? )
\end{enumerate}

\pause

\begin{enumerate}
[1.]
\setcounter{enumi}{2}
\tightlist
\item
  Le \emph{domain shift}
\end{enumerate}

\pause

\bigskip

\faArrowCircleRight{} Utilisation d’une nouvelle base de test pour
neutraliser certains de ces effets

\textbf{Modèle logit \underline{alors} meilleur que le modèle
lexical}\newline(\emph{Accuracy} de 61,9~\% contre 55,9~\%).

\end{frame}

\hypertarget{sentiments-des-tweets-et-enquuxeate-de-conjoncture-aupruxe8s-des-muxe9nages}{%
\subsection{Sentiments des tweets et enquête de conjoncture auprès des
ménages}\label{sentiments-des-tweets-et-enquuxeate-de-conjoncture-aupruxe8s-des-muxe9nages}}

\begin{frame}{Sentiments des tweets et enquête Camme}
\protect\hypertarget{sentiments-des-tweets-et-enquuxeate-camme}{}

\centering\includegraphics[width =\textwidth]{img/rmd-graphSentiments-1}

\raggedright  \pause
\begin{columns}
\begin{column}{0.5\textwidth} \bcsmmh
\begin{itemize}
\item Indicateurs relativement éloignés de l'enquête Camme
\item Similarité (DTW) avec indicateur Camme plus proche avec modèle lexical que modèle word-embedding 
\end{itemize}

\end{column}\pause
\begin{column}{0.5\textwidth} \bcsmbh
\begin{itemize}
\item Modèle word-embedding utile pour prévoir indicateur Camme (causalité de Granger) $\ne$ modèle lexical
\item Modèle indicateur \textbf{avancé} des sentiments des ménages
\end{itemize}

\end{column}
\end{columns}

\end{frame}

\hypertarget{conclusion}{%
\section*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{section}{Conclusion}

\begin{frame}{Conclusion (1/2)}
\protect\hypertarget{conclusion-12}{}

\begin{itemize}
\tightlist
\item
  \emph{Word2vec}~\dots

  \begin{itemize}
  \tightlist
  \item
    capture \textbf{très bien} la \underline{sémantique des mots} dans
    un texte
  \item
    prédit \textbf{assez bien} le \underline{sentiment d’une phrase}
  \item
    est \textbf{potentiellement utile} pour
    \underline{prédire l’indicateur synthétique de}
    \underline{confiance des ménages de l’Insee} \dots
  \item
    \dots mais demeure \textbf{très différent} de cet indicateur (en
    évolution)
  \end{itemize}
\end{itemize}

\bigskip

\pause

\begin{itemize}
\tightlist
\item
  Pourquoi très différent~?

  \begin{itemize}
  \tightlist
  \item
    Principalement en raison de leurs \textbf{différentes philosophies}
    \footnotesize (sujets spécifiques de Camme VS positivité ou non des
    tweets pour notre indice) \normalsize \dots
  \item
    \dots\{\} mais aussi à cause des \textbf{limites de la base
    d’entraînement} de tweets annotés \footnotesize (domain-shift,
    processus d’annotation, mots inconnus) \normalsize
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Conclusion (2/2)}
\protect\hypertarget{conclusion-22}{}

Pistes d’amélioration~?

\begin{itemize}
\item
  disposer d’une \textbf{base de tweets traitant de sujets divers, et
  bien annotés} \footnotesize (gradation de sentiments, modèles de type
  BERT, analyse approfondie du contenu et des auteurs des tweets \dots)
  \normalsize
\item
  améliorer le \textbf{prétraitement des tweets}
  \footnotesize (orthographe des mots, modèle à séquences d’unités de
  sous-mots type \emph{fasttext} \dots)\normalsize
\item
  utiliser des \textbf{modèles d’analyse de sentiment plus élaborés}
  \footnotesize  (type réseaux de neurones récurrents)\normalsize
\end{itemize}

\end{frame}

\begin{frame}{Merci pour votre attention}
\protect\hypertarget{merci-pour-votre-attention}{}

\href{https://github.com/ARKEnsae/TweetEmbedding}{\faGithub{} ARKEnsae/TweetEmbedding}

\href{https://arkensae.github.io//TweetEmbedding/Redaction/Rapport_Final/Rapport.pdf}{\faEdit{} Rapport du projet}

\begin{center}
\includegraphics[width = 2.5cm]{img/LOGO-ENSAE.png}
\end{center}

\end{frame}

\end{document}
